{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F  \n",
    "from torch import optim \n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch.nn.init as init\n",
    "import random\n",
    "import time\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from nn_implementation import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function for Local Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def local_search_cv(num_iterations, initial_configuration, param_ranges, train_loader, NumbOfClasses, k_folds=5, use_scaling=True):\n",
    "    best_f1 = 0.0\n",
    "    best_combination = initial_configuration\n",
    "    current_configuration = initial_configuration\n",
    "    train_times = []\n",
    "    results = []\n",
    "    \n",
    "    kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "    if use_scaling:\n",
    "        scaler = StandardScaler()\n",
    "        scaler.fit(train_loader.dataset.tensors[0].numpy())\n",
    "        \n",
    "    for _ in range(num_iterations):\n",
    "        # Small changes to the current configuration\n",
    "        new_configuration = {\n",
    "            'Hidden Layer Sizes': [\n",
    "                max(1, size + random.randint(-1, 1)) for size in current_configuration['Hidden Layer Sizes']\n",
    "            ],\n",
    "            'Activation Function': random.choice(param_ranges['activation_functions']),\n",
    "            'Learning Rate': max(param_ranges['min_lr'], min(param_ranges['max_lr'], current_configuration['Learning Rate'] + random.uniform(-0.01, 0.01))),\n",
    "            'Batch Size': random.choice(param_ranges['batch_sizes']),\n",
    "            'Number of Epochs': max(1, current_configuration['Number of Epochs'] + random.randint(-1, 1))\n",
    "        }\n",
    "        fold_accuracies = []\n",
    "        fold_f1s = []\n",
    "        training_times = []\n",
    "\n",
    "        for train_index, test_index in kf.split(train_loader.dataset):\n",
    "            \n",
    "            X_train, X_test = train_loader.dataset.tensors[0][train_index], train_loader.dataset.tensors[0][test_index]\n",
    "            y_train, y_test = train_loader.dataset.tensors[1][train_index], train_loader.dataset.tensors[1][test_index]\n",
    "\n",
    "            model = NN(input_size=train_loader.dataset.tensors[0].shape[1],\n",
    "                       num_classes=NumbOfClasses,\n",
    "                       hidden_layer_sizes=new_configuration['Hidden Layer Sizes'],\n",
    "                       activation_function=new_configuration['Activation Function'])\n",
    "            criterion = nn.CrossEntropyLoss()\n",
    "            optimizer = optim.Adam(model.parameters(), lr=new_configuration['Learning Rate'])\n",
    "\n",
    "            if use_scaling:\n",
    "                X_train_scaled = torch.tensor(scaler.transform(X_train.numpy()))\n",
    "                X_test_scaled = torch.tensor(scaler.transform(X_test.numpy()))\n",
    "            else:\n",
    "                X_train_scaled, X_test_scaled = X_train, X_test\n",
    "                 \n",
    "            train_start_time = time.time()        \n",
    "            train_model(model, DataLoader(TensorDataset(X_train_scaled, y_train), batch_size=new_configuration['Batch Size'], shuffle=True), optimizer, criterion, new_configuration['Number of Epochs'])\n",
    "            train_end_time = time.time()\n",
    "            train_time = train_end_time - train_start_time\n",
    "          \n",
    "            accuracy_test, f1_test = check_accuracy(DataLoader(TensorDataset(X_test_scaled, y_test), batch_size=new_configuration['Batch Size'], shuffle=False), model)\n",
    "            fold_accuracies.append(accuracy_test)\n",
    "            training_times.append(train_time)\n",
    "            fold_f1s.append(f1_test)\n",
    "\n",
    "        avg_accuracy = np.mean(fold_accuracies)\n",
    "        avg_f1 = np.mean(fold_f1s)\n",
    "        avg_train_time = np.mean(training_times)\n",
    "\n",
    "        result_entry = {\n",
    "                'Hidden Layer Sizes': new_configuration['Hidden Layer Sizes'],\n",
    "                'Activation Function': new_configuration['Activation Function'].__name__,\n",
    "                'Learning Rate': new_configuration['Learning Rate'],\n",
    "                'Batch Size': new_configuration['Batch Size'],\n",
    "                'Number of Epochs': new_configuration['Number of Epochs'],\n",
    "                'Average Accuracy': avg_accuracy,\n",
    "                'Average F1': avg_f1,\n",
    "                'Average Training Time': avg_train_time\n",
    "        }\n",
    "\n",
    "        results.append(result_entry)\n",
    "\n",
    "        if f1_test > best_f1:\n",
    "            best_f1 = f1_test\n",
    "            best_combination = new_configuration\n",
    "            current_configuration = new_configuration  # Update the current configuration\n",
    "\n",
    "    results_df = pd.DataFrame(results)\n",
    "    return best_combination, best_f1, results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading / preparing data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wine Quality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fixed acidity</th>\n",
       "      <th>volatile acidity</th>\n",
       "      <th>citric acid</th>\n",
       "      <th>residual sugar</th>\n",
       "      <th>chlorides</th>\n",
       "      <th>free sulfur dioxide</th>\n",
       "      <th>total sulfur dioxide</th>\n",
       "      <th>density</th>\n",
       "      <th>pH</th>\n",
       "      <th>sulphates</th>\n",
       "      <th>alcohol</th>\n",
       "      <th>class</th>\n",
       "      <th>wine_type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7.4</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.9</td>\n",
       "      <td>0.076</td>\n",
       "      <td>11.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>0.9978</td>\n",
       "      <td>3.51</td>\n",
       "      <td>0.56</td>\n",
       "      <td>9.4</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7.8</td>\n",
       "      <td>0.88</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2.6</td>\n",
       "      <td>0.098</td>\n",
       "      <td>25.0</td>\n",
       "      <td>67.0</td>\n",
       "      <td>0.9968</td>\n",
       "      <td>3.20</td>\n",
       "      <td>0.68</td>\n",
       "      <td>9.8</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7.8</td>\n",
       "      <td>0.76</td>\n",
       "      <td>0.04</td>\n",
       "      <td>2.3</td>\n",
       "      <td>0.092</td>\n",
       "      <td>15.0</td>\n",
       "      <td>54.0</td>\n",
       "      <td>0.9970</td>\n",
       "      <td>3.26</td>\n",
       "      <td>0.65</td>\n",
       "      <td>9.8</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>11.2</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.56</td>\n",
       "      <td>1.9</td>\n",
       "      <td>0.075</td>\n",
       "      <td>17.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>0.9980</td>\n",
       "      <td>3.16</td>\n",
       "      <td>0.58</td>\n",
       "      <td>9.8</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7.4</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.9</td>\n",
       "      <td>0.076</td>\n",
       "      <td>11.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>0.9978</td>\n",
       "      <td>3.51</td>\n",
       "      <td>0.56</td>\n",
       "      <td>9.4</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   fixed acidity  volatile acidity  citric acid  residual sugar  chlorides  \\\n",
       "0            7.4              0.70         0.00             1.9      0.076   \n",
       "1            7.8              0.88         0.00             2.6      0.098   \n",
       "2            7.8              0.76         0.04             2.3      0.092   \n",
       "3           11.2              0.28         0.56             1.9      0.075   \n",
       "4            7.4              0.70         0.00             1.9      0.076   \n",
       "\n",
       "   free sulfur dioxide  total sulfur dioxide  density    pH  sulphates  \\\n",
       "0                 11.0                  34.0   0.9978  3.51       0.56   \n",
       "1                 25.0                  67.0   0.9968  3.20       0.68   \n",
       "2                 15.0                  54.0   0.9970  3.26       0.65   \n",
       "3                 17.0                  60.0   0.9980  3.16       0.58   \n",
       "4                 11.0                  34.0   0.9978  3.51       0.56   \n",
       "\n",
       "   alcohol  class  wine_type  \n",
       "0      9.4      5          1  \n",
       "1      9.8      5          1  \n",
       "2      9.8      5          1  \n",
       "3      9.8      6          1  \n",
       "4      9.4      5          1  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wine_quality = pd.read_csv('./preprocessed-datasets/wine_quality_prepro.csv', index_col=0)\n",
    "wine_quality.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Congressional Voting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>handicapped-infants</th>\n",
       "      <th>water-project-cost-sharing</th>\n",
       "      <th>adoption-of-the-budget-resolution</th>\n",
       "      <th>physician-fee-freeze</th>\n",
       "      <th>el-salvador-aid</th>\n",
       "      <th>religious-groups-in-schools</th>\n",
       "      <th>anti-satellite-test-ban</th>\n",
       "      <th>aid-to-nicaraguan-contras</th>\n",
       "      <th>mx-missile</th>\n",
       "      <th>immigration</th>\n",
       "      <th>synfuels-crporation-cutback</th>\n",
       "      <th>education-spending</th>\n",
       "      <th>superfund-right-to-sue</th>\n",
       "      <th>crime</th>\n",
       "      <th>duty-free-exports</th>\n",
       "      <th>export-administration-act-south-africa</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>140</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>383</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>201</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>297</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>309</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    ID  handicapped-infants  water-project-cost-sharing  \\\n",
       "0  140                  1.0                         0.0   \n",
       "1  383                  1.0                         1.0   \n",
       "2  201                  0.0                         0.0   \n",
       "3  297                  0.0                         0.0   \n",
       "4  309                  0.0                         0.0   \n",
       "\n",
       "   adoption-of-the-budget-resolution  physician-fee-freeze  el-salvador-aid  \\\n",
       "0                                1.0                   0.0              0.0   \n",
       "1                                0.0                   1.0              1.0   \n",
       "2                                1.0                   0.0              0.0   \n",
       "3                                1.0                   1.0              1.0   \n",
       "4                                0.0                   1.0              1.0   \n",
       "\n",
       "   religious-groups-in-schools  anti-satellite-test-ban  \\\n",
       "0                          1.0                      1.0   \n",
       "1                          1.0                      0.0   \n",
       "2                          0.0                      1.0   \n",
       "3                          1.0                      0.0   \n",
       "4                          1.0                      0.0   \n",
       "\n",
       "   aid-to-nicaraguan-contras  mx-missile  immigration  \\\n",
       "0                        1.0         1.0          0.0   \n",
       "1                        0.0         0.0          0.0   \n",
       "2                        1.0         1.0          0.0   \n",
       "3                        0.0         0.0          1.0   \n",
       "4                        0.0         0.0          1.0   \n",
       "\n",
       "   synfuels-crporation-cutback  education-spending  superfund-right-to-sue  \\\n",
       "0                          0.0                 0.0                     0.0   \n",
       "1                          1.0                 0.0                     1.0   \n",
       "2                          0.0                 0.0                     0.0   \n",
       "3                          0.0                 1.0                     1.0   \n",
       "4                          0.0                 1.0                     1.0   \n",
       "\n",
       "   crime  duty-free-exports  export-administration-act-south-africa  class  \n",
       "0    0.0                1.0                                     1.0      1  \n",
       "1    1.0                0.0                                     1.0      1  \n",
       "2    1.0                1.0                                     1.0      1  \n",
       "3    1.0                1.0                                     1.0      0  \n",
       "4    1.0                0.0                                     0.0      0  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cong_voting = pd.read_csv('./preprocessed-datasets/CongressionVoting_prepro.csv')\n",
    "# encode class value democrat as 1 and republican as 0\n",
    "cong_voting['class'] = cong_voting['class'].map({'democrat': 1, 'republican': 0})\n",
    "cong_voting.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bank Marketing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>default</th>\n",
       "      <th>housing</th>\n",
       "      <th>loan</th>\n",
       "      <th>campaign</th>\n",
       "      <th>pdays</th>\n",
       "      <th>previous</th>\n",
       "      <th>emp.var.rate</th>\n",
       "      <th>cons.price.idx</th>\n",
       "      <th>cons.conf.idx</th>\n",
       "      <th>...</th>\n",
       "      <th>education_basic.9y</th>\n",
       "      <th>education_high.school</th>\n",
       "      <th>education_illiterate</th>\n",
       "      <th>education_professional.course</th>\n",
       "      <th>education_university.degree</th>\n",
       "      <th>education_unknown</th>\n",
       "      <th>poutcome_failure</th>\n",
       "      <th>poutcome_nonexistent</th>\n",
       "      <th>poutcome_success</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>56</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>999</td>\n",
       "      <td>0</td>\n",
       "      <td>1.1</td>\n",
       "      <td>93.994</td>\n",
       "      <td>-36.4</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>57</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>999</td>\n",
       "      <td>0</td>\n",
       "      <td>1.1</td>\n",
       "      <td>93.994</td>\n",
       "      <td>-36.4</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>37</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>999</td>\n",
       "      <td>0</td>\n",
       "      <td>1.1</td>\n",
       "      <td>93.994</td>\n",
       "      <td>-36.4</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>40</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>999</td>\n",
       "      <td>0</td>\n",
       "      <td>1.1</td>\n",
       "      <td>93.994</td>\n",
       "      <td>-36.4</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>56</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>999</td>\n",
       "      <td>0</td>\n",
       "      <td>1.1</td>\n",
       "      <td>93.994</td>\n",
       "      <td>-36.4</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 34 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   age  default  housing  loan  campaign  pdays  previous  emp.var.rate  \\\n",
       "0   56      0.0      0.0   0.0         1    999         0           1.1   \n",
       "1   57      0.0      0.0   0.0         1    999         0           1.1   \n",
       "2   37      0.0      1.0   0.0         1    999         0           1.1   \n",
       "3   40      0.0      0.0   0.0         1    999         0           1.1   \n",
       "4   56      0.0      0.0   1.0         1    999         0           1.1   \n",
       "\n",
       "   cons.price.idx  cons.conf.idx  ...  education_basic.9y  \\\n",
       "0          93.994          -36.4  ...                   0   \n",
       "1          93.994          -36.4  ...                   0   \n",
       "2          93.994          -36.4  ...                   0   \n",
       "3          93.994          -36.4  ...                   0   \n",
       "4          93.994          -36.4  ...                   0   \n",
       "\n",
       "   education_high.school  education_illiterate  education_professional.course  \\\n",
       "0                      0                     0                              0   \n",
       "1                      1                     0                              0   \n",
       "2                      1                     0                              0   \n",
       "3                      0                     0                              0   \n",
       "4                      1                     0                              0   \n",
       "\n",
       "   education_university.degree  education_unknown  poutcome_failure  \\\n",
       "0                            0                  0                 0   \n",
       "1                            0                  0                 0   \n",
       "2                            0                  0                 0   \n",
       "3                            0                  0                 0   \n",
       "4                            0                  0                 0   \n",
       "\n",
       "   poutcome_nonexistent  poutcome_success  class  \n",
       "0                     1                 0      0  \n",
       "1                     1                 0      0  \n",
       "2                     1                 0      0  \n",
       "3                     1                 0      0  \n",
       "4                     1                 0      0  \n",
       "\n",
       "[5 rows x 34 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bank_marketing = pd.read_csv('./preprocessed-datasets/bank_marketing_prepro.csv')\n",
    "column_to_move = 'class'\n",
    "\n",
    "# Move class to the last index\n",
    "columns = [col for col in bank_marketing.columns if col != column_to_move] + [column_to_move]\n",
    "bank_marketing = bank_marketing[columns]\n",
    "\n",
    "bank_marketing.drop('Unnamed: 0', axis=1,inplace=True)\n",
    "bank_marketing.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing Local search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/9, Average Loss: 1.1439792116483052\n",
      "Epoch 2/9, Average Loss: 1.0000627239545186\n",
      "Epoch 3/9, Average Loss: 0.8494235475858053\n",
      "Epoch 4/9, Average Loss: 0.7388467192649841\n",
      "Epoch 5/9, Average Loss: 0.6501200993855795\n",
      "Epoch 6/9, Average Loss: 0.568446695804596\n",
      "Epoch 7/9, Average Loss: 0.5038002530733744\n",
      "Epoch 8/9, Average Loss: 0.4595282773176829\n",
      "Epoch 9/9, Average Loss: 0.4187895854314168\n",
      "Epoch 1/9, Average Loss: 1.0623035033543904\n",
      "Epoch 2/9, Average Loss: 0.9240962068239847\n",
      "Epoch 3/9, Average Loss: 0.7888694405555725\n",
      "Epoch 4/9, Average Loss: 0.664382259051005\n",
      "Epoch 5/9, Average Loss: 0.5817477504412333\n",
      "Epoch 6/9, Average Loss: 0.5188534160455068\n",
      "Epoch 7/9, Average Loss: 0.4531663755575816\n",
      "Epoch 8/9, Average Loss: 0.395596315463384\n",
      "Epoch 9/9, Average Loss: 0.35818377137184143\n",
      "Epoch 1/9, Average Loss: 1.0996410846710205\n",
      "Epoch 2/9, Average Loss: 0.9563032388687134\n",
      "Epoch 3/9, Average Loss: 0.8162670135498047\n",
      "Epoch 4/9, Average Loss: 0.7206334074338278\n",
      "Epoch 5/9, Average Loss: 0.6378455758094788\n",
      "Epoch 6/9, Average Loss: 0.5550618668397268\n",
      "Epoch 7/9, Average Loss: 0.510513166586558\n",
      "Epoch 8/9, Average Loss: 0.45764051874478656\n",
      "Epoch 9/9, Average Loss: 0.4108116030693054\n",
      "Epoch 1/9, Average Loss: 1.1515995462735493\n",
      "Epoch 2/9, Average Loss: 0.9777394930521647\n",
      "Epoch 3/9, Average Loss: 0.8377528786659241\n",
      "Epoch 4/9, Average Loss: 0.7429088155428568\n",
      "Epoch 5/9, Average Loss: 0.6514070630073547\n",
      "Epoch 6/9, Average Loss: 0.5694271624088287\n",
      "Epoch 7/9, Average Loss: 0.5261018772919973\n",
      "Epoch 8/9, Average Loss: 0.4541052579879761\n",
      "Epoch 9/9, Average Loss: 0.4127278725306193\n",
      "Epoch 1/9, Average Loss: 1.0972617467244465\n",
      "Epoch 2/9, Average Loss: 0.9466367363929749\n",
      "Epoch 3/9, Average Loss: 0.8046481609344482\n",
      "Epoch 4/9, Average Loss: 0.7096453706423441\n",
      "Epoch 5/9, Average Loss: 0.618644654750824\n",
      "Epoch 6/9, Average Loss: 0.541690468788147\n",
      "Epoch 7/9, Average Loss: 0.476097305615743\n",
      "Epoch 8/9, Average Loss: 0.44440920154253644\n",
      "Epoch 9/9, Average Loss: 0.4038782815138499\n",
      "Epoch 1/10, Average Loss: 0.6006451845169067\n",
      "Epoch 2/10, Average Loss: 0.2316158302128315\n",
      "Epoch 3/10, Average Loss: 0.17674443001548448\n",
      "Epoch 4/10, Average Loss: 0.14939713974793753\n",
      "Epoch 5/10, Average Loss: 0.10963789870341618\n",
      "Epoch 6/10, Average Loss: 0.09513814809421699\n",
      "Epoch 7/10, Average Loss: 0.07449943696459134\n",
      "Epoch 8/10, Average Loss: 0.0640077708909909\n",
      "Epoch 9/10, Average Loss: 0.05140051866571108\n",
      "Epoch 10/10, Average Loss: 0.04704739556958278\n",
      "Epoch 1/10, Average Loss: 0.5927496552467346\n",
      "Epoch 2/10, Average Loss: 0.2695055852333705\n",
      "Epoch 3/10, Average Loss: 0.17671418314178786\n",
      "Epoch 4/10, Average Loss: 0.14813278801739216\n",
      "Epoch 5/10, Average Loss: 0.10387160070240498\n",
      "Epoch 6/10, Average Loss: 0.07763688980291288\n",
      "Epoch 7/10, Average Loss: 0.06004513365526994\n",
      "Epoch 8/10, Average Loss: 0.047874427090088524\n",
      "Epoch 9/10, Average Loss: 0.03673449996858835\n",
      "Epoch 10/10, Average Loss: 0.02949677687138319\n",
      "Epoch 1/10, Average Loss: 0.507184570034345\n",
      "Epoch 2/10, Average Loss: 0.23622304449478784\n",
      "Epoch 3/10, Average Loss: 0.2334804100294908\n",
      "Epoch 4/10, Average Loss: 0.16851922248800597\n",
      "Epoch 5/10, Average Loss: 0.1219427411754926\n",
      "Epoch 6/10, Average Loss: 0.12273123363653819\n",
      "Epoch 7/10, Average Loss: 0.08205160374442737\n",
      "Epoch 8/10, Average Loss: 0.06330457950631778\n",
      "Epoch 9/10, Average Loss: 0.05268656462430954\n",
      "Epoch 10/10, Average Loss: 0.05240337923169136\n",
      "Epoch 1/10, Average Loss: 0.5487733880678812\n",
      "Epoch 2/10, Average Loss: 0.2684037151436011\n",
      "Epoch 3/10, Average Loss: 0.2039147304991881\n",
      "Epoch 4/10, Average Loss: 0.12217776880909999\n",
      "Epoch 5/10, Average Loss: 0.09888160414993763\n",
      "Epoch 6/10, Average Loss: 0.07479902263730764\n",
      "Epoch 7/10, Average Loss: 0.060214701729516186\n",
      "Epoch 8/10, Average Loss: 0.045259177995224796\n",
      "Epoch 9/10, Average Loss: 0.03828659964104494\n",
      "Epoch 10/10, Average Loss: 0.031338430320223175\n",
      "Epoch 1/10, Average Loss: 0.5438461005687714\n",
      "Epoch 2/10, Average Loss: 0.24283136675755182\n",
      "Epoch 3/10, Average Loss: 0.1779437226553758\n",
      "Epoch 4/10, Average Loss: 0.1176226045936346\n",
      "Epoch 5/10, Average Loss: 0.08535208863516648\n",
      "Epoch 6/10, Average Loss: 0.0724982488900423\n",
      "Epoch 7/10, Average Loss: 0.06486094929277897\n",
      "Epoch 8/10, Average Loss: 0.04505098766336838\n",
      "Epoch 9/10, Average Loss: 0.03642123619404932\n",
      "Epoch 10/10, Average Loss: 0.032592590898275375\n",
      "Epoch 1/10, Average Loss: 0.660619705915451\n",
      "Epoch 2/10, Average Loss: 0.5859887897968292\n",
      "Epoch 3/10, Average Loss: 0.5047780126333237\n",
      "Epoch 4/10, Average Loss: 0.41125254333019257\n",
      "Epoch 5/10, Average Loss: 0.32918035984039307\n",
      "Epoch 6/10, Average Loss: 0.2576960399746895\n",
      "Epoch 7/10, Average Loss: 0.24365518242120743\n",
      "Epoch 8/10, Average Loss: 0.2152610644698143\n",
      "Epoch 9/10, Average Loss: 0.18701840937137604\n",
      "Epoch 10/10, Average Loss: 0.17525573819875717\n",
      "Epoch 1/10, Average Loss: 0.6648589670658112\n",
      "Epoch 2/10, Average Loss: 0.6157965958118439\n",
      "Epoch 3/10, Average Loss: 0.5033020526170731\n",
      "Epoch 4/10, Average Loss: 0.43576042354106903\n",
      "Epoch 5/10, Average Loss: 0.37295207381248474\n",
      "Epoch 6/10, Average Loss: 0.285804808139801\n",
      "Epoch 7/10, Average Loss: 0.26199591159820557\n",
      "Epoch 8/10, Average Loss: 0.2324206680059433\n",
      "Epoch 9/10, Average Loss: 0.22131039202213287\n",
      "Epoch 10/10, Average Loss: 0.1548500917851925\n",
      "Epoch 1/10, Average Loss: 0.6617575585842133\n",
      "Epoch 2/10, Average Loss: 0.5769598186016083\n",
      "Epoch 3/10, Average Loss: 0.5252865552902222\n",
      "Epoch 4/10, Average Loss: 0.4341554641723633\n",
      "Epoch 5/10, Average Loss: 0.3392639756202698\n",
      "Epoch 6/10, Average Loss: 0.3057422935962677\n",
      "Epoch 7/10, Average Loss: 0.2680460959672928\n",
      "Epoch 8/10, Average Loss: 0.22147801518440247\n",
      "Epoch 9/10, Average Loss: 0.21612216532230377\n",
      "Epoch 10/10, Average Loss: 0.21540024131536484\n",
      "Epoch 1/10, Average Loss: 0.6681105196475983\n",
      "Epoch 2/10, Average Loss: 0.5928050577640533\n",
      "Epoch 3/10, Average Loss: 0.5075867921113968\n",
      "Epoch 4/10, Average Loss: 0.42916959524154663\n",
      "Epoch 5/10, Average Loss: 0.3642286956310272\n",
      "Epoch 6/10, Average Loss: 0.30057184398174286\n",
      "Epoch 7/10, Average Loss: 0.265903502702713\n",
      "Epoch 8/10, Average Loss: 0.2138247936964035\n",
      "Epoch 9/10, Average Loss: 0.20948968827724457\n",
      "Epoch 10/10, Average Loss: 0.1851741299033165\n",
      "Epoch 1/10, Average Loss: 0.6513860821723938\n",
      "Epoch 2/10, Average Loss: 0.5556544065475464\n",
      "Epoch 3/10, Average Loss: 0.4863462746143341\n",
      "Epoch 4/10, Average Loss: 0.41397640109062195\n",
      "Epoch 5/10, Average Loss: 0.3470083624124527\n",
      "Epoch 6/10, Average Loss: 0.24708987772464752\n",
      "Epoch 7/10, Average Loss: 0.2651355043053627\n",
      "Epoch 8/10, Average Loss: 0.23094704747200012\n",
      "Epoch 9/10, Average Loss: 0.17210585623979568\n",
      "Epoch 10/10, Average Loss: 0.18916213512420654\n",
      "Epoch 1/10, Average Loss: 0.6193260252475739\n",
      "Epoch 2/10, Average Loss: 0.5925176739692688\n",
      "Epoch 3/10, Average Loss: 0.5656093060970306\n",
      "Epoch 4/10, Average Loss: 0.5176193118095398\n",
      "Epoch 5/10, Average Loss: 0.493791863322258\n",
      "Epoch 6/10, Average Loss: 0.44313859939575195\n",
      "Epoch 7/10, Average Loss: 0.41235940158367157\n",
      "Epoch 8/10, Average Loss: 0.38398292660713196\n",
      "Epoch 9/10, Average Loss: 0.3308652639389038\n",
      "Epoch 10/10, Average Loss: 0.3093169033527374\n",
      "Epoch 1/10, Average Loss: 0.6235172748565674\n",
      "Epoch 2/10, Average Loss: 0.5826711654663086\n",
      "Epoch 3/10, Average Loss: 0.5452359318733215\n",
      "Epoch 4/10, Average Loss: 0.5094130337238312\n",
      "Epoch 5/10, Average Loss: 0.45755626261234283\n",
      "Epoch 6/10, Average Loss: 0.4197707176208496\n",
      "Epoch 7/10, Average Loss: 0.38365592062473297\n",
      "Epoch 8/10, Average Loss: 0.34133608639240265\n",
      "Epoch 9/10, Average Loss: 0.30089958012104034\n",
      "Epoch 10/10, Average Loss: 0.3051830679178238\n",
      "Epoch 1/10, Average Loss: 0.6610060334205627\n",
      "Epoch 2/10, Average Loss: 0.5998405814170837\n",
      "Epoch 3/10, Average Loss: 0.5912623703479767\n",
      "Epoch 4/10, Average Loss: 0.5507219135761261\n",
      "Epoch 5/10, Average Loss: 0.5202147662639618\n",
      "Epoch 6/10, Average Loss: 0.50161412358284\n",
      "Epoch 7/10, Average Loss: 0.4244214743375778\n",
      "Epoch 8/10, Average Loss: 0.4115028381347656\n",
      "Epoch 9/10, Average Loss: 0.3838096112012863\n",
      "Epoch 10/10, Average Loss: 0.3755180239677429\n",
      "Epoch 1/10, Average Loss: 0.6366756856441498\n",
      "Epoch 2/10, Average Loss: 0.5996578931808472\n",
      "Epoch 3/10, Average Loss: 0.568894773721695\n",
      "Epoch 4/10, Average Loss: 0.5253183841705322\n",
      "Epoch 5/10, Average Loss: 0.48886698484420776\n",
      "Epoch 6/10, Average Loss: 0.44421418011188507\n",
      "Epoch 7/10, Average Loss: 0.40393948554992676\n",
      "Epoch 8/10, Average Loss: 0.37767480313777924\n",
      "Epoch 9/10, Average Loss: 0.31189608573913574\n",
      "Epoch 10/10, Average Loss: 0.32978956401348114\n",
      "Epoch 1/10, Average Loss: 0.6351886987686157\n",
      "Epoch 2/10, Average Loss: 0.5972005128860474\n",
      "Epoch 3/10, Average Loss: 0.5497324764728546\n",
      "Epoch 4/10, Average Loss: 0.5313166379928589\n",
      "Epoch 5/10, Average Loss: 0.5097577422857285\n",
      "Epoch 6/10, Average Loss: 0.4416353553533554\n",
      "Epoch 7/10, Average Loss: 0.42492520809173584\n",
      "Epoch 8/10, Average Loss: 0.3830445259809494\n",
      "Epoch 9/10, Average Loss: 0.3357871174812317\n",
      "Epoch 10/10, Average Loss: 0.3112867921590805\n",
      "Epoch 1/11, Average Loss: 0.37226734558741253\n",
      "Epoch 2/11, Average Loss: 0.22974715133508047\n",
      "Epoch 3/11, Average Loss: 0.17795205116271973\n",
      "Epoch 4/11, Average Loss: 0.15245336294174194\n",
      "Epoch 5/11, Average Loss: 0.12455749263366063\n",
      "Epoch 6/11, Average Loss: 0.10129970808823903\n",
      "Epoch 7/11, Average Loss: 0.08694420009851456\n",
      "Epoch 8/11, Average Loss: 0.07351420323053996\n",
      "Epoch 9/11, Average Loss: 0.06002712373932203\n",
      "Epoch 10/11, Average Loss: 0.058885845045248665\n",
      "Epoch 11/11, Average Loss: 0.048245763406157494\n",
      "Epoch 1/11, Average Loss: 0.3758322795232137\n",
      "Epoch 2/11, Average Loss: 0.21397467454274496\n",
      "Epoch 3/11, Average Loss: 0.1578208456436793\n",
      "Epoch 4/11, Average Loss: 0.12446093559265137\n",
      "Epoch 5/11, Average Loss: 0.10289302468299866\n",
      "Epoch 6/11, Average Loss: 0.07654713963468869\n",
      "Epoch 7/11, Average Loss: 0.06116047874093056\n",
      "Epoch 8/11, Average Loss: 0.047980054592092834\n",
      "Epoch 9/11, Average Loss: 0.03812454206248125\n",
      "Epoch 10/11, Average Loss: 0.03206022083759308\n",
      "Epoch 11/11, Average Loss: 0.027511257057388622\n",
      "Epoch 1/11, Average Loss: 0.37080276012420654\n",
      "Epoch 2/11, Average Loss: 0.20159453650315604\n",
      "Epoch 3/11, Average Loss: 0.16868378470341364\n",
      "Epoch 4/11, Average Loss: 0.14039273063341776\n",
      "Epoch 5/11, Average Loss: 0.11406494801243146\n",
      "Epoch 6/11, Average Loss: 0.0986524447798729\n",
      "Epoch 7/11, Average Loss: 0.08117088675498962\n",
      "Epoch 8/11, Average Loss: 0.06985899060964584\n",
      "Epoch 9/11, Average Loss: 0.060054423908392586\n",
      "Epoch 10/11, Average Loss: 0.055828310549259186\n",
      "Epoch 11/11, Average Loss: 0.049407935390869774\n",
      "Epoch 1/11, Average Loss: 0.36398879686991376\n",
      "Epoch 2/11, Average Loss: 0.20308112104733786\n",
      "Epoch 3/11, Average Loss: 0.15863567094008127\n",
      "Epoch 4/11, Average Loss: 0.1349592333038648\n",
      "Epoch 5/11, Average Loss: 0.1089212919274966\n",
      "Epoch 6/11, Average Loss: 0.08761367201805115\n",
      "Epoch 7/11, Average Loss: 0.07911218454440434\n",
      "Epoch 8/11, Average Loss: 0.060907067731022835\n",
      "Epoch 9/11, Average Loss: 0.056364111602306366\n",
      "Epoch 10/11, Average Loss: 0.04744234184424082\n",
      "Epoch 11/11, Average Loss: 0.04268303389350573\n",
      "Epoch 1/11, Average Loss: 0.37319962680339813\n",
      "Epoch 2/11, Average Loss: 0.2276605119307836\n",
      "Epoch 3/11, Average Loss: 0.17849797507127127\n",
      "Epoch 4/11, Average Loss: 0.14373532682657242\n",
      "Epoch 5/11, Average Loss: 0.11577336986859639\n",
      "Epoch 6/11, Average Loss: 0.09991954267024994\n",
      "Epoch 7/11, Average Loss: 0.08293357367316882\n",
      "Epoch 8/11, Average Loss: 0.06636262374619643\n",
      "Epoch 9/11, Average Loss: 0.0584008718530337\n",
      "Epoch 10/11, Average Loss: 0.05179656110703945\n",
      "Epoch 11/11, Average Loss: 0.051426246762275696\n",
      "Epoch 1/11, Average Loss: 0.5600510140260061\n",
      "Epoch 2/11, Average Loss: 0.2877848992745082\n",
      "Epoch 3/11, Average Loss: 0.20675942798455557\n",
      "Epoch 4/11, Average Loss: 0.14477381855249405\n",
      "Epoch 5/11, Average Loss: 0.11472744743029277\n",
      "Epoch 6/11, Average Loss: 0.08379462982217471\n",
      "Epoch 7/11, Average Loss: 0.07194495821992557\n",
      "Epoch 8/11, Average Loss: 0.055421529337763786\n",
      "Epoch 9/11, Average Loss: 0.04742079103986422\n",
      "Epoch 10/11, Average Loss: 0.043982392797867455\n",
      "Epoch 11/11, Average Loss: 0.03873179107904434\n",
      "Epoch 1/11, Average Loss: 0.52470862865448\n",
      "Epoch 2/11, Average Loss: 0.2615425686041514\n",
      "Epoch 3/11, Average Loss: 0.16993276278177896\n",
      "Epoch 4/11, Average Loss: 0.13156335055828094\n",
      "Epoch 5/11, Average Loss: 0.09488353257377942\n",
      "Epoch 6/11, Average Loss: 0.0742083340883255\n",
      "Epoch 7/11, Average Loss: 0.058841014901796974\n",
      "Epoch 8/11, Average Loss: 0.04287340119481087\n",
      "Epoch 9/11, Average Loss: 0.033538896745691694\n",
      "Epoch 10/11, Average Loss: 0.029833982077737648\n",
      "Epoch 11/11, Average Loss: 0.02259629654387633\n",
      "Epoch 1/11, Average Loss: 0.5991521080334982\n",
      "Epoch 2/11, Average Loss: 0.31992379824320477\n",
      "Epoch 3/11, Average Loss: 0.23456158737341562\n",
      "Epoch 4/11, Average Loss: 0.18483427166938782\n",
      "Epoch 5/11, Average Loss: 0.12485788265864055\n",
      "Epoch 6/11, Average Loss: 0.10500231385231018\n",
      "Epoch 7/11, Average Loss: 0.0831069437166055\n",
      "Epoch 8/11, Average Loss: 0.07059818009535472\n",
      "Epoch 9/11, Average Loss: 0.061845449109872185\n",
      "Epoch 10/11, Average Loss: 0.05375322699546814\n",
      "Epoch 11/11, Average Loss: 0.04005529855688413\n",
      "Epoch 1/11, Average Loss: 0.6208811501661936\n",
      "Epoch 2/11, Average Loss: 0.3351738750934601\n",
      "Epoch 3/11, Average Loss: 0.2223659505446752\n",
      "Epoch 4/11, Average Loss: 0.16132372121016184\n",
      "Epoch 5/11, Average Loss: 0.11231306691964467\n",
      "Epoch 6/11, Average Loss: 0.08534386257330577\n",
      "Epoch 7/11, Average Loss: 0.06849291423956554\n",
      "Epoch 8/11, Average Loss: 0.055019168804089226\n",
      "Epoch 9/11, Average Loss: 0.04215474302570025\n",
      "Epoch 10/11, Average Loss: 0.033357102423906326\n",
      "Epoch 11/11, Average Loss: 0.027198275551199913\n",
      "Epoch 1/11, Average Loss: 0.5938279330730438\n",
      "Epoch 2/11, Average Loss: 0.31022514402866364\n",
      "Epoch 3/11, Average Loss: 0.20476225763559341\n",
      "Epoch 4/11, Average Loss: 0.14883897205193838\n",
      "Epoch 5/11, Average Loss: 0.12211026748021443\n",
      "Epoch 6/11, Average Loss: 0.08660687878727913\n",
      "Epoch 7/11, Average Loss: 0.06962084025144577\n",
      "Epoch 8/11, Average Loss: 0.058620178451140724\n",
      "Epoch 9/11, Average Loss: 0.046604515674213566\n",
      "Epoch 10/11, Average Loss: 0.039232175797224045\n",
      "Epoch 11/11, Average Loss: 0.03584467868010203\n",
      "Epoch 1/11, Average Loss: 1.7527487874031067\n",
      "Epoch 2/11, Average Loss: 1.6277480125427246\n",
      "Epoch 3/11, Average Loss: 1.4380476474761963\n",
      "Epoch 4/11, Average Loss: 1.4487495422363281\n",
      "Epoch 5/11, Average Loss: 1.2895952463150024\n",
      "Epoch 6/11, Average Loss: 1.1514497995376587\n",
      "Epoch 7/11, Average Loss: 1.0507332682609558\n",
      "Epoch 8/11, Average Loss: 0.9908963143825531\n",
      "Epoch 9/11, Average Loss: 0.8854387700557709\n",
      "Epoch 10/11, Average Loss: 0.8859096169471741\n",
      "Epoch 11/11, Average Loss: 0.7721489369869232\n",
      "Epoch 1/11, Average Loss: 1.8571908473968506\n",
      "Epoch 2/11, Average Loss: 1.6416677236557007\n",
      "Epoch 3/11, Average Loss: 1.5546512007713318\n",
      "Epoch 4/11, Average Loss: 1.4154172539710999\n",
      "Epoch 5/11, Average Loss: 1.2821060419082642\n",
      "Epoch 6/11, Average Loss: 1.1647281646728516\n",
      "Epoch 7/11, Average Loss: 1.1304740905761719\n",
      "Epoch 8/11, Average Loss: 1.0191227793693542\n",
      "Epoch 9/11, Average Loss: 0.981436550617218\n",
      "Epoch 10/11, Average Loss: 0.8523795306682587\n",
      "Epoch 11/11, Average Loss: 0.8483661115169525\n",
      "Epoch 1/11, Average Loss: 1.7001734972000122\n",
      "Epoch 2/11, Average Loss: 1.535353183746338\n",
      "Epoch 3/11, Average Loss: 1.397267997264862\n",
      "Epoch 4/11, Average Loss: 1.2945395708084106\n",
      "Epoch 5/11, Average Loss: 1.2316693663597107\n",
      "Epoch 6/11, Average Loss: 1.1152442693710327\n",
      "Epoch 7/11, Average Loss: 1.0074456334114075\n",
      "Epoch 8/11, Average Loss: 0.917743057012558\n",
      "Epoch 9/11, Average Loss: 0.8424718081951141\n",
      "Epoch 10/11, Average Loss: 0.8013724982738495\n",
      "Epoch 11/11, Average Loss: 0.755312979221344\n",
      "Epoch 1/11, Average Loss: 1.7617875933647156\n",
      "Epoch 2/11, Average Loss: 1.6981576085090637\n",
      "Epoch 3/11, Average Loss: 1.5291624069213867\n",
      "Epoch 4/11, Average Loss: 1.431857705116272\n",
      "Epoch 5/11, Average Loss: 1.2886400818824768\n",
      "Epoch 6/11, Average Loss: 1.1882681250572205\n",
      "Epoch 7/11, Average Loss: 1.1066141724586487\n",
      "Epoch 8/11, Average Loss: 1.0162655115127563\n",
      "Epoch 9/11, Average Loss: 0.9497486650943756\n",
      "Epoch 10/11, Average Loss: 0.9056274890899658\n",
      "Epoch 11/11, Average Loss: 0.8212999999523163\n",
      "Epoch 1/11, Average Loss: 1.7022461295127869\n",
      "Epoch 2/11, Average Loss: 1.566149353981018\n",
      "Epoch 3/11, Average Loss: 1.4531462788581848\n",
      "Epoch 4/11, Average Loss: 1.286181390285492\n",
      "Epoch 5/11, Average Loss: 1.1779010891914368\n",
      "Epoch 6/11, Average Loss: 1.1415131092071533\n",
      "Epoch 7/11, Average Loss: 1.0388561487197876\n",
      "Epoch 8/11, Average Loss: 0.9111440479755402\n",
      "Epoch 9/11, Average Loss: 0.8470588624477386\n",
      "Epoch 10/11, Average Loss: 0.8104350864887238\n",
      "Epoch 11/11, Average Loss: 0.7420713901519775\n",
      "Epoch 1/11, Average Loss: 0.7476845979690552\n",
      "Epoch 2/11, Average Loss: 0.32746876279513043\n",
      "Epoch 3/11, Average Loss: 0.22992748022079468\n",
      "Epoch 4/11, Average Loss: 0.17802520344654718\n",
      "Epoch 5/11, Average Loss: 0.13883765041828156\n",
      "Epoch 6/11, Average Loss: 0.12072769552469254\n",
      "Epoch 7/11, Average Loss: 0.1008540503680706\n",
      "Epoch 8/11, Average Loss: 0.08720877145727475\n",
      "Epoch 9/11, Average Loss: 0.07110492264231046\n",
      "Epoch 10/11, Average Loss: 0.06305249532063802\n",
      "Epoch 11/11, Average Loss: 0.0529179722070694\n",
      "Epoch 1/11, Average Loss: 0.8381285667419434\n",
      "Epoch 2/11, Average Loss: 0.35589827597141266\n",
      "Epoch 3/11, Average Loss: 0.2455927977959315\n",
      "Epoch 4/11, Average Loss: 0.18826020260651907\n",
      "Epoch 5/11, Average Loss: 0.15973238150278726\n",
      "Epoch 6/11, Average Loss: 0.1303656001885732\n",
      "Epoch 7/11, Average Loss: 0.1072289024790128\n",
      "Epoch 8/11, Average Loss: 0.0928648163874944\n",
      "Epoch 9/11, Average Loss: 0.07647126043836276\n",
      "Epoch 10/11, Average Loss: 0.06523317967851956\n",
      "Epoch 11/11, Average Loss: 0.0498511828482151\n",
      "Epoch 1/11, Average Loss: 0.7483515640099844\n",
      "Epoch 2/11, Average Loss: 0.34907060861587524\n",
      "Epoch 3/11, Average Loss: 0.2562947968641917\n",
      "Epoch 4/11, Average Loss: 0.2186644822359085\n",
      "Epoch 5/11, Average Loss: 0.17601603517929712\n",
      "Epoch 6/11, Average Loss: 0.1433609500527382\n",
      "Epoch 7/11, Average Loss: 0.11616802712281545\n",
      "Epoch 8/11, Average Loss: 0.10390143593152364\n",
      "Epoch 9/11, Average Loss: 0.0824387123187383\n",
      "Epoch 10/11, Average Loss: 0.06682680795590083\n",
      "Epoch 11/11, Average Loss: 0.05458093931277593\n",
      "Epoch 1/11, Average Loss: 0.7695268789927164\n",
      "Epoch 2/11, Average Loss: 0.32394762833913165\n",
      "Epoch 3/11, Average Loss: 0.22751023868719736\n",
      "Epoch 4/11, Average Loss: 0.17151720821857452\n",
      "Epoch 5/11, Average Loss: 0.13241719081997871\n",
      "Epoch 6/11, Average Loss: 0.10486082856853803\n",
      "Epoch 7/11, Average Loss: 0.09334794183572133\n",
      "Epoch 8/11, Average Loss: 0.07441359013319016\n",
      "Epoch 9/11, Average Loss: 0.0610752689341704\n",
      "Epoch 10/11, Average Loss: 0.04783951242764791\n",
      "Epoch 11/11, Average Loss: 0.03698788210749626\n",
      "Epoch 1/11, Average Loss: 0.7792736887931824\n",
      "Epoch 2/11, Average Loss: 0.3456977605819702\n",
      "Epoch 3/11, Average Loss: 0.26737284660339355\n",
      "Epoch 4/11, Average Loss: 0.2034689982732137\n",
      "Epoch 5/11, Average Loss: 0.15979325026273727\n",
      "Epoch 6/11, Average Loss: 0.13779528190692267\n",
      "Epoch 7/11, Average Loss: 0.10391720446447532\n",
      "Epoch 8/11, Average Loss: 0.08834355076154073\n",
      "Epoch 9/11, Average Loss: 0.07017391671737035\n",
      "Epoch 10/11, Average Loss: 0.06434432913859685\n",
      "Epoch 11/11, Average Loss: 0.050353785355885826\n",
      "Epoch 1/11, Average Loss: 1.9960979620615642\n",
      "Epoch 2/11, Average Loss: 1.3842296202977498\n",
      "Epoch 3/11, Average Loss: 0.9605052669843038\n",
      "Epoch 4/11, Average Loss: 0.6867952148119608\n",
      "Epoch 5/11, Average Loss: 0.5367890894412994\n",
      "Epoch 6/11, Average Loss: 0.42913541197776794\n",
      "Epoch 7/11, Average Loss: 0.35709963242212933\n",
      "Epoch 8/11, Average Loss: 0.30011167128880817\n",
      "Epoch 9/11, Average Loss: 0.25351036091645557\n",
      "Epoch 10/11, Average Loss: 0.23036742210388184\n",
      "Epoch 11/11, Average Loss: 0.1924192582567533\n",
      "Epoch 1/11, Average Loss: 1.9500853617986043\n",
      "Epoch 2/11, Average Loss: 1.333739201227824\n",
      "Epoch 3/11, Average Loss: 0.8723833163579305\n",
      "Epoch 4/11, Average Loss: 0.623275468746821\n",
      "Epoch 5/11, Average Loss: 0.478609174489975\n",
      "Epoch 6/11, Average Loss: 0.37561124563217163\n",
      "Epoch 7/11, Average Loss: 0.3105591783920924\n",
      "Epoch 8/11, Average Loss: 0.28286759555339813\n",
      "Epoch 9/11, Average Loss: 0.24376322825749716\n",
      "Epoch 10/11, Average Loss: 0.20625982681910196\n",
      "Epoch 11/11, Average Loss: 0.18072517216205597\n",
      "Epoch 1/11, Average Loss: 1.941778580347697\n",
      "Epoch 2/11, Average Loss: 1.3330559730529785\n",
      "Epoch 3/11, Average Loss: 0.8981751600901285\n",
      "Epoch 4/11, Average Loss: 0.6572860876719157\n",
      "Epoch 5/11, Average Loss: 0.5133408308029175\n",
      "Epoch 6/11, Average Loss: 0.4213349223136902\n",
      "Epoch 7/11, Average Loss: 0.35430164138476056\n",
      "Epoch 8/11, Average Loss: 0.307771772146225\n",
      "Epoch 9/11, Average Loss: 0.27343301971753436\n",
      "Epoch 10/11, Average Loss: 0.24657334138949713\n",
      "Epoch 11/11, Average Loss: 0.20556692282358804\n",
      "Epoch 1/11, Average Loss: 1.9753553072611492\n",
      "Epoch 2/11, Average Loss: 1.4071658849716187\n",
      "Epoch 3/11, Average Loss: 0.951719343662262\n",
      "Epoch 4/11, Average Loss: 0.6999891797701517\n",
      "Epoch 5/11, Average Loss: 0.545524795850118\n",
      "Epoch 6/11, Average Loss: 0.4447808464368184\n",
      "Epoch 7/11, Average Loss: 0.3713124692440033\n",
      "Epoch 8/11, Average Loss: 0.3095259517431259\n",
      "Epoch 9/11, Average Loss: 0.26843832929929096\n",
      "Epoch 10/11, Average Loss: 0.2295957406361898\n",
      "Epoch 11/11, Average Loss: 0.2027758558591207\n",
      "Epoch 1/11, Average Loss: 1.9083994626998901\n",
      "Epoch 2/11, Average Loss: 1.3088782628377278\n",
      "Epoch 3/11, Average Loss: 0.8699850042661031\n",
      "Epoch 4/11, Average Loss: 0.623587449391683\n",
      "Epoch 5/11, Average Loss: 0.49132808049519855\n",
      "Epoch 6/11, Average Loss: 0.4085402190685272\n",
      "Epoch 7/11, Average Loss: 0.3397783637046814\n",
      "Epoch 8/11, Average Loss: 0.301419273018837\n",
      "Epoch 9/11, Average Loss: 0.2577398767073949\n",
      "Epoch 10/11, Average Loss: 0.2283648500839869\n",
      "Epoch 11/11, Average Loss: 0.19432348509629568\n",
      "Epoch 1/12, Average Loss: 2.0191726088523865\n",
      "Epoch 2/12, Average Loss: 1.5209235350290935\n",
      "Epoch 3/12, Average Loss: 1.1385833124319713\n",
      "Epoch 4/12, Average Loss: 0.8870325088500977\n",
      "Epoch 5/12, Average Loss: 0.6891334652900696\n",
      "Epoch 6/12, Average Loss: 0.6007020572821299\n",
      "Epoch 7/12, Average Loss: 0.49654118220011395\n",
      "Epoch 8/12, Average Loss: 0.4139576504627864\n",
      "Epoch 9/12, Average Loss: 0.3515382334589958\n",
      "Epoch 10/12, Average Loss: 0.35786199072996777\n",
      "Epoch 11/12, Average Loss: 0.2943938796718915\n",
      "Epoch 12/12, Average Loss: 0.26118742674589157\n",
      "Epoch 1/12, Average Loss: 1.9932259321212769\n",
      "Epoch 2/12, Average Loss: 1.4834128022193909\n",
      "Epoch 3/12, Average Loss: 1.0772873957951863\n",
      "Epoch 4/12, Average Loss: 0.802641232808431\n",
      "Epoch 5/12, Average Loss: 0.6278010408083597\n",
      "Epoch 6/12, Average Loss: 0.5292218377192816\n",
      "Epoch 7/12, Average Loss: 0.44499700764815014\n",
      "Epoch 8/12, Average Loss: 0.37916166086991626\n",
      "Epoch 9/12, Average Loss: 0.33580269664525986\n",
      "Epoch 10/12, Average Loss: 0.3078308378656705\n",
      "Epoch 11/12, Average Loss: 0.2740398719906807\n",
      "Epoch 12/12, Average Loss: 0.24332831054925919\n",
      "Epoch 1/12, Average Loss: 1.9352704882621765\n",
      "Epoch 2/12, Average Loss: 1.4530408183733623\n",
      "Epoch 3/12, Average Loss: 1.0924835503101349\n",
      "Epoch 4/12, Average Loss: 0.834809402624766\n",
      "Epoch 5/12, Average Loss: 0.6851357569297155\n",
      "Epoch 6/12, Average Loss: 0.5586087057987849\n",
      "Epoch 7/12, Average Loss: 0.4563313201069832\n",
      "Epoch 8/12, Average Loss: 0.41499953468640643\n",
      "Epoch 9/12, Average Loss: 0.3571451778213183\n",
      "Epoch 10/12, Average Loss: 0.3217115302880605\n",
      "Epoch 11/12, Average Loss: 0.28963443636894226\n",
      "Epoch 12/12, Average Loss: 0.29293910413980484\n",
      "Epoch 1/12, Average Loss: 1.9553067286809285\n",
      "Epoch 2/12, Average Loss: 1.5448662241299946\n",
      "Epoch 3/12, Average Loss: 1.1333770751953125\n",
      "Epoch 4/12, Average Loss: 0.9314259787400564\n",
      "Epoch 5/12, Average Loss: 0.7213922441005707\n",
      "Epoch 6/12, Average Loss: 0.5749581207831701\n",
      "Epoch 7/12, Average Loss: 0.5011560420195261\n",
      "Epoch 8/12, Average Loss: 0.4446940024693807\n",
      "Epoch 9/12, Average Loss: 0.382874791820844\n",
      "Epoch 10/12, Average Loss: 0.335058515270551\n",
      "Epoch 11/12, Average Loss: 0.2990219444036484\n",
      "Epoch 12/12, Average Loss: 0.3023591761787732\n",
      "Epoch 1/12, Average Loss: 1.9080243905385335\n",
      "Epoch 2/12, Average Loss: 1.4384685556093852\n",
      "Epoch 3/12, Average Loss: 1.0690861145655315\n",
      "Epoch 4/12, Average Loss: 0.7933124005794525\n",
      "Epoch 5/12, Average Loss: 0.657795230547587\n",
      "Epoch 6/12, Average Loss: 0.5499346901973089\n",
      "Epoch 7/12, Average Loss: 0.4663737416267395\n",
      "Epoch 8/12, Average Loss: 0.384686383108298\n",
      "Epoch 9/12, Average Loss: 0.3412679408987363\n",
      "Epoch 10/12, Average Loss: 0.30568356066942215\n",
      "Epoch 11/12, Average Loss: 0.2835150087873141\n",
      "Epoch 12/12, Average Loss: 0.25024303669730824\n",
      "Epoch 1/12, Average Loss: 0.7770138382911682\n",
      "Epoch 2/12, Average Loss: 0.7135318219661713\n",
      "Epoch 3/12, Average Loss: 0.6667557954788208\n",
      "Epoch 4/12, Average Loss: 0.6339482367038727\n",
      "Epoch 5/12, Average Loss: 0.6171563565731049\n",
      "Epoch 6/12, Average Loss: 0.6241417825222015\n",
      "Epoch 7/12, Average Loss: 0.5704993903636932\n",
      "Epoch 8/12, Average Loss: 0.5708298087120056\n",
      "Epoch 9/12, Average Loss: 0.5760040879249573\n",
      "Epoch 10/12, Average Loss: 0.5636728703975677\n",
      "Epoch 11/12, Average Loss: 0.53003990650177\n",
      "Epoch 12/12, Average Loss: 0.5082947015762329\n",
      "Epoch 1/12, Average Loss: 0.7837757468223572\n",
      "Epoch 2/12, Average Loss: 0.7144463360309601\n",
      "Epoch 3/12, Average Loss: 0.6655480563640594\n",
      "Epoch 4/12, Average Loss: 0.6402327716350555\n",
      "Epoch 5/12, Average Loss: 0.6114223301410675\n",
      "Epoch 6/12, Average Loss: 0.62044557929039\n",
      "Epoch 7/12, Average Loss: 0.6051314771175385\n",
      "Epoch 8/12, Average Loss: 0.5792354047298431\n",
      "Epoch 9/12, Average Loss: 0.5800144076347351\n",
      "Epoch 10/12, Average Loss: 0.5685714483261108\n",
      "Epoch 11/12, Average Loss: 0.5259655117988586\n",
      "Epoch 12/12, Average Loss: 0.5424956977367401\n",
      "Epoch 1/12, Average Loss: 0.780533641576767\n",
      "Epoch 2/12, Average Loss: 0.7118653059005737\n",
      "Epoch 3/12, Average Loss: 0.663503885269165\n",
      "Epoch 4/12, Average Loss: 0.6302566826343536\n",
      "Epoch 5/12, Average Loss: 0.5977277457714081\n",
      "Epoch 6/12, Average Loss: 0.5757564008235931\n",
      "Epoch 7/12, Average Loss: 0.6323359310626984\n",
      "Epoch 8/12, Average Loss: 0.6308593451976776\n",
      "Epoch 9/12, Average Loss: 0.5836247503757477\n",
      "Epoch 10/12, Average Loss: 0.5631160736083984\n",
      "Epoch 11/12, Average Loss: 0.5456311404705048\n",
      "Epoch 12/12, Average Loss: 0.5217391550540924\n",
      "Epoch 1/12, Average Loss: 0.7709770202636719\n",
      "Epoch 2/12, Average Loss: 0.7143788933753967\n",
      "Epoch 3/12, Average Loss: 0.6776774525642395\n",
      "Epoch 4/12, Average Loss: 0.6395725607872009\n",
      "Epoch 5/12, Average Loss: 0.6093750596046448\n",
      "Epoch 6/12, Average Loss: 0.6017955243587494\n",
      "Epoch 7/12, Average Loss: 0.603994756937027\n",
      "Epoch 8/12, Average Loss: 0.6373336315155029\n",
      "Epoch 9/12, Average Loss: 0.5853944420814514\n",
      "Epoch 10/12, Average Loss: 0.596669614315033\n",
      "Epoch 11/12, Average Loss: 0.5498809814453125\n",
      "Epoch 12/12, Average Loss: 0.542308509349823\n",
      "Epoch 1/12, Average Loss: 0.776835024356842\n",
      "Epoch 2/12, Average Loss: 0.7167386412620544\n",
      "Epoch 3/12, Average Loss: 0.6749159395694733\n",
      "Epoch 4/12, Average Loss: 0.6520516872406006\n",
      "Epoch 5/12, Average Loss: 0.6119314134120941\n",
      "Epoch 6/12, Average Loss: 0.5997008979320526\n",
      "Epoch 7/12, Average Loss: 0.5880426466464996\n",
      "Epoch 8/12, Average Loss: 0.5555224120616913\n",
      "Epoch 9/12, Average Loss: 0.568245530128479\n",
      "Epoch 10/12, Average Loss: 0.5642198324203491\n",
      "Epoch 11/12, Average Loss: 0.5467933714389801\n",
      "Epoch 12/12, Average Loss: 0.5441568493843079\n",
      "Epoch 1/11, Average Loss: 0.613059530655543\n",
      "Epoch 2/11, Average Loss: 0.5358451108137766\n",
      "Epoch 3/11, Average Loss: 0.4598514288663864\n",
      "Epoch 4/11, Average Loss: 0.3601762304703395\n",
      "Epoch 5/11, Average Loss: 0.29243797312180203\n",
      "Epoch 6/11, Average Loss: 0.25103552142779034\n",
      "Epoch 7/11, Average Loss: 0.22529274970293045\n",
      "Epoch 8/11, Average Loss: 0.18215351675947508\n",
      "Epoch 9/11, Average Loss: 0.18262320011854172\n",
      "Epoch 10/11, Average Loss: 0.15294338266054788\n",
      "Epoch 11/11, Average Loss: 0.12573139059046903\n",
      "Epoch 1/11, Average Loss: 0.6121308505535126\n",
      "Epoch 2/11, Average Loss: 0.5296333829561869\n",
      "Epoch 3/11, Average Loss: 0.43811101218064624\n",
      "Epoch 4/11, Average Loss: 0.33750852942466736\n",
      "Epoch 5/11, Average Loss: 0.25434760004282\n",
      "Epoch 6/11, Average Loss: 0.2116698995232582\n",
      "Epoch 7/11, Average Loss: 0.1963883489370346\n",
      "Epoch 8/11, Average Loss: 0.16498245919744173\n",
      "Epoch 9/11, Average Loss: 0.14250723520914713\n",
      "Epoch 10/11, Average Loss: 0.12078561509648959\n",
      "Epoch 11/11, Average Loss: 0.10701120582719643\n",
      "Epoch 1/11, Average Loss: 0.6204270621140798\n",
      "Epoch 2/11, Average Loss: 0.5484822044769923\n",
      "Epoch 3/11, Average Loss: 0.48199525475502014\n",
      "Epoch 4/11, Average Loss: 0.3949901858965556\n",
      "Epoch 5/11, Average Loss: 0.3310677210489909\n",
      "Epoch 6/11, Average Loss: 0.261471023162206\n",
      "Epoch 7/11, Average Loss: 0.23307346304257712\n",
      "Epoch 8/11, Average Loss: 0.19663502524296442\n",
      "Epoch 9/11, Average Loss: 0.18273256594936052\n",
      "Epoch 10/11, Average Loss: 0.1696292037765185\n",
      "Epoch 11/11, Average Loss: 0.15877877672513327\n",
      "Epoch 1/11, Average Loss: 0.6013014217217764\n",
      "Epoch 2/11, Average Loss: 0.5322672973076502\n",
      "Epoch 3/11, Average Loss: 0.44701676567395526\n",
      "Epoch 4/11, Average Loss: 0.36120633284250897\n",
      "Epoch 5/11, Average Loss: 0.29868140319983166\n",
      "Epoch 6/11, Average Loss: 0.25867679963509244\n",
      "Epoch 7/11, Average Loss: 0.21219408760468164\n",
      "Epoch 8/11, Average Loss: 0.19975698987642923\n",
      "Epoch 9/11, Average Loss: 0.16388181348641714\n",
      "Epoch 10/11, Average Loss: 0.1497716767092546\n",
      "Epoch 11/11, Average Loss: 0.12341628782451153\n",
      "Epoch 1/11, Average Loss: 0.6093387802441915\n",
      "Epoch 2/11, Average Loss: 0.5197204003731409\n",
      "Epoch 3/11, Average Loss: 0.4399046301841736\n",
      "Epoch 4/11, Average Loss: 0.3576415131489436\n",
      "Epoch 5/11, Average Loss: 0.2926005894939105\n",
      "Epoch 6/11, Average Loss: 0.2651725709438324\n",
      "Epoch 7/11, Average Loss: 0.2262917459011078\n",
      "Epoch 8/11, Average Loss: 0.1925105464955171\n",
      "Epoch 9/11, Average Loss: 0.19112665578722954\n",
      "Epoch 10/11, Average Loss: 0.1582446520527204\n",
      "Epoch 11/11, Average Loss: 0.12697527185082436\n",
      "Epoch 1/12, Average Loss: 0.8880046904087067\n",
      "Epoch 2/12, Average Loss: 0.4095078855752945\n",
      "Epoch 3/12, Average Loss: 0.24871879816055298\n",
      "Epoch 4/12, Average Loss: 0.2030520960688591\n",
      "Epoch 5/12, Average Loss: 0.17424394935369492\n",
      "Epoch 6/12, Average Loss: 0.142388466745615\n",
      "Epoch 7/12, Average Loss: 0.08793363347649574\n",
      "Epoch 8/12, Average Loss: 0.07971271872520447\n",
      "Epoch 9/12, Average Loss: 0.06190638430416584\n",
      "Epoch 10/12, Average Loss: 0.05042628012597561\n",
      "Epoch 11/12, Average Loss: 0.042656198143959045\n",
      "Epoch 12/12, Average Loss: 0.052950482815504074\n",
      "Epoch 1/12, Average Loss: 0.9150809645652771\n",
      "Epoch 2/12, Average Loss: 0.3825187683105469\n",
      "Epoch 3/12, Average Loss: 0.2353353053331375\n",
      "Epoch 4/12, Average Loss: 0.20281288772821426\n",
      "Epoch 5/12, Average Loss: 0.1394154205918312\n",
      "Epoch 6/12, Average Loss: 0.10412519797682762\n",
      "Epoch 7/12, Average Loss: 0.08581339195370674\n",
      "Epoch 8/12, Average Loss: 0.059129079803824425\n",
      "Epoch 9/12, Average Loss: 0.052687251940369606\n",
      "Epoch 10/12, Average Loss: 0.05034136772155762\n",
      "Epoch 11/12, Average Loss: 0.036085641011595726\n",
      "Epoch 12/12, Average Loss: 0.03530183620750904\n",
      "Epoch 1/12, Average Loss: 0.8539392948150635\n",
      "Epoch 2/12, Average Loss: 0.3683184087276459\n",
      "Epoch 3/12, Average Loss: 0.234090194106102\n",
      "Epoch 4/12, Average Loss: 0.2124209851026535\n",
      "Epoch 5/12, Average Loss: 0.16550154238939285\n",
      "Epoch 6/12, Average Loss: 0.09855388104915619\n",
      "Epoch 7/12, Average Loss: 0.11327334120869637\n",
      "Epoch 8/12, Average Loss: 0.08481300994753838\n",
      "Epoch 9/12, Average Loss: 0.06791752576828003\n",
      "Epoch 10/12, Average Loss: 0.06383541971445084\n",
      "Epoch 11/12, Average Loss: 0.04117485787719488\n",
      "Epoch 12/12, Average Loss: 0.04440639726817608\n",
      "Epoch 1/12, Average Loss: 0.8732750713825226\n",
      "Epoch 2/12, Average Loss: 0.39531750977039337\n",
      "Epoch 3/12, Average Loss: 0.28272169828414917\n",
      "Epoch 4/12, Average Loss: 0.19639647752046585\n",
      "Epoch 5/12, Average Loss: 0.14349231123924255\n",
      "Epoch 6/12, Average Loss: 0.10703350976109505\n",
      "Epoch 7/12, Average Loss: 0.07629035785794258\n",
      "Epoch 8/12, Average Loss: 0.07553176954388618\n",
      "Epoch 9/12, Average Loss: 0.05356354545801878\n",
      "Epoch 10/12, Average Loss: 0.050520941615104675\n",
      "Epoch 11/12, Average Loss: 0.04212656058371067\n",
      "Epoch 12/12, Average Loss: 0.041424624621868134\n",
      "Epoch 1/12, Average Loss: 0.8967679440975189\n",
      "Epoch 2/12, Average Loss: 0.40022046864032745\n",
      "Epoch 3/12, Average Loss: 0.2928282767534256\n",
      "Epoch 4/12, Average Loss: 0.19865287840366364\n",
      "Epoch 5/12, Average Loss: 0.11668933741748333\n",
      "Epoch 6/12, Average Loss: 0.11965212970972061\n",
      "Epoch 7/12, Average Loss: 0.07281477190554142\n",
      "Epoch 8/12, Average Loss: 0.06770321168005466\n",
      "Epoch 9/12, Average Loss: 0.05812809243798256\n",
      "Epoch 10/12, Average Loss: 0.04799501411616802\n",
      "Epoch 11/12, Average Loss: 0.04780254326760769\n",
      "Epoch 12/12, Average Loss: 0.03241239581257105\n",
      "Epoch 1/13, Average Loss: 0.6447213192780813\n",
      "Epoch 2/13, Average Loss: 0.3928193747997284\n",
      "Epoch 3/13, Average Loss: 0.2518029386798541\n",
      "Epoch 4/13, Average Loss: 0.169869481275479\n",
      "Epoch 5/13, Average Loss: 0.14200947666540742\n",
      "Epoch 6/13, Average Loss: 0.09806799702346325\n",
      "Epoch 7/13, Average Loss: 0.08015468282004197\n",
      "Epoch 8/13, Average Loss: 0.06508822288985054\n",
      "Epoch 9/13, Average Loss: 0.0598115778217713\n",
      "Epoch 10/13, Average Loss: 0.05659151698152224\n",
      "Epoch 11/13, Average Loss: 0.04168753977864981\n",
      "Epoch 12/13, Average Loss: 0.04395257503104707\n",
      "Epoch 13/13, Average Loss: 0.051646405326512955\n",
      "Epoch 1/13, Average Loss: 0.5700478305419286\n",
      "Epoch 2/13, Average Loss: 0.29942096769809723\n",
      "Epoch 3/13, Average Loss: 0.1841850063453118\n",
      "Epoch 4/13, Average Loss: 0.13941147550940514\n",
      "Epoch 5/13, Average Loss: 0.10240918723866343\n",
      "Epoch 6/13, Average Loss: 0.05887123476713896\n",
      "Epoch 7/13, Average Loss: 0.03819463370988766\n",
      "Epoch 8/13, Average Loss: 0.030579132804026205\n",
      "Epoch 9/13, Average Loss: 0.020376087942471106\n",
      "Epoch 10/13, Average Loss: 0.014488242411365112\n",
      "Epoch 11/13, Average Loss: 0.010475897463038564\n",
      "Epoch 12/13, Average Loss: 0.01082042270960907\n",
      "Epoch 13/13, Average Loss: 0.006846950816301008\n",
      "Epoch 1/13, Average Loss: 0.5313044041395187\n",
      "Epoch 2/13, Average Loss: 0.33526986092329025\n",
      "Epoch 3/13, Average Loss: 0.20854318141937256\n",
      "Epoch 4/13, Average Loss: 0.20721633483966193\n",
      "Epoch 5/13, Average Loss: 0.11954302107915282\n",
      "Epoch 6/13, Average Loss: 0.09831758836905162\n",
      "Epoch 7/13, Average Loss: 0.07360197448482116\n",
      "Epoch 8/13, Average Loss: 0.0676848712998132\n",
      "Epoch 9/13, Average Loss: 0.05632898801316818\n",
      "Epoch 10/13, Average Loss: 0.058322870483001076\n",
      "Epoch 11/13, Average Loss: 0.05257895107691487\n",
      "Epoch 12/13, Average Loss: 0.048878165039544307\n",
      "Epoch 13/13, Average Loss: 0.04650191396164397\n",
      "Epoch 1/13, Average Loss: 0.5614006022612253\n",
      "Epoch 2/13, Average Loss: 0.3455819810430209\n",
      "Epoch 3/13, Average Loss: 0.20573861338198185\n",
      "Epoch 4/13, Average Loss: 0.15931899100542068\n",
      "Epoch 5/13, Average Loss: 0.09058658468226592\n",
      "Epoch 6/13, Average Loss: 0.07415815660109122\n",
      "Epoch 7/13, Average Loss: 0.06198032262424628\n",
      "Epoch 8/13, Average Loss: 0.04769829070816437\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 43\u001b[0m\n\u001b[1;32m     29\u001b[0m param_ranges \u001b[39m=\u001b[39m {\n\u001b[1;32m     30\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mmin_hidden\u001b[39m\u001b[39m'\u001b[39m: \u001b[39m5\u001b[39m,\n\u001b[1;32m     31\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mmax_hidden\u001b[39m\u001b[39m'\u001b[39m: \u001b[39m50\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mnum_epochs\u001b[39m\u001b[39m'\u001b[39m: [\u001b[39m5\u001b[39m, \u001b[39m10\u001b[39m, \u001b[39m15\u001b[39m]\n\u001b[1;32m     39\u001b[0m }\n\u001b[1;32m     41\u001b[0m num_iterations \u001b[39m=\u001b[39m \u001b[39m50\u001b[39m\n\u001b[0;32m---> 43\u001b[0m best_combination, best_f1, results_df_local \u001b[39m=\u001b[39m local_search_cv(\n\u001b[1;32m     44\u001b[0m     num_iterations, initial_configuration, param_ranges, data_loader, NumbOfClasses, k_folds\u001b[39m=\u001b[39;49m\u001b[39m5\u001b[39;49m, use_scaling\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m\n\u001b[1;32m     45\u001b[0m )\n",
      "Cell \u001b[0;32mIn[14], line 48\u001b[0m, in \u001b[0;36mlocal_search_cv\u001b[0;34m(num_iterations, initial_configuration, param_ranges, train_loader, NumbOfClasses, k_folds, use_scaling)\u001b[0m\n\u001b[1;32m     45\u001b[0m     X_train_scaled, X_test_scaled \u001b[39m=\u001b[39m X_train, X_test\n\u001b[1;32m     47\u001b[0m train_start_time \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime()        \n\u001b[0;32m---> 48\u001b[0m train_model(model, DataLoader(TensorDataset(X_train_scaled, y_train), batch_size\u001b[39m=\u001b[39;49mnew_configuration[\u001b[39m'\u001b[39;49m\u001b[39mBatch Size\u001b[39;49m\u001b[39m'\u001b[39;49m], shuffle\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m), optimizer, criterion, new_configuration[\u001b[39m'\u001b[39;49m\u001b[39mNumber of Epochs\u001b[39;49m\u001b[39m'\u001b[39;49m])\n\u001b[1;32m     49\u001b[0m train_end_time \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime()\n\u001b[1;32m     50\u001b[0m train_time \u001b[39m=\u001b[39m train_end_time \u001b[39m-\u001b[39m train_start_time\n",
      "File \u001b[0;32m~/MachineLearning23ws/Exercise2/nn_implementation.py:65\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(model, train_loader, optimizer, criterion, num_epochs)\u001b[0m\n\u001b[1;32m     62\u001b[0m loss \u001b[39m=\u001b[39m criterion(scores, targets)\n\u001b[1;32m     64\u001b[0m optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[0;32m---> 65\u001b[0m loss\u001b[39m.\u001b[39;49mbackward()\n\u001b[1;32m     66\u001b[0m optimizer\u001b[39m.\u001b[39mstep()\n\u001b[1;32m     68\u001b[0m total_loss \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m loss\u001b[39m.\u001b[39mitem()  \u001b[39m# Accumulate the batch loss\u001b[39;00m\n",
      "File \u001b[0;32m~/MachineLearning23ws/.venv/lib/python3.11/site-packages/torch/_tensor.py:492\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    482\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[1;32m    483\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    484\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[1;32m    485\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    490\u001b[0m         inputs\u001b[39m=\u001b[39minputs,\n\u001b[1;32m    491\u001b[0m     )\n\u001b[0;32m--> 492\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\n\u001b[1;32m    493\u001b[0m     \u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs\n\u001b[1;32m    494\u001b[0m )\n",
      "File \u001b[0;32m~/MachineLearning23ws/.venv/lib/python3.11/site-packages/torch/autograd/__init__.py:244\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    235\u001b[0m inputs \u001b[39m=\u001b[39m (\n\u001b[1;32m    236\u001b[0m     (inputs,)\n\u001b[1;32m    237\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(inputs, torch\u001b[39m.\u001b[39mTensor)\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    240\u001b[0m     \u001b[39melse\u001b[39;00m \u001b[39mtuple\u001b[39m()\n\u001b[1;32m    241\u001b[0m )\n\u001b[1;32m    243\u001b[0m grad_tensors_ \u001b[39m=\u001b[39m _tensor_or_tensors_to_tuple(grad_tensors, \u001b[39mlen\u001b[39m(tensors))\n\u001b[0;32m--> 244\u001b[0m grad_tensors_ \u001b[39m=\u001b[39m _make_grads(tensors, grad_tensors_, is_grads_batched\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)\n\u001b[1;32m    245\u001b[0m \u001b[39mif\u001b[39;00m retain_graph \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    246\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n",
      "File \u001b[0;32m~/MachineLearning23ws/.venv/lib/python3.11/site-packages/torch/autograd/__init__.py:127\u001b[0m, in \u001b[0;36m_make_grads\u001b[0;34m(outputs, grads, is_grads_batched)\u001b[0m\n\u001b[1;32m    121\u001b[0m         msg \u001b[39m=\u001b[39m (\n\u001b[1;32m    122\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mgrad can be implicitly created only for real scalar outputs\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    123\u001b[0m             \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m but got \u001b[39m\u001b[39m{\u001b[39;00mout\u001b[39m.\u001b[39mdtype\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m    124\u001b[0m         )\n\u001b[1;32m    125\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(msg)\n\u001b[1;32m    126\u001b[0m     new_grads\u001b[39m.\u001b[39mappend(\n\u001b[0;32m--> 127\u001b[0m         torch\u001b[39m.\u001b[39;49mones_like(out, memory_format\u001b[39m=\u001b[39;49mtorch\u001b[39m.\u001b[39;49mpreserve_format)\n\u001b[1;32m    128\u001b[0m     )\n\u001b[1;32m    129\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    130\u001b[0m     new_grads\u001b[39m.\u001b[39mappend(\u001b[39mNone\u001b[39;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "dataset = cong_voting\n",
    "\n",
    "#train_X, train_Y, test_X, test_Y = train_test_split(dataset, \"class\", return_torch=True, DoSmote=smote_in)\n",
    "\n",
    "#train_data = TensorDataset(train_X, train_Y)\n",
    "#train_loader = DataLoader(train_data, batch_size=32, shuffle=False)\n",
    "\n",
    "#test_data = TensorDataset(test_X, test_Y)\n",
    "#test_loader = DataLoader(test_data, batch_size=32, shuffle=False)\n",
    "\n",
    "X = dataset.drop([\"class\"], axis=1).values\n",
    "Y = dataset[\"class\"].values\n",
    "data = TensorDataset(torch.tensor(X), torch.tensor(Y))\n",
    "data_loader = DataLoader(data, batch_size=32, shuffle=False)\n",
    "\n",
    "input_size = X.shape[1]\n",
    "\n",
    "\n",
    "NumbOfClasses = 2\n",
    "\n",
    "initial_configuration = {\n",
    "    'Hidden Layer Sizes': [25, 30],\n",
    "    'Activation Function': F.relu,\n",
    "    'Learning Rate': 0.001,\n",
    "    'Batch Size': 64,\n",
    "    'Number of Epochs': 10\n",
    "}\n",
    "\n",
    "param_ranges = {\n",
    "    'min_hidden': 5,\n",
    "    'max_hidden': 50,\n",
    "    'min_layers': 1,\n",
    "    'max_layers': 3,\n",
    "    'activation_functions': [F.relu, F.tanh, F.sigmoid],\n",
    "    'min_lr': 0.001,\n",
    "    'max_lr': 0.1,\n",
    "    'batch_sizes': [32, 64, 128],\n",
    "    'num_epochs': [5, 10, 15]\n",
    "}\n",
    "\n",
    "num_iterations = 50\n",
    "\n",
    "best_combination, best_f1, results_df_local = local_search_cv(\n",
    "    num_iterations, initial_configuration, param_ranges, data_loader, NumbOfClasses, k_folds=5, use_scaling=True\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Combination: {'Hidden Layer Sizes': [22, 28], 'Activation Function': <function tanh at 0x000001CA8817AD40>, 'Learning Rate': 0.011267827629393996, 'Batch Size': 128, 'Number of Epochs': 10}\n",
      "Best Accuracy: tensor(1.)\n"
     ]
    }
   ],
   "source": [
    "print(\"Best Combination:\", best_combination)\n",
    "print(\"Best F1:\", best_f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Hidden Layer Sizes</th>\n",
       "      <th>Activation Function</th>\n",
       "      <th>Learning Rate</th>\n",
       "      <th>Batch Size</th>\n",
       "      <th>Number of Epochs</th>\n",
       "      <th>Average Accuracy</th>\n",
       "      <th>Average Training Time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[24, 29]</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>64</td>\n",
       "      <td>11</td>\n",
       "      <td>0.558824</td>\n",
       "      <td>0.065888</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[23, 29]</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>32</td>\n",
       "      <td>10</td>\n",
       "      <td>0.558824</td>\n",
       "      <td>0.070072</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[23, 29]</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>128</td>\n",
       "      <td>10</td>\n",
       "      <td>0.794118</td>\n",
       "      <td>0.042366</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[23, 28]</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>0.006041</td>\n",
       "      <td>128</td>\n",
       "      <td>11</td>\n",
       "      <td>0.941176</td>\n",
       "      <td>0.046875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[22, 28]</td>\n",
       "      <td>tanh</td>\n",
       "      <td>0.011268</td>\n",
       "      <td>128</td>\n",
       "      <td>10</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.039804</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>[23, 27]</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.014090</td>\n",
       "      <td>64</td>\n",
       "      <td>9</td>\n",
       "      <td>0.911765</td>\n",
       "      <td>0.049102</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>[23, 29]</td>\n",
       "      <td>tanh</td>\n",
       "      <td>0.009778</td>\n",
       "      <td>128</td>\n",
       "      <td>10</td>\n",
       "      <td>0.941176</td>\n",
       "      <td>0.040652</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>[22, 28]</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.007984</td>\n",
       "      <td>64</td>\n",
       "      <td>11</td>\n",
       "      <td>0.970588</td>\n",
       "      <td>0.062218</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>[21, 29]</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>0.014320</td>\n",
       "      <td>128</td>\n",
       "      <td>9</td>\n",
       "      <td>0.852941</td>\n",
       "      <td>0.040927</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>[22, 27]</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>0.004652</td>\n",
       "      <td>32</td>\n",
       "      <td>11</td>\n",
       "      <td>0.882353</td>\n",
       "      <td>0.082060</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>[22, 27]</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>0.009271</td>\n",
       "      <td>64</td>\n",
       "      <td>10</td>\n",
       "      <td>0.911765</td>\n",
       "      <td>0.047190</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>[23, 28]</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>0.014716</td>\n",
       "      <td>64</td>\n",
       "      <td>9</td>\n",
       "      <td>0.941176</td>\n",
       "      <td>0.043734</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>[22, 28]</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>0.009573</td>\n",
       "      <td>64</td>\n",
       "      <td>9</td>\n",
       "      <td>0.882353</td>\n",
       "      <td>0.042300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>[21, 28]</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.012899</td>\n",
       "      <td>32</td>\n",
       "      <td>10</td>\n",
       "      <td>0.911765</td>\n",
       "      <td>0.078394</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>[21, 27]</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.014151</td>\n",
       "      <td>128</td>\n",
       "      <td>11</td>\n",
       "      <td>0.911765</td>\n",
       "      <td>0.044001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>[23, 28]</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.004962</td>\n",
       "      <td>32</td>\n",
       "      <td>10</td>\n",
       "      <td>0.911765</td>\n",
       "      <td>0.073014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>[22, 29]</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>0.009639</td>\n",
       "      <td>64</td>\n",
       "      <td>10</td>\n",
       "      <td>0.911765</td>\n",
       "      <td>0.048281</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>[21, 28]</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.001441</td>\n",
       "      <td>64</td>\n",
       "      <td>11</td>\n",
       "      <td>0.970588</td>\n",
       "      <td>0.050500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>[23, 28]</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.004532</td>\n",
       "      <td>32</td>\n",
       "      <td>9</td>\n",
       "      <td>0.941176</td>\n",
       "      <td>0.064363</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>[23, 29]</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.016771</td>\n",
       "      <td>64</td>\n",
       "      <td>9</td>\n",
       "      <td>0.911765</td>\n",
       "      <td>0.043896</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>[21, 28]</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>0.009238</td>\n",
       "      <td>32</td>\n",
       "      <td>10</td>\n",
       "      <td>0.911765</td>\n",
       "      <td>0.082826</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>[22, 28]</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.018675</td>\n",
       "      <td>128</td>\n",
       "      <td>9</td>\n",
       "      <td>0.941176</td>\n",
       "      <td>0.031756</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>[23, 27]</td>\n",
       "      <td>tanh</td>\n",
       "      <td>0.013834</td>\n",
       "      <td>32</td>\n",
       "      <td>9</td>\n",
       "      <td>0.970588</td>\n",
       "      <td>0.057522</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>[21, 28]</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>0.019342</td>\n",
       "      <td>64</td>\n",
       "      <td>11</td>\n",
       "      <td>0.911765</td>\n",
       "      <td>0.055539</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>[21, 29]</td>\n",
       "      <td>tanh</td>\n",
       "      <td>0.008589</td>\n",
       "      <td>128</td>\n",
       "      <td>10</td>\n",
       "      <td>0.911765</td>\n",
       "      <td>0.036676</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>[23, 27]</td>\n",
       "      <td>tanh</td>\n",
       "      <td>0.018398</td>\n",
       "      <td>64</td>\n",
       "      <td>9</td>\n",
       "      <td>0.970588</td>\n",
       "      <td>0.043809</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>[21, 27]</td>\n",
       "      <td>tanh</td>\n",
       "      <td>0.016533</td>\n",
       "      <td>32</td>\n",
       "      <td>11</td>\n",
       "      <td>0.941176</td>\n",
       "      <td>0.077189</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>[22, 29]</td>\n",
       "      <td>tanh</td>\n",
       "      <td>0.019518</td>\n",
       "      <td>64</td>\n",
       "      <td>11</td>\n",
       "      <td>0.941176</td>\n",
       "      <td>0.059051</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>[22, 27]</td>\n",
       "      <td>tanh</td>\n",
       "      <td>0.015912</td>\n",
       "      <td>128</td>\n",
       "      <td>11</td>\n",
       "      <td>0.970588</td>\n",
       "      <td>0.044592</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>[23, 28]</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>0.004399</td>\n",
       "      <td>128</td>\n",
       "      <td>9</td>\n",
       "      <td>0.823529</td>\n",
       "      <td>0.033560</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>[23, 27]</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>0.007977</td>\n",
       "      <td>128</td>\n",
       "      <td>9</td>\n",
       "      <td>0.882353</td>\n",
       "      <td>0.034484</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>[22, 29]</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>0.009166</td>\n",
       "      <td>64</td>\n",
       "      <td>11</td>\n",
       "      <td>0.911765</td>\n",
       "      <td>0.055103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>[21, 28]</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>0.017025</td>\n",
       "      <td>32</td>\n",
       "      <td>10</td>\n",
       "      <td>0.911765</td>\n",
       "      <td>0.078061</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>[23, 27]</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.016106</td>\n",
       "      <td>128</td>\n",
       "      <td>11</td>\n",
       "      <td>0.941176</td>\n",
       "      <td>0.042888</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>[23, 27]</td>\n",
       "      <td>tanh</td>\n",
       "      <td>0.015530</td>\n",
       "      <td>32</td>\n",
       "      <td>9</td>\n",
       "      <td>0.970588</td>\n",
       "      <td>0.066325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>[22, 27]</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>0.014204</td>\n",
       "      <td>32</td>\n",
       "      <td>11</td>\n",
       "      <td>0.941176</td>\n",
       "      <td>0.073894</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>[21, 27]</td>\n",
       "      <td>tanh</td>\n",
       "      <td>0.007110</td>\n",
       "      <td>64</td>\n",
       "      <td>11</td>\n",
       "      <td>0.911765</td>\n",
       "      <td>0.056120</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>[22, 28]</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>0.014896</td>\n",
       "      <td>32</td>\n",
       "      <td>10</td>\n",
       "      <td>0.941176</td>\n",
       "      <td>0.084787</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>[22, 27]</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>0.018438</td>\n",
       "      <td>64</td>\n",
       "      <td>9</td>\n",
       "      <td>0.941176</td>\n",
       "      <td>0.047353</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>[22, 29]</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.011280</td>\n",
       "      <td>128</td>\n",
       "      <td>11</td>\n",
       "      <td>0.941176</td>\n",
       "      <td>0.048502</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>[21, 29]</td>\n",
       "      <td>tanh</td>\n",
       "      <td>0.003854</td>\n",
       "      <td>128</td>\n",
       "      <td>10</td>\n",
       "      <td>0.882353</td>\n",
       "      <td>0.040789</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>[22, 29]</td>\n",
       "      <td>tanh</td>\n",
       "      <td>0.019682</td>\n",
       "      <td>32</td>\n",
       "      <td>10</td>\n",
       "      <td>0.941176</td>\n",
       "      <td>0.083748</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>[21, 28]</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.008206</td>\n",
       "      <td>32</td>\n",
       "      <td>11</td>\n",
       "      <td>0.911765</td>\n",
       "      <td>0.087719</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>[23, 27]</td>\n",
       "      <td>tanh</td>\n",
       "      <td>0.016260</td>\n",
       "      <td>32</td>\n",
       "      <td>11</td>\n",
       "      <td>0.970588</td>\n",
       "      <td>0.084284</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>[23, 27]</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.007533</td>\n",
       "      <td>32</td>\n",
       "      <td>9</td>\n",
       "      <td>0.911765</td>\n",
       "      <td>0.065556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>[23, 28]</td>\n",
       "      <td>tanh</td>\n",
       "      <td>0.002901</td>\n",
       "      <td>64</td>\n",
       "      <td>9</td>\n",
       "      <td>0.911765</td>\n",
       "      <td>0.046820</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>[22, 28]</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>0.014849</td>\n",
       "      <td>64</td>\n",
       "      <td>11</td>\n",
       "      <td>0.941176</td>\n",
       "      <td>0.056659</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>[23, 29]</td>\n",
       "      <td>tanh</td>\n",
       "      <td>0.007695</td>\n",
       "      <td>64</td>\n",
       "      <td>11</td>\n",
       "      <td>0.941176</td>\n",
       "      <td>0.050178</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>[23, 28]</td>\n",
       "      <td>tanh</td>\n",
       "      <td>0.010749</td>\n",
       "      <td>64</td>\n",
       "      <td>10</td>\n",
       "      <td>0.970588</td>\n",
       "      <td>0.043502</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>[21, 27]</td>\n",
       "      <td>tanh</td>\n",
       "      <td>0.011365</td>\n",
       "      <td>32</td>\n",
       "      <td>11</td>\n",
       "      <td>0.911765</td>\n",
       "      <td>0.081122</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Hidden Layer Sizes Activation Function  Learning Rate  Batch Size  \\\n",
       "0            [24, 29]             sigmoid       0.001000          64   \n",
       "1            [23, 29]             sigmoid       0.001000          32   \n",
       "2            [23, 29]                relu       0.001000         128   \n",
       "3            [23, 28]             sigmoid       0.006041         128   \n",
       "4            [22, 28]                tanh       0.011268         128   \n",
       "5            [23, 27]                relu       0.014090          64   \n",
       "6            [23, 29]                tanh       0.009778         128   \n",
       "7            [22, 28]                relu       0.007984          64   \n",
       "8            [21, 29]             sigmoid       0.014320         128   \n",
       "9            [22, 27]             sigmoid       0.004652          32   \n",
       "10           [22, 27]             sigmoid       0.009271          64   \n",
       "11           [23, 28]             sigmoid       0.014716          64   \n",
       "12           [22, 28]             sigmoid       0.009573          64   \n",
       "13           [21, 28]                relu       0.012899          32   \n",
       "14           [21, 27]                relu       0.014151         128   \n",
       "15           [23, 28]                relu       0.004962          32   \n",
       "16           [22, 29]             sigmoid       0.009639          64   \n",
       "17           [21, 28]                relu       0.001441          64   \n",
       "18           [23, 28]                relu       0.004532          32   \n",
       "19           [23, 29]                relu       0.016771          64   \n",
       "20           [21, 28]             sigmoid       0.009238          32   \n",
       "21           [22, 28]                relu       0.018675         128   \n",
       "22           [23, 27]                tanh       0.013834          32   \n",
       "23           [21, 28]             sigmoid       0.019342          64   \n",
       "24           [21, 29]                tanh       0.008589         128   \n",
       "25           [23, 27]                tanh       0.018398          64   \n",
       "26           [21, 27]                tanh       0.016533          32   \n",
       "27           [22, 29]                tanh       0.019518          64   \n",
       "28           [22, 27]                tanh       0.015912         128   \n",
       "29           [23, 28]             sigmoid       0.004399         128   \n",
       "30           [23, 27]             sigmoid       0.007977         128   \n",
       "31           [22, 29]             sigmoid       0.009166          64   \n",
       "32           [21, 28]             sigmoid       0.017025          32   \n",
       "33           [23, 27]                relu       0.016106         128   \n",
       "34           [23, 27]                tanh       0.015530          32   \n",
       "35           [22, 27]             sigmoid       0.014204          32   \n",
       "36           [21, 27]                tanh       0.007110          64   \n",
       "37           [22, 28]             sigmoid       0.014896          32   \n",
       "38           [22, 27]             sigmoid       0.018438          64   \n",
       "39           [22, 29]                relu       0.011280         128   \n",
       "40           [21, 29]                tanh       0.003854         128   \n",
       "41           [22, 29]                tanh       0.019682          32   \n",
       "42           [21, 28]                relu       0.008206          32   \n",
       "43           [23, 27]                tanh       0.016260          32   \n",
       "44           [23, 27]                relu       0.007533          32   \n",
       "45           [23, 28]                tanh       0.002901          64   \n",
       "46           [22, 28]             sigmoid       0.014849          64   \n",
       "47           [23, 29]                tanh       0.007695          64   \n",
       "48           [23, 28]                tanh       0.010749          64   \n",
       "49           [21, 27]                tanh       0.011365          32   \n",
       "\n",
       "    Number of Epochs  Average Accuracy  Average Training Time  \n",
       "0                 11          0.558824               0.065888  \n",
       "1                 10          0.558824               0.070072  \n",
       "2                 10          0.794118               0.042366  \n",
       "3                 11          0.941176               0.046875  \n",
       "4                 10          1.000000               0.039804  \n",
       "5                  9          0.911765               0.049102  \n",
       "6                 10          0.941176               0.040652  \n",
       "7                 11          0.970588               0.062218  \n",
       "8                  9          0.852941               0.040927  \n",
       "9                 11          0.882353               0.082060  \n",
       "10                10          0.911765               0.047190  \n",
       "11                 9          0.941176               0.043734  \n",
       "12                 9          0.882353               0.042300  \n",
       "13                10          0.911765               0.078394  \n",
       "14                11          0.911765               0.044001  \n",
       "15                10          0.911765               0.073014  \n",
       "16                10          0.911765               0.048281  \n",
       "17                11          0.970588               0.050500  \n",
       "18                 9          0.941176               0.064363  \n",
       "19                 9          0.911765               0.043896  \n",
       "20                10          0.911765               0.082826  \n",
       "21                 9          0.941176               0.031756  \n",
       "22                 9          0.970588               0.057522  \n",
       "23                11          0.911765               0.055539  \n",
       "24                10          0.911765               0.036676  \n",
       "25                 9          0.970588               0.043809  \n",
       "26                11          0.941176               0.077189  \n",
       "27                11          0.941176               0.059051  \n",
       "28                11          0.970588               0.044592  \n",
       "29                 9          0.823529               0.033560  \n",
       "30                 9          0.882353               0.034484  \n",
       "31                11          0.911765               0.055103  \n",
       "32                10          0.911765               0.078061  \n",
       "33                11          0.941176               0.042888  \n",
       "34                 9          0.970588               0.066325  \n",
       "35                11          0.941176               0.073894  \n",
       "36                11          0.911765               0.056120  \n",
       "37                10          0.941176               0.084787  \n",
       "38                 9          0.941176               0.047353  \n",
       "39                11          0.941176               0.048502  \n",
       "40                10          0.882353               0.040789  \n",
       "41                10          0.941176               0.083748  \n",
       "42                11          0.911765               0.087719  \n",
       "43                11          0.970588               0.084284  \n",
       "44                 9          0.911765               0.065556  \n",
       "45                 9          0.911765               0.046820  \n",
       "46                11          0.941176               0.056659  \n",
       "47                11          0.941176               0.050178  \n",
       "48                10          0.970588               0.043502  \n",
       "49                11          0.911765               0.081122  "
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_df_local"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Local search over all datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/9, Average Loss: 1.1833768867276198\n",
      "Epoch 2/9, Average Loss: 1.058368110217931\n",
      "Epoch 3/9, Average Loss: 1.0466419326747123\n",
      "Epoch 4/9, Average Loss: 1.0251545182035013\n",
      "Epoch 5/9, Average Loss: 1.0178479098834874\n",
      "Epoch 6/9, Average Loss: 1.0025088315361117\n",
      "Epoch 7/9, Average Loss: 1.0040231976041034\n",
      "Epoch 8/9, Average Loss: 0.9888390576912581\n",
      "Epoch 9/9, Average Loss: 0.9852977555953651\n",
      "Epoch 1/9, Average Loss: 1.1735128094813576\n",
      "Epoch 2/9, Average Loss: 1.052497517843188\n",
      "Epoch 3/9, Average Loss: 1.0364684700234537\n",
      "Epoch 4/9, Average Loss: 1.0235919052837816\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 55\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     NumbOfClasses \u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(np\u001b[39m.\u001b[39munique(Y))\n\u001b[0;32m---> 55\u001b[0m best_combination, best_f1, results_df_local \u001b[39m=\u001b[39m local_search_cv(\n\u001b[1;32m     56\u001b[0m num_iterations, initial_configuration, param_ranges, data_loader, NumbOfClasses, k_folds\u001b[39m=\u001b[39;49m\u001b[39m5\u001b[39;49m, use_scaling\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n\u001b[1;32m     58\u001b[0m results_df_local[\u001b[39m'\u001b[39m\u001b[39mdataset\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m dataset_name\n\u001b[1;32m     59\u001b[0m all_local_results\u001b[39m.\u001b[39mappend(results_df_local)\n",
      "Cell \u001b[0;32mIn[14], line 48\u001b[0m, in \u001b[0;36mlocal_search_cv\u001b[0;34m(num_iterations, initial_configuration, param_ranges, train_loader, NumbOfClasses, k_folds, use_scaling)\u001b[0m\n\u001b[1;32m     45\u001b[0m     X_train_scaled, X_test_scaled \u001b[39m=\u001b[39m X_train, X_test\n\u001b[1;32m     47\u001b[0m train_start_time \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime()        \n\u001b[0;32m---> 48\u001b[0m train_model(model, DataLoader(TensorDataset(X_train_scaled, y_train), batch_size\u001b[39m=\u001b[39;49mnew_configuration[\u001b[39m'\u001b[39;49m\u001b[39mBatch Size\u001b[39;49m\u001b[39m'\u001b[39;49m], shuffle\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m), optimizer, criterion, new_configuration[\u001b[39m'\u001b[39;49m\u001b[39mNumber of Epochs\u001b[39;49m\u001b[39m'\u001b[39;49m])\n\u001b[1;32m     49\u001b[0m train_end_time \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime()\n\u001b[1;32m     50\u001b[0m train_time \u001b[39m=\u001b[39m train_end_time \u001b[39m-\u001b[39m train_start_time\n",
      "File \u001b[0;32m~/MachineLearning23ws/Exercise2/nn_implementation.py:66\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(model, train_loader, optimizer, criterion, num_epochs)\u001b[0m\n\u001b[1;32m     64\u001b[0m     optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[1;32m     65\u001b[0m     loss\u001b[39m.\u001b[39mbackward()\n\u001b[0;32m---> 66\u001b[0m     optimizer\u001b[39m.\u001b[39;49mstep()\n\u001b[1;32m     68\u001b[0m     total_loss \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m loss\u001b[39m.\u001b[39mitem()  \u001b[39m# Accumulate the batch loss\u001b[39;00m\n\u001b[1;32m     70\u001b[0m average_loss \u001b[39m=\u001b[39m total_loss \u001b[39m/\u001b[39m num_batches\n",
      "File \u001b[0;32m~/MachineLearning23ws/.venv/lib/python3.11/site-packages/torch/optim/optimizer.py:373\u001b[0m, in \u001b[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    368\u001b[0m         \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    369\u001b[0m             \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\n\u001b[1;32m    370\u001b[0m                 \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mfunc\u001b[39m}\u001b[39;00m\u001b[39m must return None or a tuple of (new_args, new_kwargs), but got \u001b[39m\u001b[39m{\u001b[39;00mresult\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    371\u001b[0m             )\n\u001b[0;32m--> 373\u001b[0m out \u001b[39m=\u001b[39m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    374\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_optimizer_step_code()\n\u001b[1;32m    376\u001b[0m \u001b[39m# call optimizer step post hooks\u001b[39;00m\n",
      "File \u001b[0;32m~/MachineLearning23ws/.venv/lib/python3.11/site-packages/torch/optim/optimizer.py:76\u001b[0m, in \u001b[0;36m_use_grad_for_differentiable.<locals>._use_grad\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     74\u001b[0m     torch\u001b[39m.\u001b[39mset_grad_enabled(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdefaults[\u001b[39m'\u001b[39m\u001b[39mdifferentiable\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[1;32m     75\u001b[0m     torch\u001b[39m.\u001b[39m_dynamo\u001b[39m.\u001b[39mgraph_break()\n\u001b[0;32m---> 76\u001b[0m     ret \u001b[39m=\u001b[39m func(\u001b[39mself\u001b[39;49m, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     77\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m     78\u001b[0m     torch\u001b[39m.\u001b[39m_dynamo\u001b[39m.\u001b[39mgraph_break()\n",
      "File \u001b[0;32m~/MachineLearning23ws/.venv/lib/python3.11/site-packages/torch/optim/adam.py:163\u001b[0m, in \u001b[0;36mAdam.step\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    152\u001b[0m     beta1, beta2 \u001b[39m=\u001b[39m group[\u001b[39m'\u001b[39m\u001b[39mbetas\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[1;32m    154\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_init_group(\n\u001b[1;32m    155\u001b[0m         group,\n\u001b[1;32m    156\u001b[0m         params_with_grad,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    160\u001b[0m         max_exp_avg_sqs,\n\u001b[1;32m    161\u001b[0m         state_steps)\n\u001b[0;32m--> 163\u001b[0m     adam(\n\u001b[1;32m    164\u001b[0m         params_with_grad,\n\u001b[1;32m    165\u001b[0m         grads,\n\u001b[1;32m    166\u001b[0m         exp_avgs,\n\u001b[1;32m    167\u001b[0m         exp_avg_sqs,\n\u001b[1;32m    168\u001b[0m         max_exp_avg_sqs,\n\u001b[1;32m    169\u001b[0m         state_steps,\n\u001b[1;32m    170\u001b[0m         amsgrad\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mamsgrad\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m    171\u001b[0m         beta1\u001b[39m=\u001b[39;49mbeta1,\n\u001b[1;32m    172\u001b[0m         beta2\u001b[39m=\u001b[39;49mbeta2,\n\u001b[1;32m    173\u001b[0m         lr\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mlr\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m    174\u001b[0m         weight_decay\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mweight_decay\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m    175\u001b[0m         eps\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39meps\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m    176\u001b[0m         maximize\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mmaximize\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m    177\u001b[0m         foreach\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mforeach\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m    178\u001b[0m         capturable\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mcapturable\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m    179\u001b[0m         differentiable\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mdifferentiable\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m    180\u001b[0m         fused\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mfused\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m    181\u001b[0m         grad_scale\u001b[39m=\u001b[39;49m\u001b[39mgetattr\u001b[39;49m(\u001b[39mself\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mgrad_scale\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m),\n\u001b[1;32m    182\u001b[0m         found_inf\u001b[39m=\u001b[39;49m\u001b[39mgetattr\u001b[39;49m(\u001b[39mself\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mfound_inf\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m),\n\u001b[1;32m    183\u001b[0m     )\n\u001b[1;32m    185\u001b[0m \u001b[39mreturn\u001b[39;00m loss\n",
      "File \u001b[0;32m~/MachineLearning23ws/.venv/lib/python3.11/site-packages/torch/optim/adam.py:311\u001b[0m, in \u001b[0;36madam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, differentiable, fused, grad_scale, found_inf, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[0m\n\u001b[1;32m    308\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    309\u001b[0m     func \u001b[39m=\u001b[39m _single_tensor_adam\n\u001b[0;32m--> 311\u001b[0m func(params,\n\u001b[1;32m    312\u001b[0m      grads,\n\u001b[1;32m    313\u001b[0m      exp_avgs,\n\u001b[1;32m    314\u001b[0m      exp_avg_sqs,\n\u001b[1;32m    315\u001b[0m      max_exp_avg_sqs,\n\u001b[1;32m    316\u001b[0m      state_steps,\n\u001b[1;32m    317\u001b[0m      amsgrad\u001b[39m=\u001b[39;49mamsgrad,\n\u001b[1;32m    318\u001b[0m      beta1\u001b[39m=\u001b[39;49mbeta1,\n\u001b[1;32m    319\u001b[0m      beta2\u001b[39m=\u001b[39;49mbeta2,\n\u001b[1;32m    320\u001b[0m      lr\u001b[39m=\u001b[39;49mlr,\n\u001b[1;32m    321\u001b[0m      weight_decay\u001b[39m=\u001b[39;49mweight_decay,\n\u001b[1;32m    322\u001b[0m      eps\u001b[39m=\u001b[39;49meps,\n\u001b[1;32m    323\u001b[0m      maximize\u001b[39m=\u001b[39;49mmaximize,\n\u001b[1;32m    324\u001b[0m      capturable\u001b[39m=\u001b[39;49mcapturable,\n\u001b[1;32m    325\u001b[0m      differentiable\u001b[39m=\u001b[39;49mdifferentiable,\n\u001b[1;32m    326\u001b[0m      grad_scale\u001b[39m=\u001b[39;49mgrad_scale,\n\u001b[1;32m    327\u001b[0m      found_inf\u001b[39m=\u001b[39;49mfound_inf)\n",
      "File \u001b[0;32m~/MachineLearning23ws/.venv/lib/python3.11/site-packages/torch/optim/adam.py:434\u001b[0m, in \u001b[0;36m_single_tensor_adam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, grad_scale, found_inf, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize, capturable, differentiable)\u001b[0m\n\u001b[1;32m    431\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    432\u001b[0m         denom \u001b[39m=\u001b[39m (exp_avg_sq\u001b[39m.\u001b[39msqrt() \u001b[39m/\u001b[39m bias_correction2_sqrt)\u001b[39m.\u001b[39madd_(eps)\n\u001b[0;32m--> 434\u001b[0m     param\u001b[39m.\u001b[39;49maddcdiv_(exp_avg, denom, value\u001b[39m=\u001b[39;49m\u001b[39m-\u001b[39;49mstep_size)\n\u001b[1;32m    436\u001b[0m \u001b[39m# Lastly, switch back to complex view\u001b[39;00m\n\u001b[1;32m    437\u001b[0m \u001b[39mif\u001b[39;00m amsgrad \u001b[39mand\u001b[39;00m torch\u001b[39m.\u001b[39mis_complex(params[i]):\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "datasets = {'wine_quality': wine_quality, 'cong_voting': cong_voting, 'bank_marketing': bank_marketing}\n",
    "\n",
    "initial_configuration = {\n",
    "    'Hidden Layer Sizes': [25, 30],\n",
    "    'Activation Function': F.relu,\n",
    "    'Learning Rate': 0.01,\n",
    "    'Batch Size': 32,\n",
    "    'Number of Epochs': 10\n",
    "}\n",
    "\n",
    "param_ranges = {\n",
    "    'min_hidden': 5,\n",
    "    'max_hidden': 50,\n",
    "    'min_layers': 1,\n",
    "    'max_layers': 3,\n",
    "    'activation_functions': [F.relu, F.tanh, F.sigmoid],\n",
    "    'min_lr': 0.001,\n",
    "    'max_lr': 0.1,\n",
    "    'batch_sizes': [32, 64, 128],\n",
    "    'num_epochs': [5, 10, 15]\n",
    "}\n",
    "\n",
    "num_iterations = 50\n",
    "\n",
    "\n",
    "all_local_results = []\n",
    "\n",
    "for dataset_name, dataset in datasets.items():\n",
    "    if dataset_name == 'wine_quality' or dataset_name == 'bank_marketing':\n",
    "        smote_in = True\n",
    "    else:\n",
    "        smote_in = False\n",
    "    \n",
    "    #train_X, train_Y, test_X, test_Y = train_test_split(dataset, \"class\", return_torch=True, DoSmote=smote_in)\n",
    "\n",
    "    #train_data = TensorDataset(train_X, train_Y)\n",
    "    #train_loader = DataLoader(train_data, batch_size=32, shuffle=False)\n",
    "\n",
    "    #test_data = TensorDataset(test_X, test_Y)\n",
    "    #test_loader = DataLoader(test_data, batch_size=32, shuffle=False)\n",
    "\n",
    "    X = dataset.drop([\"class\"], axis=1).values\n",
    "    Y = dataset[\"class\"].values\n",
    "    data = TensorDataset(torch.tensor(X), torch.tensor(Y))\n",
    "    data_loader = DataLoader(data, batch_size=32, shuffle=False)\n",
    "\n",
    "    input_size = X.shape[1]\n",
    "\n",
    "\n",
    "    if dataset_name == 'wine_quality':\n",
    "        NumbOfClasses = 10 # theoretical number of 'targets' is 10 but in praxis only 7 are present\n",
    "    else:\n",
    "        NumbOfClasses = len(np.unique(Y))\n",
    "\n",
    "    best_combination, best_f1, results_df_local = local_search_cv(\n",
    "    num_iterations, initial_configuration, param_ranges, data_loader, NumbOfClasses, k_folds=5, use_scaling=True)\n",
    "    \n",
    "    results_df_local['dataset'] = dataset_name\n",
    "    all_local_results.append(results_df_local)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_local_results_df = pd.concat(all_local_results, ignore_index=True)\n",
    "all_local_results_df = pd.DataFrame(all_local_results_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Hidden Layer Sizes</th>\n",
       "      <th>Activation Function</th>\n",
       "      <th>Learning Rate</th>\n",
       "      <th>Batch Size</th>\n",
       "      <th>Number of Epochs</th>\n",
       "      <th>Average Accuracy</th>\n",
       "      <th>Average Training Time</th>\n",
       "      <th>dataset</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[25, 31]</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>0.007763</td>\n",
       "      <td>32</td>\n",
       "      <td>10</td>\n",
       "      <td>0.680089</td>\n",
       "      <td>6.519650</td>\n",
       "      <td>wine_quality</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[26, 30]</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.006508</td>\n",
       "      <td>32</td>\n",
       "      <td>10</td>\n",
       "      <td>0.722575</td>\n",
       "      <td>7.076098</td>\n",
       "      <td>wine_quality</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[26, 29]</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>0.010781</td>\n",
       "      <td>128</td>\n",
       "      <td>10</td>\n",
       "      <td>0.652505</td>\n",
       "      <td>2.985102</td>\n",
       "      <td>wine_quality</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[27, 29]</td>\n",
       "      <td>tanh</td>\n",
       "      <td>0.006458</td>\n",
       "      <td>32</td>\n",
       "      <td>9</td>\n",
       "      <td>0.720355</td>\n",
       "      <td>6.217975</td>\n",
       "      <td>wine_quality</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[25, 30]</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>32</td>\n",
       "      <td>9</td>\n",
       "      <td>0.654724</td>\n",
       "      <td>6.108246</td>\n",
       "      <td>wine_quality</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>145</th>\n",
       "      <td>[25, 29]</td>\n",
       "      <td>tanh</td>\n",
       "      <td>0.021298</td>\n",
       "      <td>128</td>\n",
       "      <td>8</td>\n",
       "      <td>0.834742</td>\n",
       "      <td>7.187644</td>\n",
       "      <td>bank_marketing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146</th>\n",
       "      <td>[25, 28]</td>\n",
       "      <td>tanh</td>\n",
       "      <td>0.024133</td>\n",
       "      <td>64</td>\n",
       "      <td>7</td>\n",
       "      <td>0.803143</td>\n",
       "      <td>9.593440</td>\n",
       "      <td>bank_marketing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>147</th>\n",
       "      <td>[27, 29]</td>\n",
       "      <td>tanh</td>\n",
       "      <td>0.022888</td>\n",
       "      <td>128</td>\n",
       "      <td>6</td>\n",
       "      <td>0.819882</td>\n",
       "      <td>5.363683</td>\n",
       "      <td>bank_marketing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148</th>\n",
       "      <td>[27, 27]</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>0.026978</td>\n",
       "      <td>128</td>\n",
       "      <td>8</td>\n",
       "      <td>0.825177</td>\n",
       "      <td>7.696947</td>\n",
       "      <td>bank_marketing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149</th>\n",
       "      <td>[26, 28]</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>0.017588</td>\n",
       "      <td>64</td>\n",
       "      <td>7</td>\n",
       "      <td>0.806303</td>\n",
       "      <td>9.858495</td>\n",
       "      <td>bank_marketing</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>150 rows Ã— 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    Hidden Layer Sizes Activation Function  Learning Rate  Batch Size  \\\n",
       "0             [25, 31]             sigmoid       0.007763          32   \n",
       "1             [26, 30]                relu       0.006508          32   \n",
       "2             [26, 29]             sigmoid       0.010781         128   \n",
       "3             [27, 29]                tanh       0.006458          32   \n",
       "4             [25, 30]                relu       0.001000          32   \n",
       "..                 ...                 ...            ...         ...   \n",
       "145           [25, 29]                tanh       0.021298         128   \n",
       "146           [25, 28]                tanh       0.024133          64   \n",
       "147           [27, 29]                tanh       0.022888         128   \n",
       "148           [27, 27]             sigmoid       0.026978         128   \n",
       "149           [26, 28]             sigmoid       0.017588          64   \n",
       "\n",
       "     Number of Epochs  Average Accuracy  Average Training Time         dataset  \n",
       "0                  10          0.680089               6.519650    wine_quality  \n",
       "1                  10          0.722575               7.076098    wine_quality  \n",
       "2                  10          0.652505               2.985102    wine_quality  \n",
       "3                   9          0.720355               6.217975    wine_quality  \n",
       "4                   9          0.654724               6.108246    wine_quality  \n",
       "..                ...               ...                    ...             ...  \n",
       "145                 8          0.834742               7.187644  bank_marketing  \n",
       "146                 7          0.803143               9.593440  bank_marketing  \n",
       "147                 6          0.819882               5.363683  bank_marketing  \n",
       "148                 8          0.825177               7.696947  bank_marketing  \n",
       "149                 7          0.806303               9.858495  bank_marketing  \n",
       "\n",
       "[150 rows x 8 columns]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_local_results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_local_results_df.to_csv('./results/cv_local_search_results.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Hidden Layer Sizes</th>\n",
       "      <th>Activation Function</th>\n",
       "      <th>Learning Rate</th>\n",
       "      <th>Batch Size</th>\n",
       "      <th>Number of Epochs</th>\n",
       "      <th>Average Accuracy</th>\n",
       "      <th>Average Training Time</th>\n",
       "      <th>dataset</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[24, 30]</td>\n",
       "      <td>tanh</td>\n",
       "      <td>0.016325</td>\n",
       "      <td>64</td>\n",
       "      <td>11</td>\n",
       "      <td>0.739379</td>\n",
       "      <td>4.710157</td>\n",
       "      <td>wine_quality</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[24, 30]</td>\n",
       "      <td>tanh</td>\n",
       "      <td>0.007228</td>\n",
       "      <td>32</td>\n",
       "      <td>11</td>\n",
       "      <td>0.737159</td>\n",
       "      <td>6.833775</td>\n",
       "      <td>wine_quality</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[24, 32]</td>\n",
       "      <td>tanh</td>\n",
       "      <td>0.014946</td>\n",
       "      <td>128</td>\n",
       "      <td>9</td>\n",
       "      <td>0.970588</td>\n",
       "      <td>0.034791</td>\n",
       "      <td>cong_voting</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[25, 32]</td>\n",
       "      <td>tanh</td>\n",
       "      <td>0.009191</td>\n",
       "      <td>64</td>\n",
       "      <td>8</td>\n",
       "      <td>0.970588</td>\n",
       "      <td>0.042795</td>\n",
       "      <td>cong_voting</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[26, 28]</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.020118</td>\n",
       "      <td>128</td>\n",
       "      <td>7</td>\n",
       "      <td>0.850286</td>\n",
       "      <td>6.241501</td>\n",
       "      <td>bank_marketing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>[26, 28]</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.016200</td>\n",
       "      <td>64</td>\n",
       "      <td>8</td>\n",
       "      <td>0.847041</td>\n",
       "      <td>10.612522</td>\n",
       "      <td>bank_marketing</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Hidden Layer Sizes Activation Function  Learning Rate  Batch Size  \\\n",
       "0           [24, 30]                tanh       0.016325          64   \n",
       "1           [24, 30]                tanh       0.007228          32   \n",
       "2           [24, 32]                tanh       0.014946         128   \n",
       "3           [25, 32]                tanh       0.009191          64   \n",
       "4           [26, 28]                relu       0.020118         128   \n",
       "5           [26, 28]                relu       0.016200          64   \n",
       "\n",
       "   Number of Epochs  Average Accuracy  Average Training Time         dataset  \n",
       "0                11          0.739379               4.710157    wine_quality  \n",
       "1                11          0.737159               6.833775    wine_quality  \n",
       "2                 9          0.970588               0.034791     cong_voting  \n",
       "3                 8          0.970588               0.042795     cong_voting  \n",
       "4                 7          0.850286               6.241501  bank_marketing  \n",
       "5                 8          0.847041              10.612522  bank_marketing  "
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_models_rows = []\n",
    "\n",
    "for dataset in all_local_results_df['dataset'].unique():\n",
    "    top_models_rows.extend(all_local_results_df[all_local_results_df['dataset'] == dataset].nlargest(2, 'Average Accuracy').iterrows())\n",
    "\n",
    "top_models_rows_data = [row[1] for row in top_models_rows]\n",
    "\n",
    "top_models_df = pd.DataFrame(top_models_rows_data).reset_index(drop=True)\n",
    "\n",
    "top_models_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
