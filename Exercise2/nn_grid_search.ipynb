{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F  \n",
    "from torch import optim \n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch.nn.init as init\n",
    "import random\n",
    "import time\n",
    "from sklearn.model_selection import KFold\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Class for the Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NN(nn.Module):\n",
    "    def __init__(self, input_size, num_classes, hidden_layer_sizes, activation_function, apply_softmax = False):\n",
    "        super(NN, self).__init__()\n",
    "\n",
    "        seed = 18\n",
    "        torch.manual_seed(seed)\n",
    "\n",
    "        self.activation = activation_function\n",
    "        self.apply_softmax = apply_softmax\n",
    "\n",
    "        # Input layer\n",
    "        self.input_layer = nn.Linear(input_size, hidden_layer_sizes[0])\n",
    "        init.kaiming_uniform_(self.input_layer.weight, mode='fan_in', nonlinearity=activation_function.__name__)\n",
    "\n",
    "\n",
    "        # Hidden layers\n",
    "        self.hidden_layers = nn.ModuleList([\n",
    "            nn.Linear(hidden_layer_sizes[i], hidden_layer_sizes[i + 1])\n",
    "            for i in range(len(hidden_layer_sizes) - 1)\n",
    "        ])\n",
    "\n",
    "        for layer in self.hidden_layers:\n",
    "            init.kaiming_uniform_(layer.weight, mode='fan_in', nonlinearity=activation_function.__name__)\n",
    "\n",
    "\n",
    "        # Output layer\n",
    "        self.output_layer = nn.Linear(hidden_layer_sizes[-1], num_classes)\n",
    "        init.kaiming_uniform_(self.output_layer.weight, mode='fan_in', nonlinearity=activation_function.__name__)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.float()\n",
    "        x = self.activation(self.input_layer(x))\n",
    "\n",
    "        # Process through hidden layers\n",
    "        for layer in self.hidden_layers:\n",
    "            x = self.activation(layer(x))\n",
    "\n",
    "        if self.apply_softmax:\n",
    "            x = F.softmax(self.output_layer(x), dim=1)\n",
    "        else:\n",
    "            x = self.output_layer(x)\n",
    "\n",
    "        return x\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function to train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, optimizer, criterion, num_epochs):\n",
    "    for epoch in range(num_epochs):\n",
    "        total_loss = 0.0  # Initialize total loss for the epoch\n",
    "        num_batches = len(train_loader)\n",
    "\n",
    "        for batch_idx, (data, targets) in enumerate(train_loader):\n",
    "            data = data.reshape(data.shape[0], -1)\n",
    "\n",
    "            scores = model(data)\n",
    "            loss = criterion(scores, targets)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()  # Accumulate the batch loss\n",
    "\n",
    "        average_loss = total_loss / num_batches\n",
    "        print(f\"Epoch {epoch + 1}/{num_epochs}, Average Loss: {average_loss}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function to calculate the accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_accuracy(loader, model):\n",
    "\n",
    "    num_correct = 0\n",
    "    num_samples = 0\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for x, y in loader:\n",
    "\n",
    "            x = x.reshape(x.shape[0], -1)\n",
    "\n",
    "            scores = model(x)\n",
    "            _, predictions = scores.max(1)\n",
    "\n",
    "            num_correct += (predictions == y).sum()\n",
    "            num_samples += predictions.size(0)\n",
    "\n",
    "    model.train()\n",
    "    return num_correct / num_samples\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function to split the data into train and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_split(data: pd.DataFrame, target_label : str, test_size=0.2, return_torch=None, DoSmote = False):\n",
    "        \n",
    "    # split the data into train and test\n",
    "    train = data.sample(frac=(1-test_size),random_state=200)\n",
    "    test = data.drop(train.index)\n",
    "    \n",
    "    # split the train and test into X and Y\n",
    "    train_X = train.drop([target_label], axis=1).values\n",
    "    train_Y = train[target_label].values\n",
    "    if DoSmote == True:\n",
    "        sm = SMOTE(random_state=42, k_neighbors= 3)\n",
    "        train_X, train_Y = sm.fit_resample(train_X, train_Y)\n",
    "    test_X = test.drop([target_label], axis=1).values\n",
    "    test_Y = test[target_label].values\n",
    "    \n",
    "    if return_torch:\n",
    "        train_X = torch.tensor(train_X)\n",
    "        train_Y = torch.tensor(train_Y)\n",
    "        test_X = torch.tensor(test_X)\n",
    "        test_Y = torch.tensor(test_Y)\n",
    "    \n",
    "    return train_X, train_Y, test_X, test_Y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function for Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grid_search_cv(hidden_layer_sizes_list, activation_functions, learning_rates, batch_sizes, num_epochs_list, train_loader, NumbOfClasses, k_folds=5, use_scaling=True):\n",
    "    best_accuracy = 0.0\n",
    "    best_combination = None\n",
    "    results = []\n",
    "\n",
    "    kf = KFold(n_splits=k_folds, shuffle=True, random_state=42)\n",
    "\n",
    "    if use_scaling:\n",
    "        scaler = StandardScaler()\n",
    "        scaler.fit(train_loader.dataset.tensors[0].numpy())\n",
    "\n",
    "    for hidden_layer_sizes in hidden_layer_sizes_list:\n",
    "        for activation_function in activation_functions:\n",
    "            for learning_rate in learning_rates:\n",
    "                for batch_size in batch_sizes:\n",
    "                    for num_epochs in num_epochs_list:\n",
    "\n",
    "                        fold_accuracies = []\n",
    "                        fold_training_times = []\n",
    "\n",
    "                        for train_index, test_index in kf.split(train_loader.dataset):\n",
    "                            X_train, X_test = train_loader.dataset.tensors[0][train_index], train_loader.dataset.tensors[0][test_index]\n",
    "                            y_train, y_test = train_loader.dataset.tensors[1][train_index], train_loader.dataset.tensors[1][test_index]\n",
    "\n",
    "                            model = NN(input_size=train_loader.dataset.tensors[0].shape[1],\n",
    "                                       num_classes=NumbOfClasses,\n",
    "                                       hidden_layer_sizes=hidden_layer_sizes,\n",
    "                                       activation_function=activation_function)\n",
    "                            criterion = nn.CrossEntropyLoss()\n",
    "                            optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "                        \n",
    "                            if use_scaling:\n",
    "                                X_train_scaled = torch.tensor(scaler.transform(X_train.numpy()))\n",
    "                                X_test_scaled = torch.tensor(scaler.transform(X_test.numpy()))\n",
    "                            else:\n",
    "                                X_train_scaled, X_test_scaled = X_train, X_test\n",
    "\n",
    "                            train_start_time = time.time()\n",
    "                            train_model(model, DataLoader(TensorDataset(X_train_scaled, y_train), batch_size=batch_size, shuffle=True), optimizer, criterion, num_epochs)\n",
    "                            train_end_time = time.time()\n",
    "                            fold_training_times.append(train_end_time - train_start_time)\n",
    "\n",
    "                            accuracy_test = check_accuracy(DataLoader(TensorDataset(X_test_scaled, y_test), batch_size=batch_size, shuffle=False), model)\n",
    "                            fold_accuracies.append(accuracy_test.item())\n",
    "\n",
    "                        avg_accuracy = np.mean(fold_accuracies)\n",
    "                        avg_training_time = np.mean(fold_training_times)\n",
    "\n",
    "                        result = {\n",
    "                            'Hidden Layer Sizes': hidden_layer_sizes,\n",
    "                            'Activation Function': activation_function.__name__,\n",
    "                            'Learning Rate': learning_rate,\n",
    "                            'Batch Size': batch_size,\n",
    "                            'Number of Epochs': num_epochs,\n",
    "                            'Average Accuracy': avg_accuracy,\n",
    "                            'Average Training Time': avg_training_time\n",
    "                        }\n",
    "\n",
    "                        results.append(result)\n",
    "\n",
    "                        if avg_accuracy > best_accuracy:\n",
    "                            best_accuracy = avg_accuracy\n",
    "                            best_combination = result\n",
    "\n",
    "    results_df = pd.DataFrame(results)\n",
    "    return results_df, best_accuracy, best_combination"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing the model on the wine quality dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loading the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fixed acidity</th>\n",
       "      <th>volatile acidity</th>\n",
       "      <th>citric acid</th>\n",
       "      <th>residual sugar</th>\n",
       "      <th>chlorides</th>\n",
       "      <th>free sulfur dioxide</th>\n",
       "      <th>total sulfur dioxide</th>\n",
       "      <th>density</th>\n",
       "      <th>pH</th>\n",
       "      <th>sulphates</th>\n",
       "      <th>alcohol</th>\n",
       "      <th>class</th>\n",
       "      <th>wine_type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7.4</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.9</td>\n",
       "      <td>0.076</td>\n",
       "      <td>11.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>0.9978</td>\n",
       "      <td>3.51</td>\n",
       "      <td>0.56</td>\n",
       "      <td>9.4</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7.8</td>\n",
       "      <td>0.88</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2.6</td>\n",
       "      <td>0.098</td>\n",
       "      <td>25.0</td>\n",
       "      <td>67.0</td>\n",
       "      <td>0.9968</td>\n",
       "      <td>3.20</td>\n",
       "      <td>0.68</td>\n",
       "      <td>9.8</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7.8</td>\n",
       "      <td>0.76</td>\n",
       "      <td>0.04</td>\n",
       "      <td>2.3</td>\n",
       "      <td>0.092</td>\n",
       "      <td>15.0</td>\n",
       "      <td>54.0</td>\n",
       "      <td>0.9970</td>\n",
       "      <td>3.26</td>\n",
       "      <td>0.65</td>\n",
       "      <td>9.8</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>11.2</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.56</td>\n",
       "      <td>1.9</td>\n",
       "      <td>0.075</td>\n",
       "      <td>17.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>0.9980</td>\n",
       "      <td>3.16</td>\n",
       "      <td>0.58</td>\n",
       "      <td>9.8</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7.4</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.9</td>\n",
       "      <td>0.076</td>\n",
       "      <td>11.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>0.9978</td>\n",
       "      <td>3.51</td>\n",
       "      <td>0.56</td>\n",
       "      <td>9.4</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   fixed acidity  volatile acidity  citric acid  residual sugar  chlorides  \\\n",
       "0            7.4              0.70         0.00             1.9      0.076   \n",
       "1            7.8              0.88         0.00             2.6      0.098   \n",
       "2            7.8              0.76         0.04             2.3      0.092   \n",
       "3           11.2              0.28         0.56             1.9      0.075   \n",
       "4            7.4              0.70         0.00             1.9      0.076   \n",
       "\n",
       "   free sulfur dioxide  total sulfur dioxide  density    pH  sulphates  \\\n",
       "0                 11.0                  34.0   0.9978  3.51       0.56   \n",
       "1                 25.0                  67.0   0.9968  3.20       0.68   \n",
       "2                 15.0                  54.0   0.9970  3.26       0.65   \n",
       "3                 17.0                  60.0   0.9980  3.16       0.58   \n",
       "4                 11.0                  34.0   0.9978  3.51       0.56   \n",
       "\n",
       "   alcohol  class  wine_type  \n",
       "0      9.4      5          1  \n",
       "1      9.8      5          1  \n",
       "2      9.8      5          1  \n",
       "3      9.8      6          1  \n",
       "4      9.4      5          1  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wine_quality = pd.read_csv('./preprocessed-datasets/wine_quality_prepro.csv', index_col=0)\n",
    "wine_quality.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Splitting the dataset into training and testing sets, converting to PyTorch tensors and creating PyTorch DataLoaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "smote_in = True\n",
    "train_X, train_Y, test_X, test_Y = train_test_split(wine_quality, \"class\", return_torch=True, DoSmote = smote_in)\n",
    "\n",
    "dataset = TensorDataset(train_X, train_Y)\n",
    "train_loader = DataLoader(dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "dataset = TensorDataset(test_X, test_Y)\n",
    "test_loader = DataLoader(dataset, batch_size=32, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating the model, training and testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Average Loss: 1.360640812674075\n",
      "Epoch 2/10, Average Loss: 2.306261344772773\n",
      "Epoch 3/10, Average Loss: 2.420518517726096\n",
      "Epoch 4/10, Average Loss: 2.344191050121728\n",
      "Epoch 5/10, Average Loss: 2.3038992153937516\n",
      "Epoch 6/10, Average Loss: 2.353399555964451\n",
      "Epoch 7/10, Average Loss: 2.239859869013744\n",
      "Epoch 8/10, Average Loss: 2.184582583185279\n",
      "Epoch 9/10, Average Loss: 2.3706629784815596\n",
      "Epoch 10/10, Average Loss: 2.2964973737261776\n",
      "Accuracy on training set: 0.1428571492433548\n",
      "Accuracy on test set: 0.0013071895809844136\n"
     ]
    }
   ],
   "source": [
    "input_size = train_X.shape[1] # number of features in wine quality dataset\n",
    "NumbOfClasses = 10 # 10 classes in wine quality dataset\n",
    "learning_rate = 0.01\n",
    "batch_size = 64\n",
    "num_epochs = 10\n",
    "hidden_layer_sizes = [25,30]\n",
    "activation_function = F.tanh\n",
    "\n",
    "model = NN(input_size=train_X.shape[1], num_classes=NumbOfClasses, hidden_layer_sizes=hidden_layer_sizes, activation_function=activation_function)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "train_model(model, train_loader, optimizer, criterion, num_epochs)\n",
    "\n",
    "print(f\"Accuracy on training set: {check_accuracy(train_loader, model)}\")\n",
    "print(f\"Accuracy on test set: {check_accuracy(test_loader, model)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing the model on the congressional voting dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loading the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>handicapped-infants</th>\n",
       "      <th>water-project-cost-sharing</th>\n",
       "      <th>adoption-of-the-budget-resolution</th>\n",
       "      <th>physician-fee-freeze</th>\n",
       "      <th>el-salvador-aid</th>\n",
       "      <th>religious-groups-in-schools</th>\n",
       "      <th>anti-satellite-test-ban</th>\n",
       "      <th>aid-to-nicaraguan-contras</th>\n",
       "      <th>mx-missile</th>\n",
       "      <th>immigration</th>\n",
       "      <th>synfuels-crporation-cutback</th>\n",
       "      <th>education-spending</th>\n",
       "      <th>superfund-right-to-sue</th>\n",
       "      <th>crime</th>\n",
       "      <th>duty-free-exports</th>\n",
       "      <th>export-administration-act-south-africa</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>140</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>383</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>201</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>297</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>309</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    ID  handicapped-infants  water-project-cost-sharing  \\\n",
       "0  140                  1.0                         0.0   \n",
       "1  383                  1.0                         1.0   \n",
       "2  201                  0.0                         0.0   \n",
       "3  297                  0.0                         0.0   \n",
       "4  309                  0.0                         0.0   \n",
       "\n",
       "   adoption-of-the-budget-resolution  physician-fee-freeze  el-salvador-aid  \\\n",
       "0                                1.0                   0.0              0.0   \n",
       "1                                0.0                   1.0              1.0   \n",
       "2                                1.0                   0.0              0.0   \n",
       "3                                1.0                   1.0              1.0   \n",
       "4                                0.0                   1.0              1.0   \n",
       "\n",
       "   religious-groups-in-schools  anti-satellite-test-ban  \\\n",
       "0                          1.0                      1.0   \n",
       "1                          1.0                      0.0   \n",
       "2                          0.0                      1.0   \n",
       "3                          1.0                      0.0   \n",
       "4                          1.0                      0.0   \n",
       "\n",
       "   aid-to-nicaraguan-contras  mx-missile  immigration  \\\n",
       "0                        1.0         1.0          0.0   \n",
       "1                        0.0         0.0          0.0   \n",
       "2                        1.0         1.0          0.0   \n",
       "3                        0.0         0.0          1.0   \n",
       "4                        0.0         0.0          1.0   \n",
       "\n",
       "   synfuels-crporation-cutback  education-spending  superfund-right-to-sue  \\\n",
       "0                          0.0                 0.0                     0.0   \n",
       "1                          1.0                 0.0                     1.0   \n",
       "2                          0.0                 0.0                     0.0   \n",
       "3                          0.0                 1.0                     1.0   \n",
       "4                          0.0                 1.0                     1.0   \n",
       "\n",
       "   crime  duty-free-exports  export-administration-act-south-africa  class  \n",
       "0    0.0                1.0                                     1.0      1  \n",
       "1    1.0                0.0                                     1.0      1  \n",
       "2    1.0                1.0                                     1.0      1  \n",
       "3    1.0                1.0                                     1.0      0  \n",
       "4    1.0                0.0                                     0.0      0  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cong_voting = pd.read_csv('./preprocessed-datasets/CongressionVoting_prepro.csv')\n",
    "# encode class value democrat as 1 and republican as 0\n",
    "cong_voting['class'] = cong_voting['class'].map({'democrat': 1, 'republican': 0})\n",
    "cong_voting.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Splitting the dataset into training and testing sets, converting to PyTorch tensors and creating PyTorch DataLoaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "smote_in = False\n",
    "train_X, train_Y, test_X, test_Y = train_test_split(cong_voting, \"class\", return_torch=True, DoSmote=smote_in)\n",
    "\n",
    "dataset = TensorDataset(train_X, train_Y)\n",
    "train_loader = DataLoader(dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "dataset = TensorDataset(test_X, test_Y)\n",
    "test_loader = DataLoader(dataset, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating the model, training and testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Average Loss: 0.8203917642434438\n",
      "Epoch 2/10, Average Loss: 0.6987653970718384\n",
      "Epoch 3/10, Average Loss: 0.6630895932515463\n",
      "Epoch 4/10, Average Loss: 0.5717843721310297\n",
      "Epoch 5/10, Average Loss: 0.4445936580499013\n",
      "Epoch 6/10, Average Loss: 0.3994954625765483\n",
      "Epoch 7/10, Average Loss: 0.31182117760181427\n",
      "Epoch 8/10, Average Loss: 0.26471030960480374\n",
      "Epoch 9/10, Average Loss: 0.2563413182894389\n",
      "Epoch 10/10, Average Loss: 0.14053159393370152\n",
      "Accuracy on training set: 0.959770143032074\n",
      "Accuracy on test set: 0.9069767594337463\n"
     ]
    }
   ],
   "source": [
    "input_size = train_X.shape[1] # number of features in congr voting dataset\n",
    "NumbOfClasses = 2 # 2 classes in congr voting dataset\n",
    "learning_rate = 0.01\n",
    "batch_size = 64\n",
    "num_epochs = 10\n",
    "hidden_layer_sizes = [25,30]\n",
    "activation_function = F.tanh\n",
    "\n",
    "model = NN(input_size=train_X.shape[1], num_classes=NumbOfClasses, hidden_layer_sizes=hidden_layer_sizes, activation_function=activation_function)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "train_model(model, train_loader, optimizer, criterion, num_epochs)\n",
    "\n",
    "print(f\"Accuracy on training set: {check_accuracy(train_loader, model)}\")\n",
    "print(f\"Accuracy on test set: {check_accuracy(test_loader, model)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing the model on bank marketing dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loading and preparing dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "bank_marketing = pd.read_csv('./preprocessed-datasets/bank_marketing_prepro.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>default</th>\n",
       "      <th>housing</th>\n",
       "      <th>loan</th>\n",
       "      <th>campaign</th>\n",
       "      <th>pdays</th>\n",
       "      <th>previous</th>\n",
       "      <th>emp.var.rate</th>\n",
       "      <th>cons.price.idx</th>\n",
       "      <th>cons.conf.idx</th>\n",
       "      <th>...</th>\n",
       "      <th>education_basic.9y</th>\n",
       "      <th>education_high.school</th>\n",
       "      <th>education_illiterate</th>\n",
       "      <th>education_professional.course</th>\n",
       "      <th>education_university.degree</th>\n",
       "      <th>education_unknown</th>\n",
       "      <th>poutcome_failure</th>\n",
       "      <th>poutcome_nonexistent</th>\n",
       "      <th>poutcome_success</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>56</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>999</td>\n",
       "      <td>0</td>\n",
       "      <td>1.1</td>\n",
       "      <td>93.994</td>\n",
       "      <td>-36.4</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>57</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>999</td>\n",
       "      <td>0</td>\n",
       "      <td>1.1</td>\n",
       "      <td>93.994</td>\n",
       "      <td>-36.4</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>37</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>999</td>\n",
       "      <td>0</td>\n",
       "      <td>1.1</td>\n",
       "      <td>93.994</td>\n",
       "      <td>-36.4</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>40</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>999</td>\n",
       "      <td>0</td>\n",
       "      <td>1.1</td>\n",
       "      <td>93.994</td>\n",
       "      <td>-36.4</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>56</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>999</td>\n",
       "      <td>0</td>\n",
       "      <td>1.1</td>\n",
       "      <td>93.994</td>\n",
       "      <td>-36.4</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 34 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   age  default  housing  loan  campaign  pdays  previous  emp.var.rate  \\\n",
       "0   56      0.0      0.0   0.0         1    999         0           1.1   \n",
       "1   57      0.0      0.0   0.0         1    999         0           1.1   \n",
       "2   37      0.0      1.0   0.0         1    999         0           1.1   \n",
       "3   40      0.0      0.0   0.0         1    999         0           1.1   \n",
       "4   56      0.0      0.0   1.0         1    999         0           1.1   \n",
       "\n",
       "   cons.price.idx  cons.conf.idx  ...  education_basic.9y  \\\n",
       "0          93.994          -36.4  ...                   0   \n",
       "1          93.994          -36.4  ...                   0   \n",
       "2          93.994          -36.4  ...                   0   \n",
       "3          93.994          -36.4  ...                   0   \n",
       "4          93.994          -36.4  ...                   0   \n",
       "\n",
       "   education_high.school  education_illiterate  education_professional.course  \\\n",
       "0                      0                     0                              0   \n",
       "1                      1                     0                              0   \n",
       "2                      1                     0                              0   \n",
       "3                      0                     0                              0   \n",
       "4                      1                     0                              0   \n",
       "\n",
       "   education_university.degree  education_unknown  poutcome_failure  \\\n",
       "0                            0                  0                 0   \n",
       "1                            0                  0                 0   \n",
       "2                            0                  0                 0   \n",
       "3                            0                  0                 0   \n",
       "4                            0                  0                 0   \n",
       "\n",
       "   poutcome_nonexistent  poutcome_success  class  \n",
       "0                     1                 0      0  \n",
       "1                     1                 0      0  \n",
       "2                     1                 0      0  \n",
       "3                     1                 0      0  \n",
       "4                     1                 0      0  \n",
       "\n",
       "[5 rows x 34 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "column_to_move = 'class'\n",
    "\n",
    "# Move class to the last index\n",
    "columns = [col for col in bank_marketing.columns if col != column_to_move] + [column_to_move]\n",
    "bank_marketing = bank_marketing[columns]\n",
    "\n",
    "bank_marketing.drop('Unnamed: 0', axis=1,inplace=True)\n",
    "\n",
    "bank_marketing.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['age', 'default', 'housing', 'loan', 'campaign', 'pdays', 'previous',\n",
       "       'emp.var.rate', 'cons.price.idx', 'cons.conf.idx', 'euribor3m',\n",
       "       'nr.employed', 'job_blue-collar', 'job_management', 'job_other',\n",
       "       'job_self-employed', 'job_serivces', 'job_technician',\n",
       "       'marital_divorced', 'marital_married', 'marital_single',\n",
       "       'marital_unknown', 'education_basic.4y', 'education_basic.6y',\n",
       "       'education_basic.9y', 'education_high.school', 'education_illiterate',\n",
       "       'education_professional.course', 'education_university.degree',\n",
       "       'education_unknown', 'poutcome_failure', 'poutcome_nonexistent',\n",
       "       'poutcome_success', 'class'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bank_marketing.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Splitting the dataset into training and testing sets, converting to PyTorch tensors and creating PyTorch DataLoaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rebekaangyal/MachineLearning23ws/.venv/lib/python3.11/site-packages/threadpoolctl.py:1010: RuntimeWarning: \n",
      "Found Intel OpenMP ('libiomp') and LLVM OpenMP ('libomp') loaded at\n",
      "the same time. Both libraries are known to be incompatible and this\n",
      "can cause random crashes or deadlocks on Linux when loaded in the\n",
      "same Python program.\n",
      "Using threadpoolctl may cause crashes or deadlocks. For more\n",
      "information and possible workarounds, please see\n",
      "    https://github.com/joblib/threadpoolctl/blob/master/multiple_openmp.md\n",
      "\n",
      "  warnings.warn(msg, RuntimeWarning)\n"
     ]
    }
   ],
   "source": [
    "smote_in = True\n",
    "train_X, train_Y, test_X, test_Y = train_test_split(bank_marketing, \"class\", return_torch=True, DoSmote=smote_in)\n",
    "\n",
    "dataset = TensorDataset(train_X, train_Y)\n",
    "train_loader = DataLoader(dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "dataset = TensorDataset(test_X, test_Y)\n",
    "test_loader = DataLoader(dataset, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating the model, training and testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Average Loss: 0.18898988196431674\n",
      "Epoch 2/10, Average Loss: 0.2269750215684658\n",
      "Epoch 3/10, Average Loss: 0.24861385295906963\n",
      "Epoch 4/10, Average Loss: 0.2453736917733667\n",
      "Epoch 5/10, Average Loss: 0.24322431265023542\n",
      "Epoch 6/10, Average Loss: 0.24812340481806466\n",
      "Epoch 7/10, Average Loss: 0.2489310940900721\n",
      "Epoch 8/10, Average Loss: 0.24892018978450195\n",
      "Epoch 9/10, Average Loss: 0.2489189351923551\n",
      "Epoch 10/10, Average Loss: 0.24891852699443404\n",
      "Accuracy on training set: 0.5\n",
      "Accuracy on test set: 0.11689730733633041\n"
     ]
    }
   ],
   "source": [
    "input_size = train_X.shape[1] \n",
    "NumbOfClasses = 2 \n",
    "learning_rate = 0.01\n",
    "batch_size = 64\n",
    "num_epochs = 10\n",
    "hidden_layer_sizes = [25,30]\n",
    "activation_function = F.tanh\n",
    "\n",
    "model = NN(input_size=train_X.shape[1], num_classes=NumbOfClasses, hidden_layer_sizes=hidden_layer_sizes, activation_function=activation_function)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "train_model(model, train_loader, optimizer, criterion, num_epochs)\n",
    "\n",
    "print(f\"Accuracy on training set: {check_accuracy(train_loader, model)}\")\n",
    "print(f\"Accuracy on test set: {check_accuracy(test_loader, model)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Grid search over all three datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It was not possible for us to iterate over all datasets because of the large amount of runtime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bank Marketing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Average Loss: 0.30895951283209533\n",
      "Epoch 2/10, Average Loss: 0.28735410527696886\n",
      "Epoch 3/10, Average Loss: 0.28536031419791064\n",
      "Epoch 4/10, Average Loss: 0.283694635852448\n",
      "Epoch 5/10, Average Loss: 0.28263048726378137\n",
      "Epoch 6/10, Average Loss: 0.28143955136387094\n",
      "Epoch 7/10, Average Loss: 0.28098883524681756\n",
      "Epoch 8/10, Average Loss: 0.2809191710306603\n",
      "Epoch 9/10, Average Loss: 0.2809496916468861\n",
      "Epoch 10/10, Average Loss: 0.2810058439385544\n",
      "Epoch 1/10, Average Loss: 0.3128876335412553\n",
      "Epoch 2/10, Average Loss: 0.29042269303960705\n",
      "Epoch 3/10, Average Loss: 0.2876893164752756\n",
      "Epoch 4/10, Average Loss: 0.2868161084727176\n",
      "Epoch 5/10, Average Loss: 0.28492116279972407\n",
      "Epoch 6/10, Average Loss: 0.28500315247519503\n",
      "Epoch 7/10, Average Loss: 0.2842394628137061\n",
      "Epoch 8/10, Average Loss: 0.2834020783600298\n",
      "Epoch 9/10, Average Loss: 0.28343992003248736\n",
      "Epoch 10/10, Average Loss: 0.28344538245096945\n",
      "Epoch 1/10, Average Loss: 0.3112315728681759\n",
      "Epoch 2/10, Average Loss: 0.28723683377492776\n",
      "Epoch 3/10, Average Loss: 0.2850546772763567\n",
      "Epoch 4/10, Average Loss: 0.2832905932392889\n",
      "Epoch 5/10, Average Loss: 0.2811239642136305\n",
      "Epoch 6/10, Average Loss: 0.28109776108588985\n",
      "Epoch 7/10, Average Loss: 0.2810682715142815\n",
      "Epoch 8/10, Average Loss: 0.27998942411640315\n",
      "Epoch 9/10, Average Loss: 0.2802820530475922\n",
      "Epoch 10/10, Average Loss: 0.27942993285007844\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 26\u001b[0m\n\u001b[1;32m     22\u001b[0m input_size \u001b[39m=\u001b[39m train_X\u001b[39m.\u001b[39mshape[\u001b[39m1\u001b[39m]\n\u001b[1;32m     24\u001b[0m NumbOfClasses \u001b[39m=\u001b[39m \u001b[39m2\u001b[39m\n\u001b[0;32m---> 26\u001b[0m grid_results_bank, best_accuracy, best_combination \u001b[39m=\u001b[39m grid_search_cv(\n\u001b[1;32m     27\u001b[0m     hidden_layer_sizes_list, activation_functions, learning_rates, batch_sizes, num_epochs_list, data_loader, NumbOfClasses,\n\u001b[1;32m     28\u001b[0m     k_folds\u001b[39m=\u001b[39;49m\u001b[39m5\u001b[39;49m, use_scaling\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n\u001b[1;32m     30\u001b[0m grid_results_bank[\u001b[39m'\u001b[39m\u001b[39mdataset\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mbank_marketing\u001b[39m\u001b[39m'\u001b[39m\n",
      "Cell \u001b[0;32mIn[7], line 40\u001b[0m, in \u001b[0;36mgrid_search_cv\u001b[0;34m(hidden_layer_sizes_list, activation_functions, learning_rates, batch_sizes, num_epochs_list, train_loader, NumbOfClasses, k_folds, use_scaling)\u001b[0m\n\u001b[1;32m     37\u001b[0m     X_train_scaled, X_test_scaled \u001b[39m=\u001b[39m X_train, X_test\n\u001b[1;32m     39\u001b[0m train_start_time \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime()\n\u001b[0;32m---> 40\u001b[0m train_model(model, DataLoader(TensorDataset(X_train_scaled, y_train), batch_size\u001b[39m=\u001b[39;49mbatch_size, shuffle\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m), optimizer, criterion, num_epochs)\n\u001b[1;32m     41\u001b[0m train_end_time \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime()\n\u001b[1;32m     42\u001b[0m fold_training_times\u001b[39m.\u001b[39mappend(train_end_time \u001b[39m-\u001b[39m train_start_time)\n",
      "Cell \u001b[0;32mIn[4], line 6\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(model, train_loader, optimizer, criterion, num_epochs)\u001b[0m\n\u001b[1;32m      3\u001b[0m total_loss \u001b[39m=\u001b[39m \u001b[39m0.0\u001b[39m  \u001b[39m# Initialize total loss for the epoch\u001b[39;00m\n\u001b[1;32m      4\u001b[0m num_batches \u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(train_loader)\n\u001b[0;32m----> 6\u001b[0m \u001b[39mfor\u001b[39;49;00m batch_idx, (data, targets) \u001b[39min\u001b[39;49;00m \u001b[39menumerate\u001b[39;49m(train_loader):\n\u001b[1;32m      7\u001b[0m     data \u001b[39m=\u001b[39;49m data\u001b[39m.\u001b[39;49mreshape(data\u001b[39m.\u001b[39;49mshape[\u001b[39m0\u001b[39;49m], \u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m)\n\u001b[1;32m      9\u001b[0m     scores \u001b[39m=\u001b[39;49m model(data)\n",
      "File \u001b[0;32m~/MachineLearning23ws/.venv/lib/python3.11/site-packages/torch/utils/data/dataloader.py:626\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    625\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__next__\u001b[39m(\u001b[39mself\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Any:\n\u001b[0;32m--> 626\u001b[0m     \u001b[39mwith\u001b[39;49;00m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mprofiler\u001b[39m.\u001b[39;49mrecord_function(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_profile_name):\n\u001b[1;32m    627\u001b[0m         \u001b[39mif\u001b[39;49;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_sampler_iter \u001b[39mis\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m:\n\u001b[1;32m    628\u001b[0m             \u001b[39m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;49;00m\n\u001b[1;32m    629\u001b[0m             \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_reset()  \u001b[39m# type: ignore[call-arg]\u001b[39;49;00m\n",
      "File \u001b[0;32m~/MachineLearning23ws/.venv/lib/python3.11/site-packages/torch/autograd/profiler.py:636\u001b[0m, in \u001b[0;36mrecord_function.__exit__\u001b[0;34m(self, exc_type, exc_value, traceback)\u001b[0m\n\u001b[1;32m    631\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrecord \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mops\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39m_record_function_enter_new(\n\u001b[1;32m    632\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mname, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs\n\u001b[1;32m    633\u001b[0m     )\n\u001b[1;32m    634\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\n\u001b[0;32m--> 636\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__exit__\u001b[39m(\u001b[39mself\u001b[39m, exc_type: Any, exc_value: Any, traceback: Any):\n\u001b[1;32m    637\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrun_callbacks_on_exit:\n\u001b[1;32m    638\u001b[0m         \u001b[39mreturn\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "hidden_layer_sizes_list = [[5],[10],[25, 30], [20, 25, 30]]\n",
    "activation_functions = [F.tanh, F.relu, F.sigmoid]\n",
    "learning_rates = [0.01, 0.001, 0.0001]\n",
    "batch_sizes = [64]\n",
    "num_epochs_list = [10]\n",
    "\n",
    "dataset = bank_marketing\n",
    "#smote_in = True\n",
    "#train_X, train_Y, test_X, test_Y = train_test_split(dataset, \"class\", return_torch=True, DoSmote=smote_in)\n",
    "\n",
    "#train_data = TensorDataset(train_X, train_Y)\n",
    "#train_loader = DataLoader(train_data, batch_size=32, shuffle=False)\n",
    "\n",
    "#test_data = TensorDataset(test_X, test_Y)\n",
    "#test_loader = DataLoader(test_data, batch_size=32, shuffle=False)\n",
    "\n",
    "X = dataset.drop([\"class\"], axis=1).values\n",
    "Y = dataset[\"class\"].values\n",
    "data = TensorDataset(torch.tensor(X), torch.tensor(Y))\n",
    "data_loader = DataLoader(data, batch_size=32, shuffle=False)\n",
    "\n",
    "input_size = X.shape[1]\n",
    "\n",
    "NumbOfClasses = 2\n",
    "\n",
    "grid_results_bank, best_accuracy, best_combination = grid_search_cv(\n",
    "    hidden_layer_sizes_list, activation_functions, learning_rates, batch_sizes, num_epochs_list, data_loader, NumbOfClasses,\n",
    "    k_folds=5, use_scaling=True)\n",
    "\n",
    "grid_results_bank['dataset'] = 'bank_marketing'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Hidden Layer Sizes</th>\n",
       "      <th>Activation Function</th>\n",
       "      <th>Learning Rate</th>\n",
       "      <th>Batch Size</th>\n",
       "      <th>Number of Epochs</th>\n",
       "      <th>Average Accuracy</th>\n",
       "      <th>Average Training Time</th>\n",
       "      <th>dataset</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[5]</td>\n",
       "      <td>tanh</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>64</td>\n",
       "      <td>10</td>\n",
       "      <td>0.795699</td>\n",
       "      <td>12.364878</td>\n",
       "      <td>bank_marketing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[5]</td>\n",
       "      <td>tanh</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>64</td>\n",
       "      <td>10</td>\n",
       "      <td>0.755577</td>\n",
       "      <td>12.158159</td>\n",
       "      <td>bank_marketing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[5]</td>\n",
       "      <td>tanh</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>64</td>\n",
       "      <td>10</td>\n",
       "      <td>0.718461</td>\n",
       "      <td>11.684069</td>\n",
       "      <td>bank_marketing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[5]</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>64</td>\n",
       "      <td>10</td>\n",
       "      <td>0.770642</td>\n",
       "      <td>11.399968</td>\n",
       "      <td>bank_marketing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[5]</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>64</td>\n",
       "      <td>10</td>\n",
       "      <td>0.744679</td>\n",
       "      <td>11.911103</td>\n",
       "      <td>bank_marketing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>[5]</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>64</td>\n",
       "      <td>10</td>\n",
       "      <td>0.731579</td>\n",
       "      <td>11.358967</td>\n",
       "      <td>bank_marketing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>[5]</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>64</td>\n",
       "      <td>10</td>\n",
       "      <td>0.754432</td>\n",
       "      <td>11.386674</td>\n",
       "      <td>bank_marketing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>[5]</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>64</td>\n",
       "      <td>10</td>\n",
       "      <td>0.743364</td>\n",
       "      <td>11.465565</td>\n",
       "      <td>bank_marketing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>[5]</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>64</td>\n",
       "      <td>10</td>\n",
       "      <td>0.735661</td>\n",
       "      <td>11.650078</td>\n",
       "      <td>bank_marketing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>[10]</td>\n",
       "      <td>tanh</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>64</td>\n",
       "      <td>10</td>\n",
       "      <td>0.820842</td>\n",
       "      <td>11.603699</td>\n",
       "      <td>bank_marketing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>[10]</td>\n",
       "      <td>tanh</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>64</td>\n",
       "      <td>10</td>\n",
       "      <td>0.789909</td>\n",
       "      <td>11.624668</td>\n",
       "      <td>bank_marketing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>[10]</td>\n",
       "      <td>tanh</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>64</td>\n",
       "      <td>10</td>\n",
       "      <td>0.739316</td>\n",
       "      <td>12.741981</td>\n",
       "      <td>bank_marketing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>[10]</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>64</td>\n",
       "      <td>10</td>\n",
       "      <td>0.828306</td>\n",
       "      <td>13.050930</td>\n",
       "      <td>bank_marketing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>[10]</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>64</td>\n",
       "      <td>10</td>\n",
       "      <td>0.787347</td>\n",
       "      <td>12.218218</td>\n",
       "      <td>bank_marketing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>[10]</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>64</td>\n",
       "      <td>10</td>\n",
       "      <td>0.743689</td>\n",
       "      <td>12.164853</td>\n",
       "      <td>bank_marketing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>[10]</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>64</td>\n",
       "      <td>10</td>\n",
       "      <td>0.800021</td>\n",
       "      <td>11.437619</td>\n",
       "      <td>bank_marketing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>[10]</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>64</td>\n",
       "      <td>10</td>\n",
       "      <td>0.745875</td>\n",
       "      <td>11.027652</td>\n",
       "      <td>bank_marketing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>[10]</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>64</td>\n",
       "      <td>10</td>\n",
       "      <td>0.737984</td>\n",
       "      <td>11.467428</td>\n",
       "      <td>bank_marketing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>[25, 30]</td>\n",
       "      <td>tanh</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>64</td>\n",
       "      <td>10</td>\n",
       "      <td>0.841219</td>\n",
       "      <td>13.201574</td>\n",
       "      <td>bank_marketing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>[25, 30]</td>\n",
       "      <td>tanh</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>64</td>\n",
       "      <td>10</td>\n",
       "      <td>0.829553</td>\n",
       "      <td>13.100218</td>\n",
       "      <td>bank_marketing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>[25, 30]</td>\n",
       "      <td>tanh</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>64</td>\n",
       "      <td>10</td>\n",
       "      <td>0.742766</td>\n",
       "      <td>13.141413</td>\n",
       "      <td>bank_marketing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>[25, 30]</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>64</td>\n",
       "      <td>10</td>\n",
       "      <td>0.852202</td>\n",
       "      <td>14.773023</td>\n",
       "      <td>bank_marketing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>[25, 30]</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>64</td>\n",
       "      <td>10</td>\n",
       "      <td>0.822550</td>\n",
       "      <td>13.486950</td>\n",
       "      <td>bank_marketing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>[25, 30]</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>64</td>\n",
       "      <td>10</td>\n",
       "      <td>0.748574</td>\n",
       "      <td>13.379348</td>\n",
       "      <td>bank_marketing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>[25, 30]</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>64</td>\n",
       "      <td>10</td>\n",
       "      <td>0.839425</td>\n",
       "      <td>13.288525</td>\n",
       "      <td>bank_marketing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>[25, 30]</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>64</td>\n",
       "      <td>10</td>\n",
       "      <td>0.749838</td>\n",
       "      <td>13.303207</td>\n",
       "      <td>bank_marketing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>[25, 30]</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>64</td>\n",
       "      <td>10</td>\n",
       "      <td>0.740375</td>\n",
       "      <td>287.427189</td>\n",
       "      <td>bank_marketing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>[20, 25, 30]</td>\n",
       "      <td>tanh</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>64</td>\n",
       "      <td>10</td>\n",
       "      <td>0.830287</td>\n",
       "      <td>15.945419</td>\n",
       "      <td>bank_marketing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>[20, 25, 30]</td>\n",
       "      <td>tanh</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>64</td>\n",
       "      <td>10</td>\n",
       "      <td>0.819424</td>\n",
       "      <td>16.959366</td>\n",
       "      <td>bank_marketing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>[20, 25, 30]</td>\n",
       "      <td>tanh</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>64</td>\n",
       "      <td>10</td>\n",
       "      <td>0.741468</td>\n",
       "      <td>17.706601</td>\n",
       "      <td>bank_marketing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>[20, 25, 30]</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>64</td>\n",
       "      <td>10</td>\n",
       "      <td>0.841663</td>\n",
       "      <td>17.163013</td>\n",
       "      <td>bank_marketing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>[20, 25, 30]</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>64</td>\n",
       "      <td>10</td>\n",
       "      <td>0.827725</td>\n",
       "      <td>17.058871</td>\n",
       "      <td>bank_marketing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>[20, 25, 30]</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>64</td>\n",
       "      <td>10</td>\n",
       "      <td>0.747481</td>\n",
       "      <td>17.138449</td>\n",
       "      <td>bank_marketing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>[20, 25, 30]</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>64</td>\n",
       "      <td>10</td>\n",
       "      <td>0.832132</td>\n",
       "      <td>17.476922</td>\n",
       "      <td>bank_marketing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>[20, 25, 30]</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>64</td>\n",
       "      <td>10</td>\n",
       "      <td>0.748727</td>\n",
       "      <td>18.439620</td>\n",
       "      <td>bank_marketing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>[20, 25, 30]</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>64</td>\n",
       "      <td>10</td>\n",
       "      <td>0.742869</td>\n",
       "      <td>18.202122</td>\n",
       "      <td>bank_marketing</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Hidden Layer Sizes Activation Function  Learning Rate  Batch Size  \\\n",
       "0                 [5]                tanh         0.0100          64   \n",
       "1                 [5]                tanh         0.0010          64   \n",
       "2                 [5]                tanh         0.0001          64   \n",
       "3                 [5]                relu         0.0100          64   \n",
       "4                 [5]                relu         0.0010          64   \n",
       "5                 [5]                relu         0.0001          64   \n",
       "6                 [5]             sigmoid         0.0100          64   \n",
       "7                 [5]             sigmoid         0.0010          64   \n",
       "8                 [5]             sigmoid         0.0001          64   \n",
       "9                [10]                tanh         0.0100          64   \n",
       "10               [10]                tanh         0.0010          64   \n",
       "11               [10]                tanh         0.0001          64   \n",
       "12               [10]                relu         0.0100          64   \n",
       "13               [10]                relu         0.0010          64   \n",
       "14               [10]                relu         0.0001          64   \n",
       "15               [10]             sigmoid         0.0100          64   \n",
       "16               [10]             sigmoid         0.0010          64   \n",
       "17               [10]             sigmoid         0.0001          64   \n",
       "18           [25, 30]                tanh         0.0100          64   \n",
       "19           [25, 30]                tanh         0.0010          64   \n",
       "20           [25, 30]                tanh         0.0001          64   \n",
       "21           [25, 30]                relu         0.0100          64   \n",
       "22           [25, 30]                relu         0.0010          64   \n",
       "23           [25, 30]                relu         0.0001          64   \n",
       "24           [25, 30]             sigmoid         0.0100          64   \n",
       "25           [25, 30]             sigmoid         0.0010          64   \n",
       "26           [25, 30]             sigmoid         0.0001          64   \n",
       "27       [20, 25, 30]                tanh         0.0100          64   \n",
       "28       [20, 25, 30]                tanh         0.0010          64   \n",
       "29       [20, 25, 30]                tanh         0.0001          64   \n",
       "30       [20, 25, 30]                relu         0.0100          64   \n",
       "31       [20, 25, 30]                relu         0.0010          64   \n",
       "32       [20, 25, 30]                relu         0.0001          64   \n",
       "33       [20, 25, 30]             sigmoid         0.0100          64   \n",
       "34       [20, 25, 30]             sigmoid         0.0010          64   \n",
       "35       [20, 25, 30]             sigmoid         0.0001          64   \n",
       "\n",
       "    Number of Epochs  Average Accuracy  Average Training Time         dataset  \n",
       "0                 10          0.795699              12.364878  bank_marketing  \n",
       "1                 10          0.755577              12.158159  bank_marketing  \n",
       "2                 10          0.718461              11.684069  bank_marketing  \n",
       "3                 10          0.770642              11.399968  bank_marketing  \n",
       "4                 10          0.744679              11.911103  bank_marketing  \n",
       "5                 10          0.731579              11.358967  bank_marketing  \n",
       "6                 10          0.754432              11.386674  bank_marketing  \n",
       "7                 10          0.743364              11.465565  bank_marketing  \n",
       "8                 10          0.735661              11.650078  bank_marketing  \n",
       "9                 10          0.820842              11.603699  bank_marketing  \n",
       "10                10          0.789909              11.624668  bank_marketing  \n",
       "11                10          0.739316              12.741981  bank_marketing  \n",
       "12                10          0.828306              13.050930  bank_marketing  \n",
       "13                10          0.787347              12.218218  bank_marketing  \n",
       "14                10          0.743689              12.164853  bank_marketing  \n",
       "15                10          0.800021              11.437619  bank_marketing  \n",
       "16                10          0.745875              11.027652  bank_marketing  \n",
       "17                10          0.737984              11.467428  bank_marketing  \n",
       "18                10          0.841219              13.201574  bank_marketing  \n",
       "19                10          0.829553              13.100218  bank_marketing  \n",
       "20                10          0.742766              13.141413  bank_marketing  \n",
       "21                10          0.852202              14.773023  bank_marketing  \n",
       "22                10          0.822550              13.486950  bank_marketing  \n",
       "23                10          0.748574              13.379348  bank_marketing  \n",
       "24                10          0.839425              13.288525  bank_marketing  \n",
       "25                10          0.749838              13.303207  bank_marketing  \n",
       "26                10          0.740375             287.427189  bank_marketing  \n",
       "27                10          0.830287              15.945419  bank_marketing  \n",
       "28                10          0.819424              16.959366  bank_marketing  \n",
       "29                10          0.741468              17.706601  bank_marketing  \n",
       "30                10          0.841663              17.163013  bank_marketing  \n",
       "31                10          0.827725              17.058871  bank_marketing  \n",
       "32                10          0.747481              17.138449  bank_marketing  \n",
       "33                10          0.832132              17.476922  bank_marketing  \n",
       "34                10          0.748727              18.439620  bank_marketing  \n",
       "35                10          0.742869              18.202122  bank_marketing  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_results_bank"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wine quality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Average Loss: 1.8561035031225623\n",
      "Epoch 2/10, Average Loss: 1.2454239900519208\n",
      "Epoch 3/10, Average Loss: 1.115801117042216\n",
      "Epoch 4/10, Average Loss: 1.0922411309509743\n",
      "Epoch 5/10, Average Loss: 1.078554028418006\n",
      "Epoch 6/10, Average Loss: 1.0724822486319192\n",
      "Epoch 7/10, Average Loss: 1.0665669920967846\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 27\u001b[0m\n\u001b[1;32m     23\u001b[0m input_size \u001b[39m=\u001b[39m train_X\u001b[39m.\u001b[39mshape[\u001b[39m1\u001b[39m]\n\u001b[1;32m     25\u001b[0m NumbOfClasses \u001b[39m=\u001b[39m \u001b[39m10\u001b[39m\n\u001b[0;32m---> 27\u001b[0m grid_results_wine, best_accuracy, best_combination \u001b[39m=\u001b[39m grid_search_cv(\n\u001b[1;32m     28\u001b[0m     hidden_layer_sizes_list, activation_functions, learning_rates, batch_sizes, num_epochs_list, data_loader, NumbOfClasses,\n\u001b[1;32m     29\u001b[0m     k_folds\u001b[39m=\u001b[39;49m\u001b[39m5\u001b[39;49m, use_scaling\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n\u001b[1;32m     31\u001b[0m grid_results_wine[\u001b[39m'\u001b[39m\u001b[39mdataset\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mwine_quality\u001b[39m\u001b[39m'\u001b[39m\n",
      "Cell \u001b[0;32mIn[7], line 40\u001b[0m, in \u001b[0;36mgrid_search_cv\u001b[0;34m(hidden_layer_sizes_list, activation_functions, learning_rates, batch_sizes, num_epochs_list, train_loader, NumbOfClasses, k_folds, use_scaling)\u001b[0m\n\u001b[1;32m     37\u001b[0m     X_train_scaled, X_test_scaled \u001b[39m=\u001b[39m X_train, X_test\n\u001b[1;32m     39\u001b[0m train_start_time \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime()\n\u001b[0;32m---> 40\u001b[0m train_model(model, DataLoader(TensorDataset(X_train_scaled, y_train), batch_size\u001b[39m=\u001b[39;49mbatch_size, shuffle\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m), optimizer, criterion, num_epochs)\n\u001b[1;32m     41\u001b[0m train_end_time \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime()\n\u001b[1;32m     42\u001b[0m fold_training_times\u001b[39m.\u001b[39mappend(train_end_time \u001b[39m-\u001b[39m train_start_time)\n",
      "Cell \u001b[0;32mIn[4], line 13\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(model, train_loader, optimizer, criterion, num_epochs)\u001b[0m\n\u001b[1;32m     10\u001b[0m loss \u001b[39m=\u001b[39m criterion(scores, targets)\n\u001b[1;32m     12\u001b[0m optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[0;32m---> 13\u001b[0m loss\u001b[39m.\u001b[39;49mbackward()\n\u001b[1;32m     14\u001b[0m optimizer\u001b[39m.\u001b[39mstep()\n\u001b[1;32m     16\u001b[0m total_loss \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m loss\u001b[39m.\u001b[39mitem()  \u001b[39m# Accumulate the batch loss\u001b[39;00m\n",
      "File \u001b[0;32m~/MachineLearning23ws/.venv/lib/python3.11/site-packages/torch/_tensor.py:492\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    482\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[1;32m    483\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    484\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[1;32m    485\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    490\u001b[0m         inputs\u001b[39m=\u001b[39minputs,\n\u001b[1;32m    491\u001b[0m     )\n\u001b[0;32m--> 492\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\n\u001b[1;32m    493\u001b[0m     \u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs\n\u001b[1;32m    494\u001b[0m )\n",
      "File \u001b[0;32m~/MachineLearning23ws/.venv/lib/python3.11/site-packages/torch/autograd/__init__.py:251\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    246\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[1;32m    248\u001b[0m \u001b[39m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    249\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    250\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 251\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    252\u001b[0m     tensors,\n\u001b[1;32m    253\u001b[0m     grad_tensors_,\n\u001b[1;32m    254\u001b[0m     retain_graph,\n\u001b[1;32m    255\u001b[0m     create_graph,\n\u001b[1;32m    256\u001b[0m     inputs,\n\u001b[1;32m    257\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m    258\u001b[0m     accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m    259\u001b[0m )\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "hidden_layer_sizes_list = [[5],[10],[25, 30], [20, 25, 30]]\n",
    "activation_functions = [F.tanh, F.relu, F.sigmoid]\n",
    "learning_rates = [0.01, 0.001, 0.0001]\n",
    "batch_sizes = [64]\n",
    "num_epochs_list = [10]\n",
    "\n",
    "dataset = wine_quality\n",
    "\n",
    "#smote_in = True\n",
    "#train_X, train_Y, test_X, test_Y = train_test_split(dataset, \"class\", return_torch=True, DoSmote=smote_in)\n",
    "\n",
    "#train_data = TensorDataset(train_X, train_Y)\n",
    "#train_loader = DataLoader(train_data, batch_size=32, shuffle=False)\n",
    "\n",
    "#test_data = TensorDataset(test_X, test_Y)\n",
    "#test_loader = DataLoader(test_data, batch_size=32, shuffle=False)\n",
    "\n",
    "X = dataset.drop([\"class\"], axis=1).values\n",
    "Y = dataset[\"class\"].values\n",
    "data = TensorDataset(torch.tensor(X), torch.tensor(Y))\n",
    "data_loader = DataLoader(data, batch_size=32, shuffle=False)\n",
    "\n",
    "input_size = X.shape[1]\n",
    "\n",
    "NumbOfClasses = 10\n",
    "\n",
    "grid_results_wine, best_accuracy, best_combination = grid_search_cv(\n",
    "    hidden_layer_sizes_list, activation_functions, learning_rates, batch_sizes, num_epochs_list, data_loader, NumbOfClasses,\n",
    "    k_folds=5, use_scaling=True)\n",
    "\n",
    "grid_results_wine['dataset'] = 'wine_quality'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Hidden Layer Sizes</th>\n",
       "      <th>Activation Function</th>\n",
       "      <th>Learning Rate</th>\n",
       "      <th>Batch Size</th>\n",
       "      <th>Number of Epochs</th>\n",
       "      <th>Average Accuracy</th>\n",
       "      <th>Average Training Time</th>\n",
       "      <th>dataset</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[5]</td>\n",
       "      <td>tanh</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>64</td>\n",
       "      <td>10</td>\n",
       "      <td>0.544861</td>\n",
       "      <td>3.524143</td>\n",
       "      <td>wine_quality</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[5]</td>\n",
       "      <td>tanh</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>64</td>\n",
       "      <td>10</td>\n",
       "      <td>0.463002</td>\n",
       "      <td>3.439461</td>\n",
       "      <td>wine_quality</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[5]</td>\n",
       "      <td>tanh</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>64</td>\n",
       "      <td>10</td>\n",
       "      <td>0.213747</td>\n",
       "      <td>3.510652</td>\n",
       "      <td>wine_quality</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[5]</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>64</td>\n",
       "      <td>10</td>\n",
       "      <td>0.539978</td>\n",
       "      <td>3.760917</td>\n",
       "      <td>wine_quality</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[5]</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>64</td>\n",
       "      <td>10</td>\n",
       "      <td>0.494768</td>\n",
       "      <td>4.171190</td>\n",
       "      <td>wine_quality</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>[5]</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>64</td>\n",
       "      <td>10</td>\n",
       "      <td>0.207216</td>\n",
       "      <td>7.932570</td>\n",
       "      <td>wine_quality</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>[5]</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>64</td>\n",
       "      <td>10</td>\n",
       "      <td>0.516708</td>\n",
       "      <td>8.525369</td>\n",
       "      <td>wine_quality</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>[5]</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>64</td>\n",
       "      <td>10</td>\n",
       "      <td>0.454062</td>\n",
       "      <td>6.224549</td>\n",
       "      <td>wine_quality</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>[5]</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>64</td>\n",
       "      <td>10</td>\n",
       "      <td>0.150149</td>\n",
       "      <td>4.097279</td>\n",
       "      <td>wine_quality</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>[10]</td>\n",
       "      <td>tanh</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>64</td>\n",
       "      <td>10</td>\n",
       "      <td>0.582018</td>\n",
       "      <td>5.518821</td>\n",
       "      <td>wine_quality</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>[10]</td>\n",
       "      <td>tanh</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>64</td>\n",
       "      <td>10</td>\n",
       "      <td>0.503519</td>\n",
       "      <td>5.342001</td>\n",
       "      <td>wine_quality</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>[10]</td>\n",
       "      <td>tanh</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>64</td>\n",
       "      <td>10</td>\n",
       "      <td>0.302454</td>\n",
       "      <td>5.276892</td>\n",
       "      <td>wine_quality</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>[10]</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>64</td>\n",
       "      <td>10</td>\n",
       "      <td>0.585125</td>\n",
       "      <td>5.669784</td>\n",
       "      <td>wine_quality</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>[10]</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>64</td>\n",
       "      <td>10</td>\n",
       "      <td>0.523556</td>\n",
       "      <td>6.142356</td>\n",
       "      <td>wine_quality</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>[10]</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>64</td>\n",
       "      <td>10</td>\n",
       "      <td>0.316277</td>\n",
       "      <td>5.634707</td>\n",
       "      <td>wine_quality</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>[10]</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>64</td>\n",
       "      <td>10</td>\n",
       "      <td>0.572761</td>\n",
       "      <td>5.642807</td>\n",
       "      <td>wine_quality</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>[10]</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>64</td>\n",
       "      <td>10</td>\n",
       "      <td>0.487667</td>\n",
       "      <td>5.214640</td>\n",
       "      <td>wine_quality</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>[10]</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>64</td>\n",
       "      <td>10</td>\n",
       "      <td>0.292563</td>\n",
       "      <td>5.778226</td>\n",
       "      <td>wine_quality</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>[25, 30]</td>\n",
       "      <td>tanh</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>64</td>\n",
       "      <td>10</td>\n",
       "      <td>0.726525</td>\n",
       "      <td>6.269039</td>\n",
       "      <td>wine_quality</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>[25, 30]</td>\n",
       "      <td>tanh</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>64</td>\n",
       "      <td>10</td>\n",
       "      <td>0.643967</td>\n",
       "      <td>6.095573</td>\n",
       "      <td>wine_quality</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>[25, 30]</td>\n",
       "      <td>tanh</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>64</td>\n",
       "      <td>10</td>\n",
       "      <td>0.457930</td>\n",
       "      <td>6.224814</td>\n",
       "      <td>wine_quality</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>[25, 30]</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>64</td>\n",
       "      <td>10</td>\n",
       "      <td>0.718155</td>\n",
       "      <td>6.442483</td>\n",
       "      <td>wine_quality</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>[25, 30]</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>64</td>\n",
       "      <td>10</td>\n",
       "      <td>0.651259</td>\n",
       "      <td>6.544118</td>\n",
       "      <td>wine_quality</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>[25, 30]</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>64</td>\n",
       "      <td>10</td>\n",
       "      <td>0.459325</td>\n",
       "      <td>6.681592</td>\n",
       "      <td>wine_quality</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>[25, 30]</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>64</td>\n",
       "      <td>10</td>\n",
       "      <td>0.652653</td>\n",
       "      <td>7.109723</td>\n",
       "      <td>wine_quality</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>[25, 30]</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>64</td>\n",
       "      <td>10</td>\n",
       "      <td>0.517596</td>\n",
       "      <td>7.182300</td>\n",
       "      <td>wine_quality</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>[25, 30]</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>64</td>\n",
       "      <td>10</td>\n",
       "      <td>0.320906</td>\n",
       "      <td>7.181801</td>\n",
       "      <td>wine_quality</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>[20, 25, 30]</td>\n",
       "      <td>tanh</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>64</td>\n",
       "      <td>10</td>\n",
       "      <td>0.737303</td>\n",
       "      <td>7.681701</td>\n",
       "      <td>wine_quality</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>[20, 25, 30]</td>\n",
       "      <td>tanh</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>64</td>\n",
       "      <td>10</td>\n",
       "      <td>0.661912</td>\n",
       "      <td>7.515853</td>\n",
       "      <td>wine_quality</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>[20, 25, 30]</td>\n",
       "      <td>tanh</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>64</td>\n",
       "      <td>10</td>\n",
       "      <td>0.487477</td>\n",
       "      <td>6.840449</td>\n",
       "      <td>wine_quality</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>[20, 25, 30]</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>64</td>\n",
       "      <td>10</td>\n",
       "      <td>0.716885</td>\n",
       "      <td>7.709406</td>\n",
       "      <td>wine_quality</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>[20, 25, 30]</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>64</td>\n",
       "      <td>10</td>\n",
       "      <td>0.665145</td>\n",
       "      <td>7.856151</td>\n",
       "      <td>wine_quality</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>[20, 25, 30]</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>64</td>\n",
       "      <td>10</td>\n",
       "      <td>0.468391</td>\n",
       "      <td>7.569727</td>\n",
       "      <td>wine_quality</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>[20, 25, 30]</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>64</td>\n",
       "      <td>10</td>\n",
       "      <td>0.647644</td>\n",
       "      <td>7.380021</td>\n",
       "      <td>wine_quality</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>[20, 25, 30]</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>64</td>\n",
       "      <td>10</td>\n",
       "      <td>0.502758</td>\n",
       "      <td>8.032739</td>\n",
       "      <td>wine_quality</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>[20, 25, 30]</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>64</td>\n",
       "      <td>10</td>\n",
       "      <td>0.263140</td>\n",
       "      <td>9.075448</td>\n",
       "      <td>wine_quality</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Hidden Layer Sizes Activation Function  Learning Rate  Batch Size  \\\n",
       "0                 [5]                tanh         0.0100          64   \n",
       "1                 [5]                tanh         0.0010          64   \n",
       "2                 [5]                tanh         0.0001          64   \n",
       "3                 [5]                relu         0.0100          64   \n",
       "4                 [5]                relu         0.0010          64   \n",
       "5                 [5]                relu         0.0001          64   \n",
       "6                 [5]             sigmoid         0.0100          64   \n",
       "7                 [5]             sigmoid         0.0010          64   \n",
       "8                 [5]             sigmoid         0.0001          64   \n",
       "9                [10]                tanh         0.0100          64   \n",
       "10               [10]                tanh         0.0010          64   \n",
       "11               [10]                tanh         0.0001          64   \n",
       "12               [10]                relu         0.0100          64   \n",
       "13               [10]                relu         0.0010          64   \n",
       "14               [10]                relu         0.0001          64   \n",
       "15               [10]             sigmoid         0.0100          64   \n",
       "16               [10]             sigmoid         0.0010          64   \n",
       "17               [10]             sigmoid         0.0001          64   \n",
       "18           [25, 30]                tanh         0.0100          64   \n",
       "19           [25, 30]                tanh         0.0010          64   \n",
       "20           [25, 30]                tanh         0.0001          64   \n",
       "21           [25, 30]                relu         0.0100          64   \n",
       "22           [25, 30]                relu         0.0010          64   \n",
       "23           [25, 30]                relu         0.0001          64   \n",
       "24           [25, 30]             sigmoid         0.0100          64   \n",
       "25           [25, 30]             sigmoid         0.0010          64   \n",
       "26           [25, 30]             sigmoid         0.0001          64   \n",
       "27       [20, 25, 30]                tanh         0.0100          64   \n",
       "28       [20, 25, 30]                tanh         0.0010          64   \n",
       "29       [20, 25, 30]                tanh         0.0001          64   \n",
       "30       [20, 25, 30]                relu         0.0100          64   \n",
       "31       [20, 25, 30]                relu         0.0010          64   \n",
       "32       [20, 25, 30]                relu         0.0001          64   \n",
       "33       [20, 25, 30]             sigmoid         0.0100          64   \n",
       "34       [20, 25, 30]             sigmoid         0.0010          64   \n",
       "35       [20, 25, 30]             sigmoid         0.0001          64   \n",
       "\n",
       "    Number of Epochs  Average Accuracy  Average Training Time       dataset  \n",
       "0                 10          0.544861               3.524143  wine_quality  \n",
       "1                 10          0.463002               3.439461  wine_quality  \n",
       "2                 10          0.213747               3.510652  wine_quality  \n",
       "3                 10          0.539978               3.760917  wine_quality  \n",
       "4                 10          0.494768               4.171190  wine_quality  \n",
       "5                 10          0.207216               7.932570  wine_quality  \n",
       "6                 10          0.516708               8.525369  wine_quality  \n",
       "7                 10          0.454062               6.224549  wine_quality  \n",
       "8                 10          0.150149               4.097279  wine_quality  \n",
       "9                 10          0.582018               5.518821  wine_quality  \n",
       "10                10          0.503519               5.342001  wine_quality  \n",
       "11                10          0.302454               5.276892  wine_quality  \n",
       "12                10          0.585125               5.669784  wine_quality  \n",
       "13                10          0.523556               6.142356  wine_quality  \n",
       "14                10          0.316277               5.634707  wine_quality  \n",
       "15                10          0.572761               5.642807  wine_quality  \n",
       "16                10          0.487667               5.214640  wine_quality  \n",
       "17                10          0.292563               5.778226  wine_quality  \n",
       "18                10          0.726525               6.269039  wine_quality  \n",
       "19                10          0.643967               6.095573  wine_quality  \n",
       "20                10          0.457930               6.224814  wine_quality  \n",
       "21                10          0.718155               6.442483  wine_quality  \n",
       "22                10          0.651259               6.544118  wine_quality  \n",
       "23                10          0.459325               6.681592  wine_quality  \n",
       "24                10          0.652653               7.109723  wine_quality  \n",
       "25                10          0.517596               7.182300  wine_quality  \n",
       "26                10          0.320906               7.181801  wine_quality  \n",
       "27                10          0.737303               7.681701  wine_quality  \n",
       "28                10          0.661912               7.515853  wine_quality  \n",
       "29                10          0.487477               6.840449  wine_quality  \n",
       "30                10          0.716885               7.709406  wine_quality  \n",
       "31                10          0.665145               7.856151  wine_quality  \n",
       "32                10          0.468391               7.569727  wine_quality  \n",
       "33                10          0.647644               7.380021  wine_quality  \n",
       "34                10          0.502758               8.032739  wine_quality  \n",
       "35                10          0.263140               9.075448  wine_quality  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_results_wine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Congressional Voting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Average Loss: 1.0931169390678406\n",
      "Epoch 2/10, Average Loss: 0.8803235093752543\n",
      "Epoch 3/10, Average Loss: 0.7031833132108053\n",
      "Epoch 4/10, Average Loss: 0.5734289089838663\n",
      "Epoch 5/10, Average Loss: 0.4860897362232208\n",
      "Epoch 6/10, Average Loss: 0.40965110063552856\n",
      "Epoch 7/10, Average Loss: 0.36114127437273663\n",
      "Epoch 8/10, Average Loss: 0.32630948225657147\n",
      "Epoch 9/10, Average Loss: 0.2915192147095998\n",
      "Epoch 10/10, Average Loss: 0.2733388940493266\n",
      "Epoch 1/10, Average Loss: 1.104884922504425\n",
      "Epoch 2/10, Average Loss: 0.8754033048947653\n",
      "Epoch 3/10, Average Loss: 0.7022010485331217\n",
      "Epoch 4/10, Average Loss: 0.5597648223241171\n",
      "Epoch 5/10, Average Loss: 0.46973538398742676\n",
      "Epoch 6/10, Average Loss: 0.3878113627433777\n",
      "Epoch 7/10, Average Loss: 0.32780036330223083\n",
      "Epoch 8/10, Average Loss: 0.2840626736481984\n",
      "Epoch 9/10, Average Loss: 0.26743921140829724\n",
      "Epoch 10/10, Average Loss: 0.24184114237626395\n",
      "Epoch 1/10, Average Loss: 1.102731962998708\n",
      "Epoch 2/10, Average Loss: 0.8889010945955912\n",
      "Epoch 3/10, Average Loss: 0.7252159913380941\n",
      "Epoch 4/10, Average Loss: 0.5913541913032532\n",
      "Epoch 5/10, Average Loss: 0.48930923144022626\n",
      "Epoch 6/10, Average Loss: 0.4196431338787079\n",
      "Epoch 7/10, Average Loss: 0.3711295525232951\n",
      "Epoch 8/10, Average Loss: 0.32753802339235943\n",
      "Epoch 9/10, Average Loss: 0.30915406346321106\n",
      "Epoch 10/10, Average Loss: 0.28856085737546283\n",
      "Epoch 1/10, Average Loss: 1.106957693894704\n",
      "Epoch 2/10, Average Loss: 0.8898626565933228\n",
      "Epoch 3/10, Average Loss: 0.7198844154675802\n",
      "Epoch 4/10, Average Loss: 0.5949467221895853\n",
      "Epoch 5/10, Average Loss: 0.49331212043762207\n",
      "Epoch 6/10, Average Loss: 0.42471054196357727\n",
      "Epoch 7/10, Average Loss: 0.37825578451156616\n",
      "Epoch 8/10, Average Loss: 0.3384880820910136\n",
      "Epoch 9/10, Average Loss: 0.31458242734273273\n",
      "Epoch 10/10, Average Loss: 0.28708280126253766\n",
      "Epoch 1/10, Average Loss: 1.0794202486673992\n",
      "Epoch 2/10, Average Loss: 0.8626826604207357\n",
      "Epoch 3/10, Average Loss: 0.6952765981356303\n",
      "Epoch 4/10, Average Loss: 0.5737174153327942\n",
      "Epoch 5/10, Average Loss: 0.47181076804796857\n",
      "Epoch 6/10, Average Loss: 0.4012583593527476\n",
      "Epoch 7/10, Average Loss: 0.3491394917170207\n",
      "Epoch 8/10, Average Loss: 0.32359512646993\n",
      "Epoch 9/10, Average Loss: 0.2942578097184499\n",
      "Epoch 10/10, Average Loss: 0.2649090488751729\n",
      "Epoch 1/10, Average Loss: 1.161855657895406\n",
      "Epoch 2/10, Average Loss: 1.1499100923538208\n",
      "Epoch 3/10, Average Loss: 1.1177000602086384\n",
      "Epoch 4/10, Average Loss: 1.1077334483464558\n",
      "Epoch 5/10, Average Loss: 1.0741946697235107\n",
      "Epoch 6/10, Average Loss: 1.0395254294077556\n",
      "Epoch 7/10, Average Loss: 1.0313474734624226\n",
      "Epoch 8/10, Average Loss: 1.0011760195096333\n",
      "Epoch 9/10, Average Loss: 0.9784696300824484\n",
      "Epoch 10/10, Average Loss: 0.9631786545117696\n",
      "Epoch 1/10, Average Loss: 1.178369164466858\n",
      "Epoch 2/10, Average Loss: 1.1636985143025715\n",
      "Epoch 3/10, Average Loss: 1.1219650904337566\n",
      "Epoch 4/10, Average Loss: 1.0993179480234783\n",
      "Epoch 5/10, Average Loss: 1.0841776529947917\n",
      "Epoch 6/10, Average Loss: 1.0593568483988445\n",
      "Epoch 7/10, Average Loss: 1.0281883279482524\n",
      "Epoch 8/10, Average Loss: 0.9968601862589518\n",
      "Epoch 9/10, Average Loss: 0.9772295753161112\n",
      "Epoch 10/10, Average Loss: 0.9639872709910074\n",
      "Epoch 1/10, Average Loss: 1.1782253980636597\n",
      "Epoch 2/10, Average Loss: 1.1644312143325806\n",
      "Epoch 3/10, Average Loss: 1.1316145658493042\n",
      "Epoch 4/10, Average Loss: 1.1096617778142293\n",
      "Epoch 5/10, Average Loss: 1.0789985259373982\n",
      "Epoch 6/10, Average Loss: 1.0581544240315754\n",
      "Epoch 7/10, Average Loss: 1.0458231568336487\n",
      "Epoch 8/10, Average Loss: 1.0198400616645813\n",
      "Epoch 9/10, Average Loss: 0.9912982980410258\n",
      "Epoch 10/10, Average Loss: 0.9765432278315226\n",
      "Epoch 1/10, Average Loss: 1.1812278827031453\n",
      "Epoch 2/10, Average Loss: 1.160137136777242\n",
      "Epoch 3/10, Average Loss: 1.1392126083374023\n",
      "Epoch 4/10, Average Loss: 1.1032408078511555\n",
      "Epoch 5/10, Average Loss: 1.0932724873224895\n",
      "Epoch 6/10, Average Loss: 1.0646506945292156\n",
      "Epoch 7/10, Average Loss: 1.0329644680023193\n",
      "Epoch 8/10, Average Loss: 1.0174850424130757\n",
      "Epoch 9/10, Average Loss: 1.004867136478424\n",
      "Epoch 10/10, Average Loss: 0.9713082313537598\n",
      "Epoch 1/10, Average Loss: 1.1601636012395222\n",
      "Epoch 2/10, Average Loss: 1.1338187058766682\n",
      "Epoch 3/10, Average Loss: 1.1067631642023723\n",
      "Epoch 4/10, Average Loss: 1.0931599934895833\n",
      "Epoch 5/10, Average Loss: 1.0684966643651326\n",
      "Epoch 6/10, Average Loss: 1.0263110001881917\n",
      "Epoch 7/10, Average Loss: 1.014970858891805\n",
      "Epoch 8/10, Average Loss: 0.9902340769767761\n",
      "Epoch 9/10, Average Loss: 0.9628112514813741\n",
      "Epoch 10/10, Average Loss: 0.9523979624112447\n",
      "Epoch 1/10, Average Loss: 1.1691670020421345\n",
      "Epoch 2/10, Average Loss: 1.1809285084406536\n",
      "Epoch 3/10, Average Loss: 1.1711461941401164\n",
      "Epoch 4/10, Average Loss: 1.1861859162648518\n",
      "Epoch 5/10, Average Loss: 1.1711978912353516\n",
      "Epoch 6/10, Average Loss: 1.156395395596822\n",
      "Epoch 7/10, Average Loss: 1.1703189214070637\n",
      "Epoch 8/10, Average Loss: 1.1593454678853352\n",
      "Epoch 9/10, Average Loss: 1.157830278078715\n",
      "Epoch 10/10, Average Loss: 1.1614975929260254\n",
      "Epoch 1/10, Average Loss: 1.1861473321914673\n",
      "Epoch 2/10, Average Loss: 1.196724494298299\n",
      "Epoch 3/10, Average Loss: 1.175900936126709\n",
      "Epoch 4/10, Average Loss: 1.1776984532674153\n",
      "Epoch 5/10, Average Loss: 1.1843615372975667\n",
      "Epoch 6/10, Average Loss: 1.1829901536305745\n",
      "Epoch 7/10, Average Loss: 1.1722696622212727\n",
      "Epoch 8/10, Average Loss: 1.1631959676742554\n",
      "Epoch 9/10, Average Loss: 1.1612881819407146\n",
      "Epoch 10/10, Average Loss: 1.1690703630447388\n",
      "Epoch 1/10, Average Loss: 1.1861884196599324\n",
      "Epoch 2/10, Average Loss: 1.1960536638895671\n",
      "Epoch 3/10, Average Loss: 1.1836401621500652\n",
      "Epoch 4/10, Average Loss: 1.1839646100997925\n",
      "Epoch 5/10, Average Loss: 1.1766599019368489\n",
      "Epoch 6/10, Average Loss: 1.1755940914154053\n",
      "Epoch 7/10, Average Loss: 1.1841353575388591\n",
      "Epoch 8/10, Average Loss: 1.1787137587865193\n",
      "Epoch 9/10, Average Loss: 1.1659516493479412\n",
      "Epoch 10/10, Average Loss: 1.1731571753819783\n",
      "Epoch 1/10, Average Loss: 1.1890665292739868\n",
      "Epoch 2/10, Average Loss: 1.1910914580027263\n",
      "Epoch 3/10, Average Loss: 1.1936359802881877\n",
      "Epoch 4/10, Average Loss: 1.1765367190043132\n",
      "Epoch 5/10, Average Loss: 1.1930731137593586\n",
      "Epoch 6/10, Average Loss: 1.1827486753463745\n",
      "Epoch 7/10, Average Loss: 1.1697988112767537\n",
      "Epoch 8/10, Average Loss: 1.176295280456543\n",
      "Epoch 9/10, Average Loss: 1.1863284905751545\n",
      "Epoch 10/10, Average Loss: 1.1687403122584026\n",
      "Epoch 1/10, Average Loss: 1.168684760729472\n",
      "Epoch 2/10, Average Loss: 1.1654109954833984\n",
      "Epoch 3/10, Average Loss: 1.1609952052434285\n",
      "Epoch 4/10, Average Loss: 1.1705832878748577\n",
      "Epoch 5/10, Average Loss: 1.1699055830637615\n",
      "Epoch 6/10, Average Loss: 1.145667831103007\n",
      "Epoch 7/10, Average Loss: 1.1592090924580891\n",
      "Epoch 8/10, Average Loss: 1.151147445042928\n",
      "Epoch 9/10, Average Loss: 1.14437735080719\n",
      "Epoch 10/10, Average Loss: 1.1572974920272827\n",
      "Epoch 1/10, Average Loss: 1.4168819586435955\n",
      "Epoch 2/10, Average Loss: 1.1347241401672363\n",
      "Epoch 3/10, Average Loss: 0.8782995343208313\n",
      "Epoch 4/10, Average Loss: 0.6866296927134196\n",
      "Epoch 5/10, Average Loss: 0.6070772608121237\n",
      "Epoch 6/10, Average Loss: 0.5528135498364767\n",
      "Epoch 7/10, Average Loss: 0.5252770880858103\n",
      "Epoch 8/10, Average Loss: 0.4881376624107361\n",
      "Epoch 9/10, Average Loss: 0.44497289260228473\n",
      "Epoch 10/10, Average Loss: 0.41801586747169495\n",
      "Epoch 1/10, Average Loss: 1.4568955898284912\n",
      "Epoch 2/10, Average Loss: 1.1518959601720173\n",
      "Epoch 3/10, Average Loss: 0.8963884711265564\n",
      "Epoch 4/10, Average Loss: 0.702186385790507\n",
      "Epoch 5/10, Average Loss: 0.5858662724494934\n",
      "Epoch 6/10, Average Loss: 0.5261831084887186\n",
      "Epoch 7/10, Average Loss: 0.4837846060593923\n",
      "Epoch 8/10, Average Loss: 0.4544950524965922\n",
      "Epoch 9/10, Average Loss: 0.42143447200457257\n",
      "Epoch 10/10, Average Loss: 0.39357640345891315\n",
      "Epoch 1/10, Average Loss: 1.4199013710021973\n",
      "Epoch 2/10, Average Loss: 1.1210778951644897\n",
      "Epoch 3/10, Average Loss: 0.88538924853007\n",
      "Epoch 4/10, Average Loss: 0.7101138035456339\n",
      "Epoch 5/10, Average Loss: 0.6030843257904053\n",
      "Epoch 6/10, Average Loss: 0.5655913750330607\n",
      "Epoch 7/10, Average Loss: 0.5223484536012014\n",
      "Epoch 8/10, Average Loss: 0.4778340657552083\n",
      "Epoch 9/10, Average Loss: 0.4600643416245778\n",
      "Epoch 10/10, Average Loss: 0.4227980673313141\n",
      "Epoch 1/10, Average Loss: 1.4241050879160564\n",
      "Epoch 2/10, Average Loss: 1.1393569707870483\n",
      "Epoch 3/10, Average Loss: 0.8987920085589091\n",
      "Epoch 4/10, Average Loss: 0.7238291700681051\n",
      "Epoch 5/10, Average Loss: 0.6055598855018616\n",
      "Epoch 6/10, Average Loss: 0.5683741172154745\n",
      "Epoch 7/10, Average Loss: 0.5308586756388346\n",
      "Epoch 8/10, Average Loss: 0.49138830105463666\n",
      "Epoch 9/10, Average Loss: 0.45060935616493225\n",
      "Epoch 10/10, Average Loss: 0.42169703046480816\n",
      "Epoch 1/10, Average Loss: 1.3750719626744587\n",
      "Epoch 2/10, Average Loss: 1.1027671694755554\n",
      "Epoch 3/10, Average Loss: 0.8695654074350992\n",
      "Epoch 4/10, Average Loss: 0.7018522421518961\n",
      "Epoch 5/10, Average Loss: 0.5839415291945139\n",
      "Epoch 6/10, Average Loss: 0.5486680070559183\n",
      "Epoch 7/10, Average Loss: 0.5072456995646158\n",
      "Epoch 8/10, Average Loss: 0.47963419556617737\n",
      "Epoch 9/10, Average Loss: 0.44356314341227215\n",
      "Epoch 10/10, Average Loss: 0.4076351225376129\n",
      "Epoch 1/10, Average Loss: 1.5102052688598633\n",
      "Epoch 2/10, Average Loss: 1.485722303390503\n",
      "Epoch 3/10, Average Loss: 1.4567091067632039\n",
      "Epoch 4/10, Average Loss: 1.4602032105127971\n",
      "Epoch 5/10, Average Loss: 1.3873880704243977\n",
      "Epoch 6/10, Average Loss: 1.3530229727427165\n",
      "Epoch 7/10, Average Loss: 1.3339705069859822\n",
      "Epoch 8/10, Average Loss: 1.3073764244715373\n",
      "Epoch 9/10, Average Loss: 1.277089277903239\n",
      "Epoch 10/10, Average Loss: 1.2413671414057414\n",
      "Epoch 1/10, Average Loss: 1.546865185101827\n",
      "Epoch 2/10, Average Loss: 1.5353983640670776\n",
      "Epoch 3/10, Average Loss: 1.493790825208028\n",
      "Epoch 4/10, Average Loss: 1.4418782790501912\n",
      "Epoch 5/10, Average Loss: 1.4296406110127766\n",
      "Epoch 6/10, Average Loss: 1.3950676520665486\n",
      "Epoch 7/10, Average Loss: 1.3733693361282349\n",
      "Epoch 8/10, Average Loss: 1.3119962215423584\n",
      "Epoch 9/10, Average Loss: 1.2907806237538655\n",
      "Epoch 10/10, Average Loss: 1.2853569984436035\n",
      "Epoch 1/10, Average Loss: 1.516136606534322\n",
      "Epoch 2/10, Average Loss: 1.47974693775177\n",
      "Epoch 3/10, Average Loss: 1.4457180102666218\n",
      "Epoch 4/10, Average Loss: 1.421811620394389\n",
      "Epoch 5/10, Average Loss: 1.3852445284525554\n",
      "Epoch 6/10, Average Loss: 1.3511441548665364\n",
      "Epoch 7/10, Average Loss: 1.330472707748413\n",
      "Epoch 8/10, Average Loss: 1.3155823548634846\n",
      "Epoch 9/10, Average Loss: 1.251775582631429\n",
      "Epoch 10/10, Average Loss: 1.261792818705241\n",
      "Epoch 1/10, Average Loss: 1.512620170911153\n",
      "Epoch 2/10, Average Loss: 1.5010789632797241\n",
      "Epoch 3/10, Average Loss: 1.490902304649353\n",
      "Epoch 4/10, Average Loss: 1.4371650218963623\n",
      "Epoch 5/10, Average Loss: 1.4141019582748413\n",
      "Epoch 6/10, Average Loss: 1.3732575972874959\n",
      "Epoch 7/10, Average Loss: 1.319823185602824\n",
      "Epoch 8/10, Average Loss: 1.3189301093419392\n",
      "Epoch 9/10, Average Loss: 1.297156572341919\n",
      "Epoch 10/10, Average Loss: 1.2426997025807698\n",
      "Epoch 1/10, Average Loss: 1.4654606183369954\n",
      "Epoch 2/10, Average Loss: 1.4551304976145427\n",
      "Epoch 3/10, Average Loss: 1.4106518030166626\n",
      "Epoch 4/10, Average Loss: 1.3901095787684123\n",
      "Epoch 5/10, Average Loss: 1.356492241223653\n",
      "Epoch 6/10, Average Loss: 1.3027954697608948\n",
      "Epoch 7/10, Average Loss: 1.3065963586171467\n",
      "Epoch 8/10, Average Loss: 1.2708578904469807\n",
      "Epoch 9/10, Average Loss: 1.235456943511963\n",
      "Epoch 10/10, Average Loss: 1.2367819547653198\n",
      "Epoch 1/10, Average Loss: 1.5200498501459758\n",
      "Epoch 2/10, Average Loss: 1.5258012215296428\n",
      "Epoch 3/10, Average Loss: 1.526847243309021\n",
      "Epoch 4/10, Average Loss: 1.5650864044825237\n",
      "Epoch 5/10, Average Loss: 1.5124839146931965\n",
      "Epoch 6/10, Average Loss: 1.5071860154469807\n",
      "Epoch 7/10, Average Loss: 1.5110222895940144\n",
      "Epoch 8/10, Average Loss: 1.5151077508926392\n",
      "Epoch 9/10, Average Loss: 1.5134611527125041\n",
      "Epoch 10/10, Average Loss: 1.502493977546692\n",
      "Epoch 1/10, Average Loss: 1.5562451283137004\n",
      "Epoch 2/10, Average Loss: 1.5777428150177002\n",
      "Epoch 3/10, Average Loss: 1.5644797881444295\n",
      "Epoch 4/10, Average Loss: 1.5400002002716064\n",
      "Epoch 5/10, Average Loss: 1.5585428476333618\n",
      "Epoch 6/10, Average Loss: 1.5507516860961914\n",
      "Epoch 7/10, Average Loss: 1.563042163848877\n",
      "Epoch 8/10, Average Loss: 1.5211131572723389\n",
      "Epoch 9/10, Average Loss: 1.5293956995010376\n",
      "Epoch 10/10, Average Loss: 1.5539389848709106\n",
      "Epoch 1/10, Average Loss: 1.5262171030044556\n",
      "Epoch 2/10, Average Loss: 1.5199108521143596\n",
      "Epoch 3/10, Average Loss: 1.513722538948059\n",
      "Epoch 4/10, Average Loss: 1.519846002260844\n",
      "Epoch 5/10, Average Loss: 1.5116552511850994\n",
      "Epoch 6/10, Average Loss: 1.5034786065419514\n",
      "Epoch 7/10, Average Loss: 1.5126171509424846\n",
      "Epoch 8/10, Average Loss: 1.5303370157877605\n",
      "Epoch 9/10, Average Loss: 1.4794669151306152\n",
      "Epoch 10/10, Average Loss: 1.5282095670700073\n",
      "Epoch 1/10, Average Loss: 1.5220956405003865\n",
      "Epoch 2/10, Average Loss: 1.5408798058827717\n",
      "Epoch 3/10, Average Loss: 1.5611093441645305\n",
      "Epoch 4/10, Average Loss: 1.5322006940841675\n",
      "Epoch 5/10, Average Loss: 1.541604995727539\n",
      "Epoch 6/10, Average Loss: 1.5256389776865642\n",
      "Epoch 7/10, Average Loss: 1.4957008759180705\n",
      "Epoch 8/10, Average Loss: 1.527552843093872\n",
      "Epoch 9/10, Average Loss: 1.536711613337199\n",
      "Epoch 10/10, Average Loss: 1.4963452418645222\n",
      "Epoch 1/10, Average Loss: 1.4749339024225872\n",
      "Epoch 2/10, Average Loss: 1.4942310253779094\n",
      "Epoch 3/10, Average Loss: 1.4762653907140095\n",
      "Epoch 4/10, Average Loss: 1.4838413000106812\n",
      "Epoch 5/10, Average Loss: 1.4794085423151653\n",
      "Epoch 6/10, Average Loss: 1.4473964770634968\n",
      "Epoch 7/10, Average Loss: 1.4845839341481526\n",
      "Epoch 8/10, Average Loss: 1.4718900918960571\n",
      "Epoch 9/10, Average Loss: 1.462857683499654\n",
      "Epoch 10/10, Average Loss: 1.4999613364537556\n",
      "Epoch 1/10, Average Loss: 0.7642374436060587\n",
      "Epoch 2/10, Average Loss: 0.7110618352890015\n",
      "Epoch 3/10, Average Loss: 0.6694051027297974\n",
      "Epoch 4/10, Average Loss: 0.6347196102142334\n",
      "Epoch 5/10, Average Loss: 0.5948000550270081\n",
      "Epoch 6/10, Average Loss: 0.5588457584381104\n",
      "Epoch 7/10, Average Loss: 0.5191956162452698\n",
      "Epoch 8/10, Average Loss: 0.4870142837365468\n",
      "Epoch 9/10, Average Loss: 0.4551298717657725\n",
      "Epoch 10/10, Average Loss: 0.42467256387074787\n",
      "Epoch 1/10, Average Loss: 0.7667656143506368\n",
      "Epoch 2/10, Average Loss: 0.7161857684453329\n",
      "Epoch 3/10, Average Loss: 0.6761613289515177\n",
      "Epoch 4/10, Average Loss: 0.6367937922477722\n",
      "Epoch 5/10, Average Loss: 0.6061655879020691\n",
      "Epoch 6/10, Average Loss: 0.5690450668334961\n",
      "Epoch 7/10, Average Loss: 0.5328758955001831\n",
      "Epoch 8/10, Average Loss: 0.49600494901339215\n",
      "Epoch 9/10, Average Loss: 0.4661247630914052\n",
      "Epoch 10/10, Average Loss: 0.42976290980974835\n",
      "Epoch 1/10, Average Loss: 0.7642251253128052\n",
      "Epoch 2/10, Average Loss: 0.7134290933609009\n",
      "Epoch 3/10, Average Loss: 0.6735356251398722\n",
      "Epoch 4/10, Average Loss: 0.6362586816151937\n",
      "Epoch 5/10, Average Loss: 0.5963293313980103\n",
      "Epoch 6/10, Average Loss: 0.5594048500061035\n",
      "Epoch 7/10, Average Loss: 0.5238627791404724\n",
      "Epoch 8/10, Average Loss: 0.4869296948115031\n",
      "Epoch 9/10, Average Loss: 0.45560222864151\n",
      "Epoch 10/10, Average Loss: 0.4258149762948354\n",
      "Epoch 1/10, Average Loss: 0.7634536425272623\n",
      "Epoch 2/10, Average Loss: 0.7156525055567423\n",
      "Epoch 3/10, Average Loss: 0.6762033303578695\n",
      "Epoch 4/10, Average Loss: 0.6408173243204752\n",
      "Epoch 5/10, Average Loss: 0.6031098365783691\n",
      "Epoch 6/10, Average Loss: 0.5669005314509074\n",
      "Epoch 7/10, Average Loss: 0.5324031710624695\n",
      "Epoch 8/10, Average Loss: 0.49667858084042865\n",
      "Epoch 9/10, Average Loss: 0.46572981278101605\n",
      "Epoch 10/10, Average Loss: 0.4354826013247172\n",
      "Epoch 1/10, Average Loss: 0.7626664837201437\n",
      "Epoch 2/10, Average Loss: 0.7116292715072632\n",
      "Epoch 3/10, Average Loss: 0.6664602955182394\n",
      "Epoch 4/10, Average Loss: 0.628609816233317\n",
      "Epoch 5/10, Average Loss: 0.5892266631126404\n",
      "Epoch 6/10, Average Loss: 0.551402231057485\n",
      "Epoch 7/10, Average Loss: 0.5167340238889059\n",
      "Epoch 8/10, Average Loss: 0.486381471157074\n",
      "Epoch 9/10, Average Loss: 0.45236586531003314\n",
      "Epoch 10/10, Average Loss: 0.42027262846628827\n",
      "Epoch 1/10, Average Loss: 0.780491848786672\n",
      "Epoch 2/10, Average Loss: 0.7767666180928549\n",
      "Epoch 3/10, Average Loss: 0.7696420351664225\n",
      "Epoch 4/10, Average Loss: 0.766593873500824\n",
      "Epoch 5/10, Average Loss: 0.7586831251780192\n",
      "Epoch 6/10, Average Loss: 0.7517899672190348\n",
      "Epoch 7/10, Average Loss: 0.7484074831008911\n",
      "Epoch 8/10, Average Loss: 0.7429991364479065\n",
      "Epoch 9/10, Average Loss: 0.7369509140650431\n",
      "Epoch 10/10, Average Loss: 0.733115017414093\n",
      "Epoch 1/10, Average Loss: 0.7834653457005819\n",
      "Epoch 2/10, Average Loss: 0.7798004349072775\n",
      "Epoch 3/10, Average Loss: 0.7707169651985168\n",
      "Epoch 4/10, Average Loss: 0.7654104034105936\n",
      "Epoch 5/10, Average Loss: 0.7610997358957926\n",
      "Epoch 6/10, Average Loss: 0.7557340264320374\n",
      "Epoch 7/10, Average Loss: 0.7498959501584371\n",
      "Epoch 8/10, Average Loss: 0.742806633313497\n",
      "Epoch 9/10, Average Loss: 0.7376019358634949\n",
      "Epoch 10/10, Average Loss: 0.7333714365959167\n",
      "Epoch 1/10, Average Loss: 0.7840118209520975\n",
      "Epoch 2/10, Average Loss: 0.7789993683497111\n",
      "Epoch 3/10, Average Loss: 0.7716101010640463\n",
      "Epoch 4/10, Average Loss: 0.7655450105667114\n",
      "Epoch 5/10, Average Loss: 0.7599320610364279\n",
      "Epoch 6/10, Average Loss: 0.7540717919667562\n",
      "Epoch 7/10, Average Loss: 0.7516074180603027\n",
      "Epoch 8/10, Average Loss: 0.7462676564852396\n",
      "Epoch 9/10, Average Loss: 0.7391988436381022\n",
      "Epoch 10/10, Average Loss: 0.7361703316370646\n",
      "Epoch 1/10, Average Loss: 0.7812094291051229\n",
      "Epoch 2/10, Average Loss: 0.7757846713066101\n",
      "Epoch 3/10, Average Loss: 0.7714318434397379\n",
      "Epoch 4/10, Average Loss: 0.7630985975265503\n",
      "Epoch 5/10, Average Loss: 0.760785977045695\n",
      "Epoch 6/10, Average Loss: 0.754256546497345\n",
      "Epoch 7/10, Average Loss: 0.7473624547322592\n",
      "Epoch 8/10, Average Loss: 0.744437019030253\n",
      "Epoch 9/10, Average Loss: 0.7416394154230753\n",
      "Epoch 10/10, Average Loss: 0.7334394454956055\n",
      "Epoch 1/10, Average Loss: 0.7813827991485596\n",
      "Epoch 2/10, Average Loss: 0.7744311491648356\n",
      "Epoch 3/10, Average Loss: 0.7686420679092407\n",
      "Epoch 4/10, Average Loss: 0.7642185091972351\n",
      "Epoch 5/10, Average Loss: 0.7586349248886108\n",
      "Epoch 6/10, Average Loss: 0.7500093181927999\n",
      "Epoch 7/10, Average Loss: 0.7466224829355875\n",
      "Epoch 8/10, Average Loss: 0.740970234076182\n",
      "Epoch 9/10, Average Loss: 0.7353924512863159\n",
      "Epoch 10/10, Average Loss: 0.7321309447288513\n",
      "Epoch 1/10, Average Loss: 0.7822263836860657\n",
      "Epoch 2/10, Average Loss: 0.7843642036120096\n",
      "Epoch 3/10, Average Loss: 0.782098650932312\n",
      "Epoch 4/10, Average Loss: 0.7838393052419027\n",
      "Epoch 5/10, Average Loss: 0.7814401586850485\n",
      "Epoch 6/10, Average Loss: 0.7788087725639343\n",
      "Epoch 7/10, Average Loss: 0.7817604740460714\n",
      "Epoch 8/10, Average Loss: 0.7797892689704895\n",
      "Epoch 9/10, Average Loss: 0.7779515186945597\n",
      "Epoch 10/10, Average Loss: 0.7791707714398702\n",
      "Epoch 1/10, Average Loss: 0.7852227687835693\n",
      "Epoch 2/10, Average Loss: 0.787221093972524\n",
      "Epoch 3/10, Average Loss: 0.7829020023345947\n",
      "Epoch 4/10, Average Loss: 0.7835725545883179\n",
      "Epoch 5/10, Average Loss: 0.7837686538696289\n",
      "Epoch 6/10, Average Loss: 0.7842190265655518\n",
      "Epoch 7/10, Average Loss: 0.7822980284690857\n",
      "Epoch 8/10, Average Loss: 0.7810563445091248\n",
      "Epoch 9/10, Average Loss: 0.7789161006609598\n",
      "Epoch 10/10, Average Loss: 0.7802407542864481\n",
      "Epoch 1/10, Average Loss: 0.7861316800117493\n",
      "Epoch 2/10, Average Loss: 0.7866981029510498\n",
      "Epoch 3/10, Average Loss: 0.7841013669967651\n",
      "Epoch 4/10, Average Loss: 0.7828983068466187\n",
      "Epoch 5/10, Average Loss: 0.7827054460843405\n",
      "Epoch 6/10, Average Loss: 0.7818158268928528\n",
      "Epoch 7/10, Average Loss: 0.7838907440503439\n",
      "Epoch 8/10, Average Loss: 0.7821825941403707\n",
      "Epoch 9/10, Average Loss: 0.7809017896652222\n",
      "Epoch 10/10, Average Loss: 0.7812278668085734\n",
      "Epoch 1/10, Average Loss: 0.7830472389856974\n",
      "Epoch 2/10, Average Loss: 0.782620926698049\n",
      "Epoch 3/10, Average Loss: 0.7833737134933472\n",
      "Epoch 4/10, Average Loss: 0.7795064250628153\n",
      "Epoch 5/10, Average Loss: 0.7826685110727946\n",
      "Epoch 6/10, Average Loss: 0.7809859315554301\n",
      "Epoch 7/10, Average Loss: 0.7786198258399963\n",
      "Epoch 8/10, Average Loss: 0.7796809474627177\n",
      "Epoch 9/10, Average Loss: 0.7807823816935221\n",
      "Epoch 10/10, Average Loss: 0.7772144675254822\n",
      "Epoch 1/10, Average Loss: 0.7833911180496216\n",
      "Epoch 2/10, Average Loss: 0.7817625403404236\n",
      "Epoch 3/10, Average Loss: 0.7816879351933798\n",
      "Epoch 4/10, Average Loss: 0.78264319896698\n",
      "Epoch 5/10, Average Loss: 0.7824576497077942\n",
      "Epoch 6/10, Average Loss: 0.7788434624671936\n",
      "Epoch 7/10, Average Loss: 0.7798553903897604\n",
      "Epoch 8/10, Average Loss: 0.7786609927813212\n",
      "Epoch 9/10, Average Loss: 0.777945359547933\n",
      "Epoch 10/10, Average Loss: 0.7790613571802775\n",
      "Epoch 1/10, Average Loss: 0.9526531298955282\n",
      "Epoch 2/10, Average Loss: 0.597745935122172\n",
      "Epoch 3/10, Average Loss: 0.42039037744204205\n",
      "Epoch 4/10, Average Loss: 0.32337483763694763\n",
      "Epoch 5/10, Average Loss: 0.2871093948682149\n",
      "Epoch 6/10, Average Loss: 0.25568652153015137\n",
      "Epoch 7/10, Average Loss: 0.22599262992540994\n",
      "Epoch 8/10, Average Loss: 0.19660218308369318\n",
      "Epoch 9/10, Average Loss: 0.19355249404907227\n",
      "Epoch 10/10, Average Loss: 0.16639250765244165\n",
      "Epoch 1/10, Average Loss: 0.9568756818771362\n",
      "Epoch 2/10, Average Loss: 0.5839642186959585\n",
      "Epoch 3/10, Average Loss: 0.38971277077992755\n",
      "Epoch 4/10, Average Loss: 0.2939084420601527\n",
      "Epoch 5/10, Average Loss: 0.24153678119182587\n",
      "Epoch 6/10, Average Loss: 0.208407461643219\n",
      "Epoch 7/10, Average Loss: 0.1776288946469625\n",
      "Epoch 8/10, Average Loss: 0.15619786083698273\n",
      "Epoch 9/10, Average Loss: 0.15804927547772726\n",
      "Epoch 10/10, Average Loss: 0.13005350033442178\n",
      "Epoch 1/10, Average Loss: 1.0040133396784465\n",
      "Epoch 2/10, Average Loss: 0.6154366036256155\n",
      "Epoch 3/10, Average Loss: 0.4330635766188304\n",
      "Epoch 4/10, Average Loss: 0.3341931700706482\n",
      "Epoch 5/10, Average Loss: 0.28695209821065265\n",
      "Epoch 6/10, Average Loss: 0.2474452704191208\n",
      "Epoch 7/10, Average Loss: 0.22497008740901947\n",
      "Epoch 8/10, Average Loss: 0.20620508243640265\n",
      "Epoch 9/10, Average Loss: 0.18596105525890985\n",
      "Epoch 10/10, Average Loss: 0.1743290275335312\n",
      "Epoch 1/10, Average Loss: 0.9478411873181661\n",
      "Epoch 2/10, Average Loss: 0.5931483507156372\n",
      "Epoch 3/10, Average Loss: 0.4134134252866109\n",
      "Epoch 4/10, Average Loss: 0.31977935632069904\n",
      "Epoch 5/10, Average Loss: 0.28571900228659314\n",
      "Epoch 6/10, Average Loss: 0.24736430247624716\n",
      "Epoch 7/10, Average Loss: 0.21049530307451883\n",
      "Epoch 8/10, Average Loss: 0.19752764701843262\n",
      "Epoch 9/10, Average Loss: 0.17799497892459235\n",
      "Epoch 10/10, Average Loss: 0.15622766315937042\n",
      "Epoch 1/10, Average Loss: 0.9413673281669617\n",
      "Epoch 2/10, Average Loss: 0.6227815349896749\n",
      "Epoch 3/10, Average Loss: 0.4227958122889201\n",
      "Epoch 4/10, Average Loss: 0.3371298561493556\n",
      "Epoch 5/10, Average Loss: 0.2888547231753667\n",
      "Epoch 6/10, Average Loss: 0.25860289732615155\n",
      "Epoch 7/10, Average Loss: 0.23005873461564383\n",
      "Epoch 8/10, Average Loss: 0.2200943926970164\n",
      "Epoch 9/10, Average Loss: 0.19092818101247153\n",
      "Epoch 10/10, Average Loss: 0.1788944328824679\n",
      "Epoch 1/10, Average Loss: 1.088836908340454\n",
      "Epoch 2/10, Average Loss: 1.0436142086982727\n",
      "Epoch 3/10, Average Loss: 0.9802949825922648\n",
      "Epoch 4/10, Average Loss: 0.9390166997909546\n",
      "Epoch 5/10, Average Loss: 0.903939962387085\n",
      "Epoch 6/10, Average Loss: 0.8566791216532389\n",
      "Epoch 7/10, Average Loss: 0.8256158034006754\n",
      "Epoch 8/10, Average Loss: 0.7833625674247742\n",
      "Epoch 9/10, Average Loss: 0.7476401925086975\n",
      "Epoch 10/10, Average Loss: 0.7156785726547241\n",
      "Epoch 1/10, Average Loss: 1.0891355673472087\n",
      "Epoch 2/10, Average Loss: 1.040841778119405\n",
      "Epoch 3/10, Average Loss: 0.9812530477841696\n",
      "Epoch 4/10, Average Loss: 0.9376272559165955\n",
      "Epoch 5/10, Average Loss: 0.9074693322181702\n",
      "Epoch 6/10, Average Loss: 0.8728571136792501\n",
      "Epoch 7/10, Average Loss: 0.8260100483894348\n",
      "Epoch 8/10, Average Loss: 0.7826712528864542\n",
      "Epoch 9/10, Average Loss: 0.7532936731974283\n",
      "Epoch 10/10, Average Loss: 0.7115223209063212\n",
      "Epoch 1/10, Average Loss: 1.140596826871236\n",
      "Epoch 2/10, Average Loss: 1.0522991220156352\n",
      "Epoch 3/10, Average Loss: 1.0312914649645488\n",
      "Epoch 4/10, Average Loss: 0.9739705125490824\n",
      "Epoch 5/10, Average Loss: 0.9263561964035034\n",
      "Epoch 6/10, Average Loss: 0.8862135807673136\n",
      "Epoch 7/10, Average Loss: 0.8503923614819845\n",
      "Epoch 8/10, Average Loss: 0.816933810710907\n",
      "Epoch 9/10, Average Loss: 0.7750977277755737\n",
      "Epoch 10/10, Average Loss: 0.7568958600362142\n",
      "Epoch 1/10, Average Loss: 1.0753647089004517\n",
      "Epoch 2/10, Average Loss: 1.0196937322616577\n",
      "Epoch 3/10, Average Loss: 0.9728356003761292\n",
      "Epoch 4/10, Average Loss: 0.9353399078051249\n",
      "Epoch 5/10, Average Loss: 0.9026299715042114\n",
      "Epoch 6/10, Average Loss: 0.8612086375554403\n",
      "Epoch 7/10, Average Loss: 0.8115511933962504\n",
      "Epoch 8/10, Average Loss: 0.7775972882906595\n",
      "Epoch 9/10, Average Loss: 0.7326899568239847\n",
      "Epoch 10/10, Average Loss: 0.7124757766723633\n",
      "Epoch 1/10, Average Loss: 1.0711002945899963\n",
      "Epoch 2/10, Average Loss: 1.044979453086853\n",
      "Epoch 3/10, Average Loss: 0.9805513223012289\n",
      "Epoch 4/10, Average Loss: 0.9407550493876139\n",
      "Epoch 5/10, Average Loss: 0.9056115547815958\n",
      "Epoch 6/10, Average Loss: 0.8544087409973145\n",
      "Epoch 7/10, Average Loss: 0.8262719909350077\n",
      "Epoch 8/10, Average Loss: 0.8020724852879842\n",
      "Epoch 9/10, Average Loss: 0.7477416396141052\n",
      "Epoch 10/10, Average Loss: 0.727184534072876\n",
      "Epoch 1/10, Average Loss: 1.1039588848749797\n",
      "Epoch 2/10, Average Loss: 1.104915201663971\n",
      "Epoch 3/10, Average Loss: 1.081530491511027\n",
      "Epoch 4/10, Average Loss: 1.081904927889506\n",
      "Epoch 5/10, Average Loss: 1.0835410157839458\n",
      "Epoch 6/10, Average Loss: 1.0713610450426738\n",
      "Epoch 7/10, Average Loss: 1.0774227380752563\n",
      "Epoch 8/10, Average Loss: 1.0674368540445964\n",
      "Epoch 9/10, Average Loss: 1.0537234942118328\n",
      "Epoch 10/10, Average Loss: 1.0499399900436401\n",
      "Epoch 1/10, Average Loss: 1.1039940118789673\n",
      "Epoch 2/10, Average Loss: 1.1028666098912556\n",
      "Epoch 3/10, Average Loss: 1.0857061743736267\n",
      "Epoch 4/10, Average Loss: 1.082420249780019\n",
      "Epoch 5/10, Average Loss: 1.092678705851237\n",
      "Epoch 6/10, Average Loss: 1.0975202719370525\n",
      "Epoch 7/10, Average Loss: 1.0804921785990398\n",
      "Epoch 8/10, Average Loss: 1.0695961117744446\n",
      "Epoch 9/10, Average Loss: 1.0714863936106365\n",
      "Epoch 10/10, Average Loss: 1.057874083518982\n",
      "Epoch 1/10, Average Loss: 1.1556628147761028\n",
      "Epoch 2/10, Average Loss: 1.112685223420461\n",
      "Epoch 3/10, Average Loss: 1.1366950869560242\n",
      "Epoch 4/10, Average Loss: 1.1196451981862385\n",
      "Epoch 5/10, Average Loss: 1.109570026397705\n",
      "Epoch 6/10, Average Loss: 1.1046313047409058\n",
      "Epoch 7/10, Average Loss: 1.1040666103363037\n",
      "Epoch 8/10, Average Loss: 1.1027739842732747\n",
      "Epoch 9/10, Average Loss: 1.0895111560821533\n",
      "Epoch 10/10, Average Loss: 1.1030276616414387\n",
      "Epoch 1/10, Average Loss: 1.0897270838419597\n",
      "Epoch 2/10, Average Loss: 1.0777988036473591\n",
      "Epoch 3/10, Average Loss: 1.0725682973861694\n",
      "Epoch 4/10, Average Loss: 1.0780129035313923\n",
      "Epoch 5/10, Average Loss: 1.0843030214309692\n",
      "Epoch 6/10, Average Loss: 1.0754974484443665\n",
      "Epoch 7/10, Average Loss: 1.0620113611221313\n",
      "Epoch 8/10, Average Loss: 1.0567614634831746\n",
      "Epoch 9/10, Average Loss: 1.036859432856242\n",
      "Epoch 10/10, Average Loss: 1.057074209054311\n",
      "Epoch 1/10, Average Loss: 1.0855640570322673\n",
      "Epoch 2/10, Average Loss: 1.1024854977925618\n",
      "Epoch 3/10, Average Loss: 1.0792083342870076\n",
      "Epoch 4/10, Average Loss: 1.0766413013140361\n",
      "Epoch 5/10, Average Loss: 1.079034686088562\n",
      "Epoch 6/10, Average Loss: 1.0612664024035137\n",
      "Epoch 7/10, Average Loss: 1.069659431775411\n",
      "Epoch 8/10, Average Loss: 1.0732604265213013\n",
      "Epoch 9/10, Average Loss: 1.042056381702423\n",
      "Epoch 10/10, Average Loss: 1.0493916074434917\n",
      "Epoch 1/10, Average Loss: 1.0579779545466106\n",
      "Epoch 2/10, Average Loss: 0.6857357819875082\n",
      "Epoch 3/10, Average Loss: 0.4805119534333547\n",
      "Epoch 4/10, Average Loss: 0.38278792301813763\n",
      "Epoch 5/10, Average Loss: 0.3259077270825704\n",
      "Epoch 6/10, Average Loss: 0.27111541231473285\n",
      "Epoch 7/10, Average Loss: 0.2414932201306025\n",
      "Epoch 8/10, Average Loss: 0.2050887644290924\n",
      "Epoch 9/10, Average Loss: 0.19648023943106332\n",
      "Epoch 10/10, Average Loss: 0.17275846501191458\n",
      "Epoch 1/10, Average Loss: 1.075968345006307\n",
      "Epoch 2/10, Average Loss: 0.6958608229955038\n",
      "Epoch 3/10, Average Loss: 0.4986795485019684\n",
      "Epoch 4/10, Average Loss: 0.38114969929059345\n",
      "Epoch 5/10, Average Loss: 0.3264642159144084\n",
      "Epoch 6/10, Average Loss: 0.27346037824948627\n",
      "Epoch 7/10, Average Loss: 0.23252934714158377\n",
      "Epoch 8/10, Average Loss: 0.2017904371023178\n",
      "Epoch 9/10, Average Loss: 0.1963799943526586\n",
      "Epoch 10/10, Average Loss: 0.16437684247891107\n",
      "Epoch 1/10, Average Loss: 1.1319477955500286\n",
      "Epoch 2/10, Average Loss: 0.7145096659660339\n",
      "Epoch 3/10, Average Loss: 0.5139415661493937\n",
      "Epoch 4/10, Average Loss: 0.40414755543073017\n",
      "Epoch 5/10, Average Loss: 0.3358240524927775\n",
      "Epoch 6/10, Average Loss: 0.2875539908806483\n",
      "Epoch 7/10, Average Loss: 0.2561233937740326\n",
      "Epoch 8/10, Average Loss: 0.23720106482505798\n",
      "Epoch 9/10, Average Loss: 0.2109170456727346\n",
      "Epoch 10/10, Average Loss: 0.1927200655142466\n",
      "Epoch 1/10, Average Loss: 1.1146246592203777\n",
      "Epoch 2/10, Average Loss: 0.7323249777158102\n",
      "Epoch 3/10, Average Loss: 0.5267069439093272\n",
      "Epoch 4/10, Average Loss: 0.4180416365464528\n",
      "Epoch 5/10, Average Loss: 0.3586955666542053\n",
      "Epoch 6/10, Average Loss: 0.3081243932247162\n",
      "Epoch 7/10, Average Loss: 0.26250627140204114\n",
      "Epoch 8/10, Average Loss: 0.23890235523382822\n",
      "Epoch 9/10, Average Loss: 0.21123321851094565\n",
      "Epoch 10/10, Average Loss: 0.18957138061523438\n",
      "Epoch 1/10, Average Loss: 1.0947706302007039\n",
      "Epoch 2/10, Average Loss: 0.7164071798324585\n",
      "Epoch 3/10, Average Loss: 0.5005430380503336\n",
      "Epoch 4/10, Average Loss: 0.4075656433900197\n",
      "Epoch 5/10, Average Loss: 0.3511979877948761\n",
      "Epoch 6/10, Average Loss: 0.30248509844144184\n",
      "Epoch 7/10, Average Loss: 0.2669978241125743\n",
      "Epoch 8/10, Average Loss: 0.2475897173086802\n",
      "Epoch 9/10, Average Loss: 0.21907385687033334\n",
      "Epoch 10/10, Average Loss: 0.20691039164861044\n",
      "Epoch 1/10, Average Loss: 1.1974233388900757\n",
      "Epoch 2/10, Average Loss: 1.1641286214192708\n",
      "Epoch 3/10, Average Loss: 1.0986611644426982\n",
      "Epoch 4/10, Average Loss: 1.0622400442759197\n",
      "Epoch 5/10, Average Loss: 0.9939977725346884\n",
      "Epoch 6/10, Average Loss: 0.956204374631246\n",
      "Epoch 7/10, Average Loss: 0.9043425718943278\n",
      "Epoch 8/10, Average Loss: 0.8628995219866434\n",
      "Epoch 9/10, Average Loss: 0.8238244851430258\n",
      "Epoch 10/10, Average Loss: 0.7967752814292908\n",
      "Epoch 1/10, Average Loss: 1.2195005019505818\n",
      "Epoch 2/10, Average Loss: 1.1620643138885498\n",
      "Epoch 3/10, Average Loss: 1.0993881225585938\n",
      "Epoch 4/10, Average Loss: 1.0613322655359905\n",
      "Epoch 5/10, Average Loss: 1.0149734020233154\n",
      "Epoch 6/10, Average Loss: 0.9850973089536031\n",
      "Epoch 7/10, Average Loss: 0.9402206142743429\n",
      "Epoch 8/10, Average Loss: 0.8984238306681315\n",
      "Epoch 9/10, Average Loss: 0.8425755699475607\n",
      "Epoch 10/10, Average Loss: 0.819220761458079\n",
      "Epoch 1/10, Average Loss: 1.2560189167658489\n",
      "Epoch 2/10, Average Loss: 1.2095286846160889\n",
      "Epoch 3/10, Average Loss: 1.1623683373133342\n",
      "Epoch 4/10, Average Loss: 1.1049773295720418\n",
      "Epoch 5/10, Average Loss: 1.0454596877098083\n",
      "Epoch 6/10, Average Loss: 0.9980652729670206\n",
      "Epoch 7/10, Average Loss: 0.9766995509465536\n",
      "Epoch 8/10, Average Loss: 0.9205017487208048\n",
      "Epoch 9/10, Average Loss: 0.875204841295878\n",
      "Epoch 10/10, Average Loss: 0.8445291121800741\n",
      "Epoch 1/10, Average Loss: 1.255225380261739\n",
      "Epoch 2/10, Average Loss: 1.2021981080373128\n",
      "Epoch 3/10, Average Loss: 1.1437203486760457\n",
      "Epoch 4/10, Average Loss: 1.1111471056938171\n",
      "Epoch 5/10, Average Loss: 1.0711236993471782\n",
      "Epoch 6/10, Average Loss: 1.0071535110473633\n",
      "Epoch 7/10, Average Loss: 0.9594708283742269\n",
      "Epoch 8/10, Average Loss: 0.9248749812444051\n",
      "Epoch 9/10, Average Loss: 0.8780330816904703\n",
      "Epoch 10/10, Average Loss: 0.8543188770612081\n",
      "Epoch 1/10, Average Loss: 1.2534880638122559\n",
      "Epoch 2/10, Average Loss: 1.2062207857767742\n",
      "Epoch 3/10, Average Loss: 1.1247910857200623\n",
      "Epoch 4/10, Average Loss: 1.077172080675761\n",
      "Epoch 5/10, Average Loss: 1.0291171272595723\n",
      "Epoch 6/10, Average Loss: 0.9725239872932434\n",
      "Epoch 7/10, Average Loss: 0.9362545013427734\n",
      "Epoch 8/10, Average Loss: 0.9112569093704224\n",
      "Epoch 9/10, Average Loss: 0.8587652444839478\n",
      "Epoch 10/10, Average Loss: 0.8410860300064087\n",
      "Epoch 1/10, Average Loss: 1.2135591109593709\n",
      "Epoch 2/10, Average Loss: 1.2337944507598877\n",
      "Epoch 3/10, Average Loss: 1.2188504139582317\n",
      "Epoch 4/10, Average Loss: 1.2293245395024617\n",
      "Epoch 5/10, Average Loss: 1.1960115035374959\n",
      "Epoch 6/10, Average Loss: 1.2036443154017131\n",
      "Epoch 7/10, Average Loss: 1.1817543109258015\n",
      "Epoch 8/10, Average Loss: 1.1794100602467854\n",
      "Epoch 9/10, Average Loss: 1.1612473328908284\n",
      "Epoch 10/10, Average Loss: 1.168673038482666\n",
      "Epoch 1/10, Average Loss: 1.236566424369812\n",
      "Epoch 2/10, Average Loss: 1.2310067017873128\n",
      "Epoch 3/10, Average Loss: 1.2189104557037354\n",
      "Epoch 4/10, Average Loss: 1.2283192078272502\n",
      "Epoch 5/10, Average Loss: 1.2238948742548625\n",
      "Epoch 6/10, Average Loss: 1.233350694179535\n",
      "Epoch 7/10, Average Loss: 1.2254081964492798\n",
      "Epoch 8/10, Average Loss: 1.2198799848556519\n",
      "Epoch 9/10, Average Loss: 1.1898683706919353\n",
      "Epoch 10/10, Average Loss: 1.1965255737304688\n",
      "Epoch 1/10, Average Loss: 1.270355502764384\n",
      "Epoch 2/10, Average Loss: 1.282998005549113\n",
      "Epoch 3/10, Average Loss: 1.285650650660197\n",
      "Epoch 4/10, Average Loss: 1.2741336425145466\n",
      "Epoch 5/10, Average Loss: 1.2553929487864177\n",
      "Epoch 6/10, Average Loss: 1.2527902523676555\n",
      "Epoch 7/10, Average Loss: 1.2732085386912029\n",
      "Epoch 8/10, Average Loss: 1.24332594871521\n",
      "Epoch 9/10, Average Loss: 1.2342698971430461\n",
      "Epoch 10/10, Average Loss: 1.2312859694163005\n",
      "Epoch 1/10, Average Loss: 1.2714299360911052\n",
      "Epoch 2/10, Average Loss: 1.2697511911392212\n",
      "Epoch 3/10, Average Loss: 1.2610992590586345\n",
      "Epoch 4/10, Average Loss: 1.2817797660827637\n",
      "Epoch 5/10, Average Loss: 1.2805364926656086\n",
      "Epoch 6/10, Average Loss: 1.2515734036763508\n",
      "Epoch 7/10, Average Loss: 1.2407089074452717\n",
      "Epoch 8/10, Average Loss: 1.247321605682373\n",
      "Epoch 9/10, Average Loss: 1.2350298166275024\n",
      "Epoch 10/10, Average Loss: 1.239428202311198\n",
      "Epoch 1/10, Average Loss: 1.2717150847117107\n",
      "Epoch 2/10, Average Loss: 1.2787080605824788\n",
      "Epoch 3/10, Average Loss: 1.2454125881195068\n",
      "Epoch 4/10, Average Loss: 1.2432485421498616\n",
      "Epoch 5/10, Average Loss: 1.2392316261927288\n",
      "Epoch 6/10, Average Loss: 1.2239702145258586\n",
      "Epoch 7/10, Average Loss: 1.224101146062215\n",
      "Epoch 8/10, Average Loss: 1.2332199414571126\n",
      "Epoch 9/10, Average Loss: 1.2089755535125732\n",
      "Epoch 10/10, Average Loss: 1.2223186890284221\n",
      "Epoch 1/10, Average Loss: 0.8461300333340963\n",
      "Epoch 2/10, Average Loss: 0.6597559253374735\n",
      "Epoch 3/10, Average Loss: 0.53117968638738\n",
      "Epoch 4/10, Average Loss: 0.4550279875596364\n",
      "Epoch 5/10, Average Loss: 0.4169487754503886\n",
      "Epoch 6/10, Average Loss: 0.3751689891020457\n",
      "Epoch 7/10, Average Loss: 0.3535468677679698\n",
      "Epoch 8/10, Average Loss: 0.32315022746721905\n",
      "Epoch 9/10, Average Loss: 0.3101242184638977\n",
      "Epoch 10/10, Average Loss: 0.2847977976004283\n",
      "Epoch 1/10, Average Loss: 0.8448986212412516\n",
      "Epoch 2/10, Average Loss: 0.6573531627655029\n",
      "Epoch 3/10, Average Loss: 0.5279684166113535\n",
      "Epoch 4/10, Average Loss: 0.4498816827932994\n",
      "Epoch 5/10, Average Loss: 0.4098506172498067\n",
      "Epoch 6/10, Average Loss: 0.37212618192036945\n",
      "Epoch 7/10, Average Loss: 0.34018561244010925\n",
      "Epoch 8/10, Average Loss: 0.3167482018470764\n",
      "Epoch 9/10, Average Loss: 0.30196236570676166\n",
      "Epoch 10/10, Average Loss: 0.2739480833212535\n",
      "Epoch 1/10, Average Loss: 0.8552273313204447\n",
      "Epoch 2/10, Average Loss: 0.6642333269119263\n",
      "Epoch 3/10, Average Loss: 0.5453199942906698\n",
      "Epoch 4/10, Average Loss: 0.46932007869084674\n",
      "Epoch 5/10, Average Loss: 0.420840044816335\n",
      "Epoch 6/10, Average Loss: 0.3876095612843831\n",
      "Epoch 7/10, Average Loss: 0.3621134360631307\n",
      "Epoch 8/10, Average Loss: 0.3511234869559606\n",
      "Epoch 9/10, Average Loss: 0.3216090500354767\n",
      "Epoch 10/10, Average Loss: 0.30492157737414044\n",
      "Epoch 1/10, Average Loss: 0.8414719700813293\n",
      "Epoch 2/10, Average Loss: 0.6614014705022176\n",
      "Epoch 3/10, Average Loss: 0.5418321291605631\n",
      "Epoch 4/10, Average Loss: 0.4656418561935425\n",
      "Epoch 5/10, Average Loss: 0.4269406795501709\n",
      "Epoch 6/10, Average Loss: 0.39735568563143414\n",
      "Epoch 7/10, Average Loss: 0.3705134987831116\n",
      "Epoch 8/10, Average Loss: 0.345913290977478\n",
      "Epoch 9/10, Average Loss: 0.3177712659041087\n",
      "Epoch 10/10, Average Loss: 0.298299103975296\n",
      "Epoch 1/10, Average Loss: 0.853930135567983\n",
      "Epoch 2/10, Average Loss: 0.6628874937693278\n",
      "Epoch 3/10, Average Loss: 0.5351566771666209\n",
      "Epoch 4/10, Average Loss: 0.45957175890604657\n",
      "Epoch 5/10, Average Loss: 0.4155540466308594\n",
      "Epoch 6/10, Average Loss: 0.38314589858055115\n",
      "Epoch 7/10, Average Loss: 0.3628747860590617\n",
      "Epoch 8/10, Average Loss: 0.345094233751297\n",
      "Epoch 9/10, Average Loss: 0.31469691793123883\n",
      "Epoch 10/10, Average Loss: 0.3051268458366394\n",
      "Epoch 1/10, Average Loss: 0.9094696442286173\n",
      "Epoch 2/10, Average Loss: 0.8915650049845377\n",
      "Epoch 3/10, Average Loss: 0.8721445202827454\n",
      "Epoch 4/10, Average Loss: 0.8465875784556071\n",
      "Epoch 5/10, Average Loss: 0.8171836932500204\n",
      "Epoch 6/10, Average Loss: 0.8061380386352539\n",
      "Epoch 7/10, Average Loss: 0.7796748081843058\n",
      "Epoch 8/10, Average Loss: 0.7635701696077982\n",
      "Epoch 9/10, Average Loss: 0.7419365247090658\n",
      "Epoch 10/10, Average Loss: 0.726576050122579\n",
      "Epoch 1/10, Average Loss: 0.9102556308110555\n",
      "Epoch 2/10, Average Loss: 0.8818226854006449\n",
      "Epoch 3/10, Average Loss: 0.8599892656008402\n",
      "Epoch 4/10, Average Loss: 0.8409080108006796\n",
      "Epoch 5/10, Average Loss: 0.8177209893862406\n",
      "Epoch 6/10, Average Loss: 0.8012171387672424\n",
      "Epoch 7/10, Average Loss: 0.7838233908017477\n",
      "Epoch 8/10, Average Loss: 0.7599593798319498\n",
      "Epoch 9/10, Average Loss: 0.7402922113736471\n",
      "Epoch 10/10, Average Loss: 0.7223105033238729\n",
      "Epoch 1/10, Average Loss: 0.9069838722546896\n",
      "Epoch 2/10, Average Loss: 0.894991417725881\n",
      "Epoch 3/10, Average Loss: 0.8705669045448303\n",
      "Epoch 4/10, Average Loss: 0.8471422592798868\n",
      "Epoch 5/10, Average Loss: 0.8243709802627563\n",
      "Epoch 6/10, Average Loss: 0.8077848156293234\n",
      "Epoch 7/10, Average Loss: 0.7924834887186686\n",
      "Epoch 8/10, Average Loss: 0.7648897965749105\n",
      "Epoch 9/10, Average Loss: 0.7493424614270529\n",
      "Epoch 10/10, Average Loss: 0.7319629987080892\n",
      "Epoch 1/10, Average Loss: 0.898838738600413\n",
      "Epoch 2/10, Average Loss: 0.87767094373703\n",
      "Epoch 3/10, Average Loss: 0.8515047033627828\n",
      "Epoch 4/10, Average Loss: 0.8410747051239014\n",
      "Epoch 5/10, Average Loss: 0.8200451135635376\n",
      "Epoch 6/10, Average Loss: 0.7914568583170573\n",
      "Epoch 7/10, Average Loss: 0.7690138419469198\n",
      "Epoch 8/10, Average Loss: 0.7593991557757059\n",
      "Epoch 9/10, Average Loss: 0.7423709829648336\n",
      "Epoch 10/10, Average Loss: 0.7238903840382894\n",
      "Epoch 1/10, Average Loss: 0.9259664614995321\n",
      "Epoch 2/10, Average Loss: 0.8946699897448221\n",
      "Epoch 3/10, Average Loss: 0.8672840595245361\n",
      "Epoch 4/10, Average Loss: 0.8456170757611593\n",
      "Epoch 5/10, Average Loss: 0.82576056321462\n",
      "Epoch 6/10, Average Loss: 0.8057183821996053\n",
      "Epoch 7/10, Average Loss: 0.7831516663233439\n",
      "Epoch 8/10, Average Loss: 0.7698877652486166\n",
      "Epoch 9/10, Average Loss: 0.7472628752390543\n",
      "Epoch 10/10, Average Loss: 0.7330140074094137\n",
      "Epoch 1/10, Average Loss: 0.9162908792495728\n",
      "Epoch 2/10, Average Loss: 0.9200472434361776\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 27\u001b[0m\n\u001b[1;32m     23\u001b[0m input_size \u001b[39m=\u001b[39m train_X\u001b[39m.\u001b[39mshape[\u001b[39m1\u001b[39m]\n\u001b[1;32m     25\u001b[0m NumbOfClasses \u001b[39m=\u001b[39m \u001b[39m2\u001b[39m\n\u001b[0;32m---> 27\u001b[0m grid_results_voting, best_accuracy, best_combination \u001b[39m=\u001b[39m grid_search_cv(\n\u001b[1;32m     28\u001b[0m     hidden_layer_sizes_list, activation_functions, learning_rates, batch_sizes, num_epochs_list, data_loader, NumbOfClasses,\n\u001b[1;32m     29\u001b[0m     k_folds\u001b[39m=\u001b[39;49m\u001b[39m5\u001b[39;49m, use_scaling\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n\u001b[1;32m     31\u001b[0m grid_results_voting[\u001b[39m'\u001b[39m\u001b[39mdataset\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mcong_voting\u001b[39m\u001b[39m'\u001b[39m\n",
      "Cell \u001b[0;32mIn[7], line 40\u001b[0m, in \u001b[0;36mgrid_search_cv\u001b[0;34m(hidden_layer_sizes_list, activation_functions, learning_rates, batch_sizes, num_epochs_list, train_loader, NumbOfClasses, k_folds, use_scaling)\u001b[0m\n\u001b[1;32m     37\u001b[0m     X_train_scaled, X_test_scaled \u001b[39m=\u001b[39m X_train, X_test\n\u001b[1;32m     39\u001b[0m train_start_time \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime()\n\u001b[0;32m---> 40\u001b[0m train_model(model, DataLoader(TensorDataset(X_train_scaled, y_train), batch_size\u001b[39m=\u001b[39;49mbatch_size, shuffle\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m), optimizer, criterion, num_epochs)\n\u001b[1;32m     41\u001b[0m train_end_time \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime()\n\u001b[1;32m     42\u001b[0m fold_training_times\u001b[39m.\u001b[39mappend(train_end_time \u001b[39m-\u001b[39m train_start_time)\n",
      "Cell \u001b[0;32mIn[4], line 14\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(model, train_loader, optimizer, criterion, num_epochs)\u001b[0m\n\u001b[1;32m     12\u001b[0m     optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[1;32m     13\u001b[0m     loss\u001b[39m.\u001b[39mbackward()\n\u001b[0;32m---> 14\u001b[0m     optimizer\u001b[39m.\u001b[39;49mstep()\n\u001b[1;32m     16\u001b[0m     total_loss \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m loss\u001b[39m.\u001b[39mitem()  \u001b[39m# Accumulate the batch loss\u001b[39;00m\n\u001b[1;32m     18\u001b[0m average_loss \u001b[39m=\u001b[39m total_loss \u001b[39m/\u001b[39m num_batches\n",
      "File \u001b[0;32m~/MachineLearning23ws/.venv/lib/python3.11/site-packages/torch/optim/optimizer.py:373\u001b[0m, in \u001b[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    368\u001b[0m         \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    369\u001b[0m             \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\n\u001b[1;32m    370\u001b[0m                 \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mfunc\u001b[39m}\u001b[39;00m\u001b[39m must return None or a tuple of (new_args, new_kwargs), but got \u001b[39m\u001b[39m{\u001b[39;00mresult\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    371\u001b[0m             )\n\u001b[0;32m--> 373\u001b[0m out \u001b[39m=\u001b[39m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    374\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_optimizer_step_code()\n\u001b[1;32m    376\u001b[0m \u001b[39m# call optimizer step post hooks\u001b[39;00m\n",
      "File \u001b[0;32m~/MachineLearning23ws/.venv/lib/python3.11/site-packages/torch/optim/optimizer.py:76\u001b[0m, in \u001b[0;36m_use_grad_for_differentiable.<locals>._use_grad\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     74\u001b[0m     torch\u001b[39m.\u001b[39mset_grad_enabled(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdefaults[\u001b[39m'\u001b[39m\u001b[39mdifferentiable\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[1;32m     75\u001b[0m     torch\u001b[39m.\u001b[39m_dynamo\u001b[39m.\u001b[39mgraph_break()\n\u001b[0;32m---> 76\u001b[0m     ret \u001b[39m=\u001b[39m func(\u001b[39mself\u001b[39;49m, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     77\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m     78\u001b[0m     torch\u001b[39m.\u001b[39m_dynamo\u001b[39m.\u001b[39mgraph_break()\n",
      "File \u001b[0;32m~/MachineLearning23ws/.venv/lib/python3.11/site-packages/torch/optim/adam.py:163\u001b[0m, in \u001b[0;36mAdam.step\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    152\u001b[0m     beta1, beta2 \u001b[39m=\u001b[39m group[\u001b[39m'\u001b[39m\u001b[39mbetas\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[1;32m    154\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_init_group(\n\u001b[1;32m    155\u001b[0m         group,\n\u001b[1;32m    156\u001b[0m         params_with_grad,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    160\u001b[0m         max_exp_avg_sqs,\n\u001b[1;32m    161\u001b[0m         state_steps)\n\u001b[0;32m--> 163\u001b[0m     adam(\n\u001b[1;32m    164\u001b[0m         params_with_grad,\n\u001b[1;32m    165\u001b[0m         grads,\n\u001b[1;32m    166\u001b[0m         exp_avgs,\n\u001b[1;32m    167\u001b[0m         exp_avg_sqs,\n\u001b[1;32m    168\u001b[0m         max_exp_avg_sqs,\n\u001b[1;32m    169\u001b[0m         state_steps,\n\u001b[1;32m    170\u001b[0m         amsgrad\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mamsgrad\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m    171\u001b[0m         beta1\u001b[39m=\u001b[39;49mbeta1,\n\u001b[1;32m    172\u001b[0m         beta2\u001b[39m=\u001b[39;49mbeta2,\n\u001b[1;32m    173\u001b[0m         lr\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mlr\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m    174\u001b[0m         weight_decay\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mweight_decay\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m    175\u001b[0m         eps\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39meps\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m    176\u001b[0m         maximize\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mmaximize\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m    177\u001b[0m         foreach\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mforeach\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m    178\u001b[0m         capturable\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mcapturable\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m    179\u001b[0m         differentiable\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mdifferentiable\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m    180\u001b[0m         fused\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mfused\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m    181\u001b[0m         grad_scale\u001b[39m=\u001b[39;49m\u001b[39mgetattr\u001b[39;49m(\u001b[39mself\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mgrad_scale\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m),\n\u001b[1;32m    182\u001b[0m         found_inf\u001b[39m=\u001b[39;49m\u001b[39mgetattr\u001b[39;49m(\u001b[39mself\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mfound_inf\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m),\n\u001b[1;32m    183\u001b[0m     )\n\u001b[1;32m    185\u001b[0m \u001b[39mreturn\u001b[39;00m loss\n",
      "File \u001b[0;32m~/MachineLearning23ws/.venv/lib/python3.11/site-packages/torch/optim/adam.py:311\u001b[0m, in \u001b[0;36madam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, differentiable, fused, grad_scale, found_inf, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[0m\n\u001b[1;32m    308\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    309\u001b[0m     func \u001b[39m=\u001b[39m _single_tensor_adam\n\u001b[0;32m--> 311\u001b[0m func(params,\n\u001b[1;32m    312\u001b[0m      grads,\n\u001b[1;32m    313\u001b[0m      exp_avgs,\n\u001b[1;32m    314\u001b[0m      exp_avg_sqs,\n\u001b[1;32m    315\u001b[0m      max_exp_avg_sqs,\n\u001b[1;32m    316\u001b[0m      state_steps,\n\u001b[1;32m    317\u001b[0m      amsgrad\u001b[39m=\u001b[39;49mamsgrad,\n\u001b[1;32m    318\u001b[0m      beta1\u001b[39m=\u001b[39;49mbeta1,\n\u001b[1;32m    319\u001b[0m      beta2\u001b[39m=\u001b[39;49mbeta2,\n\u001b[1;32m    320\u001b[0m      lr\u001b[39m=\u001b[39;49mlr,\n\u001b[1;32m    321\u001b[0m      weight_decay\u001b[39m=\u001b[39;49mweight_decay,\n\u001b[1;32m    322\u001b[0m      eps\u001b[39m=\u001b[39;49meps,\n\u001b[1;32m    323\u001b[0m      maximize\u001b[39m=\u001b[39;49mmaximize,\n\u001b[1;32m    324\u001b[0m      capturable\u001b[39m=\u001b[39;49mcapturable,\n\u001b[1;32m    325\u001b[0m      differentiable\u001b[39m=\u001b[39;49mdifferentiable,\n\u001b[1;32m    326\u001b[0m      grad_scale\u001b[39m=\u001b[39;49mgrad_scale,\n\u001b[1;32m    327\u001b[0m      found_inf\u001b[39m=\u001b[39;49mfound_inf)\n",
      "File \u001b[0;32m~/MachineLearning23ws/.venv/lib/python3.11/site-packages/torch/optim/adam.py:384\u001b[0m, in \u001b[0;36m_single_tensor_adam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, grad_scale, found_inf, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize, capturable, differentiable)\u001b[0m\n\u001b[1;32m    381\u001b[0m     param \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mview_as_real(param)\n\u001b[1;32m    383\u001b[0m \u001b[39m# Decay the first and second moment running average coefficient\u001b[39;00m\n\u001b[0;32m--> 384\u001b[0m exp_avg\u001b[39m.\u001b[39;49mlerp_(grad, \u001b[39m1\u001b[39;49m \u001b[39m-\u001b[39;49m beta1)\n\u001b[1;32m    385\u001b[0m exp_avg_sq\u001b[39m.\u001b[39mmul_(beta2)\u001b[39m.\u001b[39maddcmul_(grad, grad\u001b[39m.\u001b[39mconj(), value\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m \u001b[39m-\u001b[39m beta2)\n\u001b[1;32m    387\u001b[0m \u001b[39mif\u001b[39;00m capturable \u001b[39mor\u001b[39;00m differentiable:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "hidden_layer_sizes_list = [[5],[10],[25, 30], [20, 25, 30]]\n",
    "activation_functions = [F.tanh, F.relu, F.sigmoid]\n",
    "learning_rates = [0.01, 0.001, 0.0001]\n",
    "batch_sizes = [64]\n",
    "num_epochs_list = [10]\n",
    "\n",
    "dataset = cong_voting\n",
    "\n",
    "#smote_in = True\n",
    "#train_X, train_Y, test_X, test_Y = train_test_split(dataset, \"class\", return_torch=True, DoSmote=smote_in)\n",
    "\n",
    "#train_data = TensorDataset(train_X, train_Y)\n",
    "#train_loader = DataLoader(train_data, batch_size=32, shuffle=False)\n",
    "\n",
    "#test_data = TensorDataset(test_X, test_Y)\n",
    "#test_loader = DataLoader(test_data, batch_size=32, shuffle=False)\n",
    "\n",
    "X = dataset.drop([\"class\"], axis=1).values\n",
    "Y = dataset[\"class\"].values\n",
    "data = TensorDataset(torch.tensor(X), torch.tensor(Y))\n",
    "data_loader = DataLoader(data, batch_size=32, shuffle=False)\n",
    "\n",
    "input_size = X.shape[1]\n",
    "\n",
    "NumbOfClasses = 2\n",
    "\n",
    "grid_results_voting, best_accuracy, best_combination = grid_search_cv(\n",
    "    hidden_layer_sizes_list, activation_functions, learning_rates, batch_sizes, num_epochs_list, data_loader, NumbOfClasses,\n",
    "    k_folds=5, use_scaling=True)\n",
    "\n",
    "grid_results_voting['dataset'] = 'cong_voting'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Hidden Layer Sizes</th>\n",
       "      <th>Activation Function</th>\n",
       "      <th>Learning Rate</th>\n",
       "      <th>Batch Size</th>\n",
       "      <th>Number of Epochs</th>\n",
       "      <th>Average Accuracy</th>\n",
       "      <th>Average Training Time</th>\n",
       "      <th>dataset</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[5]</td>\n",
       "      <td>tanh</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>64</td>\n",
       "      <td>10</td>\n",
       "      <td>0.896471</td>\n",
       "      <td>0.141268</td>\n",
       "      <td>cong_voting</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[5]</td>\n",
       "      <td>tanh</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>64</td>\n",
       "      <td>10</td>\n",
       "      <td>0.287731</td>\n",
       "      <td>0.122248</td>\n",
       "      <td>cong_voting</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[5]</td>\n",
       "      <td>tanh</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>64</td>\n",
       "      <td>10</td>\n",
       "      <td>0.212773</td>\n",
       "      <td>0.124714</td>\n",
       "      <td>cong_voting</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[5]</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>64</td>\n",
       "      <td>10</td>\n",
       "      <td>0.890756</td>\n",
       "      <td>0.126250</td>\n",
       "      <td>cong_voting</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[5]</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>64</td>\n",
       "      <td>10</td>\n",
       "      <td>0.304706</td>\n",
       "      <td>0.119831</td>\n",
       "      <td>cong_voting</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>[5]</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>64</td>\n",
       "      <td>10</td>\n",
       "      <td>0.235798</td>\n",
       "      <td>0.114162</td>\n",
       "      <td>cong_voting</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>[5]</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>64</td>\n",
       "      <td>10</td>\n",
       "      <td>0.913782</td>\n",
       "      <td>0.058026</td>\n",
       "      <td>cong_voting</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>[5]</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>64</td>\n",
       "      <td>10</td>\n",
       "      <td>0.264706</td>\n",
       "      <td>0.057321</td>\n",
       "      <td>cong_voting</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>[5]</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>64</td>\n",
       "      <td>10</td>\n",
       "      <td>0.166891</td>\n",
       "      <td>0.054440</td>\n",
       "      <td>cong_voting</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>[10]</td>\n",
       "      <td>tanh</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>64</td>\n",
       "      <td>10</td>\n",
       "      <td>0.936639</td>\n",
       "      <td>0.057579</td>\n",
       "      <td>cong_voting</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>[10]</td>\n",
       "      <td>tanh</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>64</td>\n",
       "      <td>10</td>\n",
       "      <td>0.528571</td>\n",
       "      <td>0.058448</td>\n",
       "      <td>cong_voting</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>[10]</td>\n",
       "      <td>tanh</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>64</td>\n",
       "      <td>10</td>\n",
       "      <td>0.373782</td>\n",
       "      <td>0.058223</td>\n",
       "      <td>cong_voting</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>[10]</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>64</td>\n",
       "      <td>10</td>\n",
       "      <td>0.896639</td>\n",
       "      <td>0.059396</td>\n",
       "      <td>cong_voting</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>[10]</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>64</td>\n",
       "      <td>10</td>\n",
       "      <td>0.581008</td>\n",
       "      <td>0.065138</td>\n",
       "      <td>cong_voting</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>[10]</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>64</td>\n",
       "      <td>10</td>\n",
       "      <td>0.449244</td>\n",
       "      <td>0.055740</td>\n",
       "      <td>cong_voting</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>[10]</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>64</td>\n",
       "      <td>10</td>\n",
       "      <td>0.919328</td>\n",
       "      <td>0.057038</td>\n",
       "      <td>cong_voting</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>[10]</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>64</td>\n",
       "      <td>10</td>\n",
       "      <td>0.385714</td>\n",
       "      <td>0.056266</td>\n",
       "      <td>cong_voting</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>[10]</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>64</td>\n",
       "      <td>10</td>\n",
       "      <td>0.368235</td>\n",
       "      <td>0.059511</td>\n",
       "      <td>cong_voting</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>[25, 30]</td>\n",
       "      <td>tanh</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>64</td>\n",
       "      <td>10</td>\n",
       "      <td>0.965546</td>\n",
       "      <td>0.066926</td>\n",
       "      <td>cong_voting</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>[25, 30]</td>\n",
       "      <td>tanh</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>64</td>\n",
       "      <td>10</td>\n",
       "      <td>0.936639</td>\n",
       "      <td>0.071426</td>\n",
       "      <td>cong_voting</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>[25, 30]</td>\n",
       "      <td>tanh</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>64</td>\n",
       "      <td>10</td>\n",
       "      <td>0.890756</td>\n",
       "      <td>0.067815</td>\n",
       "      <td>cong_voting</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>[25, 30]</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>64</td>\n",
       "      <td>10</td>\n",
       "      <td>0.965378</td>\n",
       "      <td>0.069432</td>\n",
       "      <td>cong_voting</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>[25, 30]</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>64</td>\n",
       "      <td>10</td>\n",
       "      <td>0.948067</td>\n",
       "      <td>0.071041</td>\n",
       "      <td>cong_voting</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>[25, 30]</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>64</td>\n",
       "      <td>10</td>\n",
       "      <td>0.873445</td>\n",
       "      <td>0.065911</td>\n",
       "      <td>cong_voting</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>[25, 30]</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>64</td>\n",
       "      <td>10</td>\n",
       "      <td>0.948235</td>\n",
       "      <td>0.067652</td>\n",
       "      <td>cong_voting</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>[25, 30]</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>64</td>\n",
       "      <td>10</td>\n",
       "      <td>0.631765</td>\n",
       "      <td>0.068159</td>\n",
       "      <td>cong_voting</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>[25, 30]</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>64</td>\n",
       "      <td>10</td>\n",
       "      <td>0.919328</td>\n",
       "      <td>0.072122</td>\n",
       "      <td>cong_voting</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>[20, 25, 30]</td>\n",
       "      <td>tanh</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>64</td>\n",
       "      <td>10</td>\n",
       "      <td>0.948067</td>\n",
       "      <td>0.074412</td>\n",
       "      <td>cong_voting</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>[20, 25, 30]</td>\n",
       "      <td>tanh</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>64</td>\n",
       "      <td>10</td>\n",
       "      <td>0.873445</td>\n",
       "      <td>0.075941</td>\n",
       "      <td>cong_voting</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>[20, 25, 30]</td>\n",
       "      <td>tanh</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>64</td>\n",
       "      <td>10</td>\n",
       "      <td>0.494286</td>\n",
       "      <td>0.080217</td>\n",
       "      <td>cong_voting</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>[20, 25, 30]</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>64</td>\n",
       "      <td>10</td>\n",
       "      <td>0.925210</td>\n",
       "      <td>0.078178</td>\n",
       "      <td>cong_voting</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>[20, 25, 30]</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>64</td>\n",
       "      <td>10</td>\n",
       "      <td>0.890420</td>\n",
       "      <td>0.081912</td>\n",
       "      <td>cong_voting</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>[20, 25, 30]</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>64</td>\n",
       "      <td>10</td>\n",
       "      <td>0.631597</td>\n",
       "      <td>0.077753</td>\n",
       "      <td>cong_voting</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>[20, 25, 30]</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>64</td>\n",
       "      <td>10</td>\n",
       "      <td>0.919496</td>\n",
       "      <td>0.081349</td>\n",
       "      <td>cong_voting</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>[20, 25, 30]</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>64</td>\n",
       "      <td>10</td>\n",
       "      <td>0.631765</td>\n",
       "      <td>0.078873</td>\n",
       "      <td>cong_voting</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>[20, 25, 30]</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>64</td>\n",
       "      <td>10</td>\n",
       "      <td>0.631765</td>\n",
       "      <td>0.078178</td>\n",
       "      <td>cong_voting</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Hidden Layer Sizes Activation Function  Learning Rate  Batch Size  \\\n",
       "0                 [5]                tanh         0.0100          64   \n",
       "1                 [5]                tanh         0.0010          64   \n",
       "2                 [5]                tanh         0.0001          64   \n",
       "3                 [5]                relu         0.0100          64   \n",
       "4                 [5]                relu         0.0010          64   \n",
       "5                 [5]                relu         0.0001          64   \n",
       "6                 [5]             sigmoid         0.0100          64   \n",
       "7                 [5]             sigmoid         0.0010          64   \n",
       "8                 [5]             sigmoid         0.0001          64   \n",
       "9                [10]                tanh         0.0100          64   \n",
       "10               [10]                tanh         0.0010          64   \n",
       "11               [10]                tanh         0.0001          64   \n",
       "12               [10]                relu         0.0100          64   \n",
       "13               [10]                relu         0.0010          64   \n",
       "14               [10]                relu         0.0001          64   \n",
       "15               [10]             sigmoid         0.0100          64   \n",
       "16               [10]             sigmoid         0.0010          64   \n",
       "17               [10]             sigmoid         0.0001          64   \n",
       "18           [25, 30]                tanh         0.0100          64   \n",
       "19           [25, 30]                tanh         0.0010          64   \n",
       "20           [25, 30]                tanh         0.0001          64   \n",
       "21           [25, 30]                relu         0.0100          64   \n",
       "22           [25, 30]                relu         0.0010          64   \n",
       "23           [25, 30]                relu         0.0001          64   \n",
       "24           [25, 30]             sigmoid         0.0100          64   \n",
       "25           [25, 30]             sigmoid         0.0010          64   \n",
       "26           [25, 30]             sigmoid         0.0001          64   \n",
       "27       [20, 25, 30]                tanh         0.0100          64   \n",
       "28       [20, 25, 30]                tanh         0.0010          64   \n",
       "29       [20, 25, 30]                tanh         0.0001          64   \n",
       "30       [20, 25, 30]                relu         0.0100          64   \n",
       "31       [20, 25, 30]                relu         0.0010          64   \n",
       "32       [20, 25, 30]                relu         0.0001          64   \n",
       "33       [20, 25, 30]             sigmoid         0.0100          64   \n",
       "34       [20, 25, 30]             sigmoid         0.0010          64   \n",
       "35       [20, 25, 30]             sigmoid         0.0001          64   \n",
       "\n",
       "    Number of Epochs  Average Accuracy  Average Training Time      dataset  \n",
       "0                 10          0.896471               0.141268  cong_voting  \n",
       "1                 10          0.287731               0.122248  cong_voting  \n",
       "2                 10          0.212773               0.124714  cong_voting  \n",
       "3                 10          0.890756               0.126250  cong_voting  \n",
       "4                 10          0.304706               0.119831  cong_voting  \n",
       "5                 10          0.235798               0.114162  cong_voting  \n",
       "6                 10          0.913782               0.058026  cong_voting  \n",
       "7                 10          0.264706               0.057321  cong_voting  \n",
       "8                 10          0.166891               0.054440  cong_voting  \n",
       "9                 10          0.936639               0.057579  cong_voting  \n",
       "10                10          0.528571               0.058448  cong_voting  \n",
       "11                10          0.373782               0.058223  cong_voting  \n",
       "12                10          0.896639               0.059396  cong_voting  \n",
       "13                10          0.581008               0.065138  cong_voting  \n",
       "14                10          0.449244               0.055740  cong_voting  \n",
       "15                10          0.919328               0.057038  cong_voting  \n",
       "16                10          0.385714               0.056266  cong_voting  \n",
       "17                10          0.368235               0.059511  cong_voting  \n",
       "18                10          0.965546               0.066926  cong_voting  \n",
       "19                10          0.936639               0.071426  cong_voting  \n",
       "20                10          0.890756               0.067815  cong_voting  \n",
       "21                10          0.965378               0.069432  cong_voting  \n",
       "22                10          0.948067               0.071041  cong_voting  \n",
       "23                10          0.873445               0.065911  cong_voting  \n",
       "24                10          0.948235               0.067652  cong_voting  \n",
       "25                10          0.631765               0.068159  cong_voting  \n",
       "26                10          0.919328               0.072122  cong_voting  \n",
       "27                10          0.948067               0.074412  cong_voting  \n",
       "28                10          0.873445               0.075941  cong_voting  \n",
       "29                10          0.494286               0.080217  cong_voting  \n",
       "30                10          0.925210               0.078178  cong_voting  \n",
       "31                10          0.890420               0.081912  cong_voting  \n",
       "32                10          0.631597               0.077753  cong_voting  \n",
       "33                10          0.919496               0.081349  cong_voting  \n",
       "34                10          0.631765               0.078873  cong_voting  \n",
       "35                10          0.631765               0.078178  cong_voting  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_results_voting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Merging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Hidden Layer Sizes</th>\n",
       "      <th>Activation Function</th>\n",
       "      <th>Learning Rate</th>\n",
       "      <th>Batch Size</th>\n",
       "      <th>Number of Epochs</th>\n",
       "      <th>Average Accuracy</th>\n",
       "      <th>Average Training Time</th>\n",
       "      <th>dataset</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[5]</td>\n",
       "      <td>tanh</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>64</td>\n",
       "      <td>10</td>\n",
       "      <td>0.896471</td>\n",
       "      <td>0.141268</td>\n",
       "      <td>cong_voting</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[5]</td>\n",
       "      <td>tanh</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>64</td>\n",
       "      <td>10</td>\n",
       "      <td>0.287731</td>\n",
       "      <td>0.122248</td>\n",
       "      <td>cong_voting</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[5]</td>\n",
       "      <td>tanh</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>64</td>\n",
       "      <td>10</td>\n",
       "      <td>0.212773</td>\n",
       "      <td>0.124714</td>\n",
       "      <td>cong_voting</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[5]</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>64</td>\n",
       "      <td>10</td>\n",
       "      <td>0.890756</td>\n",
       "      <td>0.126250</td>\n",
       "      <td>cong_voting</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[5]</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>64</td>\n",
       "      <td>10</td>\n",
       "      <td>0.304706</td>\n",
       "      <td>0.119831</td>\n",
       "      <td>cong_voting</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103</th>\n",
       "      <td>[20, 25, 30]</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>64</td>\n",
       "      <td>10</td>\n",
       "      <td>0.827725</td>\n",
       "      <td>17.058871</td>\n",
       "      <td>bank_marketing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104</th>\n",
       "      <td>[20, 25, 30]</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>64</td>\n",
       "      <td>10</td>\n",
       "      <td>0.747481</td>\n",
       "      <td>17.138449</td>\n",
       "      <td>bank_marketing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105</th>\n",
       "      <td>[20, 25, 30]</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>64</td>\n",
       "      <td>10</td>\n",
       "      <td>0.832132</td>\n",
       "      <td>17.476922</td>\n",
       "      <td>bank_marketing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106</th>\n",
       "      <td>[20, 25, 30]</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>64</td>\n",
       "      <td>10</td>\n",
       "      <td>0.748727</td>\n",
       "      <td>18.439620</td>\n",
       "      <td>bank_marketing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107</th>\n",
       "      <td>[20, 25, 30]</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>64</td>\n",
       "      <td>10</td>\n",
       "      <td>0.742869</td>\n",
       "      <td>18.202122</td>\n",
       "      <td>bank_marketing</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>108 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    Hidden Layer Sizes Activation Function  Learning Rate  Batch Size  \\\n",
       "0                  [5]                tanh         0.0100          64   \n",
       "1                  [5]                tanh         0.0010          64   \n",
       "2                  [5]                tanh         0.0001          64   \n",
       "3                  [5]                relu         0.0100          64   \n",
       "4                  [5]                relu         0.0010          64   \n",
       "..                 ...                 ...            ...         ...   \n",
       "103       [20, 25, 30]                relu         0.0010          64   \n",
       "104       [20, 25, 30]                relu         0.0001          64   \n",
       "105       [20, 25, 30]             sigmoid         0.0100          64   \n",
       "106       [20, 25, 30]             sigmoid         0.0010          64   \n",
       "107       [20, 25, 30]             sigmoid         0.0001          64   \n",
       "\n",
       "     Number of Epochs  Average Accuracy  Average Training Time         dataset  \n",
       "0                  10          0.896471               0.141268     cong_voting  \n",
       "1                  10          0.287731               0.122248     cong_voting  \n",
       "2                  10          0.212773               0.124714     cong_voting  \n",
       "3                  10          0.890756               0.126250     cong_voting  \n",
       "4                  10          0.304706               0.119831     cong_voting  \n",
       "..                ...               ...                    ...             ...  \n",
       "103                10          0.827725              17.058871  bank_marketing  \n",
       "104                10          0.747481              17.138449  bank_marketing  \n",
       "105                10          0.832132              17.476922  bank_marketing  \n",
       "106                10          0.748727              18.439620  bank_marketing  \n",
       "107                10          0.742869              18.202122  bank_marketing  \n",
       "\n",
       "[108 rows x 8 columns]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_results_df = pd.concat([grid_results_voting,grid_results_wine,grid_results_bank], ignore_index=True)\n",
    "full_results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_results_df.to_csv('./results/cv_grid_search_results.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Top Perfroming Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Hidden Layer Sizes</th>\n",
       "      <th>Activation Function</th>\n",
       "      <th>Learning Rate</th>\n",
       "      <th>Batch Size</th>\n",
       "      <th>Number of Epochs</th>\n",
       "      <th>Average Accuracy</th>\n",
       "      <th>Average Training Time</th>\n",
       "      <th>dataset</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[25, 30]</td>\n",
       "      <td>tanh</td>\n",
       "      <td>0.01</td>\n",
       "      <td>64</td>\n",
       "      <td>10</td>\n",
       "      <td>0.965546</td>\n",
       "      <td>0.066926</td>\n",
       "      <td>cong_voting</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[25, 30]</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.01</td>\n",
       "      <td>64</td>\n",
       "      <td>10</td>\n",
       "      <td>0.965378</td>\n",
       "      <td>0.069432</td>\n",
       "      <td>cong_voting</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[20, 25, 30]</td>\n",
       "      <td>tanh</td>\n",
       "      <td>0.01</td>\n",
       "      <td>64</td>\n",
       "      <td>10</td>\n",
       "      <td>0.737303</td>\n",
       "      <td>7.681701</td>\n",
       "      <td>wine_quality</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[25, 30]</td>\n",
       "      <td>tanh</td>\n",
       "      <td>0.01</td>\n",
       "      <td>64</td>\n",
       "      <td>10</td>\n",
       "      <td>0.726525</td>\n",
       "      <td>6.269039</td>\n",
       "      <td>wine_quality</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[25, 30]</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.01</td>\n",
       "      <td>64</td>\n",
       "      <td>10</td>\n",
       "      <td>0.852202</td>\n",
       "      <td>14.773023</td>\n",
       "      <td>bank_marketing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>[20, 25, 30]</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.01</td>\n",
       "      <td>64</td>\n",
       "      <td>10</td>\n",
       "      <td>0.841663</td>\n",
       "      <td>17.163013</td>\n",
       "      <td>bank_marketing</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Hidden Layer Sizes Activation Function  Learning Rate  Batch Size  \\\n",
       "0           [25, 30]                tanh           0.01          64   \n",
       "1           [25, 30]                relu           0.01          64   \n",
       "2       [20, 25, 30]                tanh           0.01          64   \n",
       "3           [25, 30]                tanh           0.01          64   \n",
       "4           [25, 30]                relu           0.01          64   \n",
       "5       [20, 25, 30]                relu           0.01          64   \n",
       "\n",
       "   Number of Epochs  Average Accuracy  Average Training Time         dataset  \n",
       "0                10          0.965546               0.066926     cong_voting  \n",
       "1                10          0.965378               0.069432     cong_voting  \n",
       "2                10          0.737303               7.681701    wine_quality  \n",
       "3                10          0.726525               6.269039    wine_quality  \n",
       "4                10          0.852202              14.773023  bank_marketing  \n",
       "5                10          0.841663              17.163013  bank_marketing  "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_models_rows = []\n",
    "\n",
    "for dataset in full_results_df['dataset'].unique():\n",
    "    top_models_rows.extend(full_results_df[full_results_df['dataset'] == dataset].nlargest(2, 'Average Accuracy').iterrows())\n",
    "\n",
    "top_models_rows_data = [row[1] for row in top_models_rows]\n",
    "\n",
    "top_models_df = pd.DataFrame(top_models_rows_data).reset_index(drop=True)\n",
    "\n",
    "top_models_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
